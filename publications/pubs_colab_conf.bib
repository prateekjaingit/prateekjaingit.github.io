
@inproceedings{PalVMJTASS24,
title = {Sample-Efficient Personalization: Modeling User Parameters as Low Rank Plus Sparse Components},
author = {Soumyabrata Pal, Prateek Varshney, Gagan Madan, Prateek Jain, Abhradeep Thakurta, Gaurav Aggarwal, Pradeep Shenoy, Gaurav Srivastava},
abstract = {Personalization of machine learning (ML) predictions for individual users/domains/enterprises is critical for practical recommendation systems. Standard personalization approaches involve learning a user/domain specific\emph {embedding} that is fed into a fixed global model which can be limiting. On the other hand, personalizing/fine-tuning model itself for each user/domain—aka meta-learning—has high storage/infrastructure cost. Moreover, rigorous theoretical studies of scalable personalization approaches have been very limited. To address the above issues, we propose a novel meta-learning style approach that models network weights as a sum of low-rank and sparse components. This captures common information from multiple individuals/users together in the low-rank part while sparse part captures user-specific idiosyncrasies. We then study the framework in the linear setting, where the problem reduces to that of estimating the sum of a rank- and a -column sparse matrix using a small number of linear measurements. We propose a computationally efficient alternating minimization method with iterative hard thresholding—AMHT-LRS—to learn the low-rank and sparse part. Theoretically, for the realizable Gaussian data setting, we show that AMHT-LRS solves the problem efficiently with nearly optimal sample complexity. Finally, a significant challenge in personalization is ensuring privacy of each user’s sensitive data. We alleviate this problem by proposing a differentially private variant of our method that also is equipped with strong generalization guarantees.},
booktitle = {International Conference on Artificial Intelligence and Statistics},
year = {2024},
pages = {1702-1710},
month = apr,
url = {all_papers/PalVMJTASS24.pdf}
}
@inproceedings{LiuJKOS24,
title = {Label Robust and Differentially Private Linear Regression: Computational and Statistical Efficiency},
author = {Xiyang Liu, Prateek Jain, Weihao Kong, Sewoong Oh, Arun Suggala},
abstract = {We study the canonical problem of linear regression under -differential privacy when the datapoints are sampled iid~ from a distribution and a fraction of response variables are adversarially corrupted. We provide the first provably efficient--both computationally and statistically--method for this problem, assuming standard assumptions on the data distribution. Our algorithm is a variant of the popular differentially private stochastic gradient descent (DP-SGD) algorithm with two key innovations: a full-batch gradient descent to improve sample complexity and a novel adaptive clipping to guarantee robustness. Our method requires only linear time in input size, and still matches the information theoretical optimal sample complexity up to a data distribution dependent condition number factor. Interestingly, the same algorithm, when applied to a setting where there is no adversarial corruption, still improves upon the existing state-of-the-art and achieves a near optimal sample complexity.},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
year = {2024},
pages = {},
month = feb,
url = {all_papers/LiuJKOS24.pdf}
}
@inproceedings{PalSSJ24,
title = {Blocked Collaborative Bandits: Online Collaborative Filtering with Per-Item Budget Constraints},
author = {Soumyabrata Pal, Arun Suggala, Karthikeyan Shanmugam, Prateek Jain},
abstract = {We consider the problem of\emph {blocked} collaborative bandits where there are multiple users, each with an associated multi-armed bandit problem. These users are grouped into\emph {latent} clusters such that the mean reward vectors of users within the same cluster are identical. Our goal is to design algorithms that maximize the cumulative reward accrued by all the users over time, under the\emph {constraint} that no arm of a user is pulled more than times. This problem has been originally considered by\cite {Bresler: 2014}, and designing regret-optimal algorithms for it has since remained an open problem. In this work, we propose an algorithm called B-LATTICE (Blocked Latent bAndiTs via maTrIx ComplEtion) that collaborates across users, while simultaneously satisfying the budget constraints, to maximize their cumulative rewards. Theoretically, under certain reasonable assumptions on the latent structure, with users, arms, rounds per user, and latent clusters, B-LATTICE achieves a per-user regret of under a budget constraint of . These are the first sub-linear regret bounds for this problem, and match the minimax regret bounds when . Empirically, we demonstrate that our algorithm has superior performance over baselines even when . B-LATTICE is a phased algorithm where in each phase it clusters users into groups and collaborates across users within a group to quickly learn their reward models.},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
year = {2024},
pages = {},
month = feb,
url = {all_papers/PalSSJ24.pdf}
}
@inproceedings{RegeKFCKJF24,
title = {Adanns: A framework for adaptive semantic search},
author = {Aniket Rege, Aditya Kusupati, Alan Fan, Qingqing Cao, Sham Kakade, Prateek Jain, Ali Farhadi},
abstract = {Web-scale search systems learn an encoder to embed a given query which is then hooked into an approximate nearest neighbor search (ANNS) pipeline to retrieve similar data points. To accurately capture tail queries and data points, learned representations typically are _rigid, high-dimensional_ vectors that are generally used as-is in the entire ANNS pipeline and can lead to computationally expensive retrieval. In this paper, we argue that instead of rigid representations, different stages of ANNS can leverage _adaptive representations_ of varying capacities to achieve significantly better accuracy-compute trade-offs, ie, stages of ANNS that can get away with more approximate computation should use a lower-capacity representation of the same data point. To this end, we introduce AdANNS, a novel ANNS design framework that explicitly leverages the flexibility of Matryoshka Representations. We demonstrate state-of-the-art accuracy-compute trade-offs using novel AdANNS-based key ANNS building blocks like search data structures (AdANNS-IVF) and quantization (AdANNS-OPQ). For example on ImageNet retrieval, AdANNS-IVF is up to % more accurate than the rigid representations-based IVF at the same compute budget; and matches accuracy while being up to faster in _wall-clock time_. For Natural Questions, -byte AdANNS-OPQ matches the accuracy of the -byte OPQ baseline constructed using rigid representations--_same accuracy at half the cost! _ We further show that the gains from AdANNS translate to modern-day composite ANNS indices that combine search structures and quantization. Finally, we demonstrate that …},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
year = {2024},
pages = {},
month = feb,
url = {all_papers/RegeKFCKJF24.pdf}
}
@inproceedings{MorwaniBJN24,
title = {Simplicity bias in 1-hidden layer neural networks},
author = {Depen Morwani, Jatin Batra, Prateek Jain, Praneeth Netrapalli},
abstract = {Recent works have demonstrated that neural networks exhibit extreme* simplicity bias*(SB). That is, they learn* only the simplest* features to solve a task at hand, even in the presence of other, more robust but more complex features. Due to the lack of a general and rigorous definition of* features*, these works showcase SB on* semi-synthetic* datasets such as Color-MNIST, MNIST-CIFAR where defining features is relatively easier. In this work, we rigorously define as well as thoroughly establish SB for* one hidden layer* neural networks in the infinite width regime. More concretely,(i) we define SB as the network essentially being a function of a low dimensional projection of the inputs (ii) theoretically, we show that when the data is linearly separable, the network primarily depends on only the linearly separable (-dimensional) subspace even in the presence of an arbitrarily large number of other, more complex features which could have led to a significantly more robust classifier,(iii) empirically, we show that models trained on* real* datasets such as Imagenet and Waterbirds-Landbirds indeed depend on a low dimensional projection of the inputs, thereby demonstrating SB on these datasets, iv) finally, we present a natural ensemble approach that encourages diversity in models by training successive models on features not used by earlier models, and demonstrate that it yields models that are significantly more robust to Gaussian noise.},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
year = {2024},
pages = {},
month = feb,
url = {all_papers/MorwaniBJN24.pdf}
}
@inproceedings{DevvritKKDCDTHKFJ23,
title = {Matformer: Nested transformer for elastic inference},
author = {Fnu Devvrit, Sneha Kudugunta, Aditya Kusupati, Tim Dettmers, Kaifeng Chen, Inderjit S Dhillon, Yulia Tsvetkov, Hannaneh Hajishirzi, Sham M Kakade, Ali Farhadi, Prateek Jain},
abstract = {Transformer models are deployed in a wide range of settings, from multi-accelerator clusters to standalone mobile phones. The diverse inference constraints in these scenarios necessitate practitioners to train foundation models such as PaLM 2, Llama, & ViTs as a series of models of varying sizes. Due to significant training costs, only a select few model sizes are trained and supported, limiting more fine-grained control over relevant tradeoffs, including latency, cost, and accuracy. This work introduces MatFormer, a nested Transformer architecture designed to offer elasticity in a variety of deployment constraints. Each Feed Forward Network (FFN) block of a MatFormer model is jointly optimized with a few nested smaller FFN blocks. This training procedure allows for the Mix'n'Match of model granularities across layers -- i.e., a trained universal MatFormer model enables extraction of hundreds of accurate smaller models, which were never explicitly optimized. We empirically demonstrate MatFormer's effectiveness across different model classes (decoders & encoders), modalities (language & vision), and scales (up to 2.6B parameters). We find that a 2.6B decoder-only MatFormer language model (MatLM) allows us to extract smaller models spanning from 1.5B to 2.6B, each exhibiting comparable validation loss and one-shot downstream evaluations to their independently trained counterparts. Furthermore, we observe that smaller encoders extracted from a universal MatFormer-based ViT (MatViT) encoder preserve the metric-space structure for adaptive large-scale retrieval. Finally, we showcase that speculative decoding with the accurate and …},
booktitle = {Workshop on Advancing Neural Network Training: Computational Efficiency, Scalability, and Resource Optimization (WANT@ NeurIPS 2023)},
year = {2023},
pages = {},
month = oct,
url = {all_papers/DevvritKKDCDTHKFJ23.pdf}
}
@inproceedings{KricheneJSSTZ23,
title = {Multi-task differential privacy under distribution skew},
author = {Walid Krichene, Prateek Jain, Shuang Song, Mukund Sundararajan, Abhradeep Guha Thakurta, Li Zhang},
abstract = {We study the problem of multi-task learning under user-level differential privacy, in which n users contribute data to m tasks, each involving a subset of users. One important aspect of the problem, that can significantly impact quality, is the distribution skew among tasks. Tasks that have much fewer data samples than others are more susceptible to the noise added for privacy. It is natural to ask whether algorithms can adapt to this skew to improve the overall utility. We give a systematic analysis of the problem, by studying how to optimally allocate a user’s privacy budget among tasks. We propose a generic algorithm, based on an adaptive reweighting of the empirical loss, and show that in the presence of distribution skew, this gives a quantifiable improvement of excess empirical risk. Experimental studies on recommendation problems that exhibit a long tail of small tasks, demonstrate that our methods significantly improve utility, achieving the state of the art on two standard benchmarks.},
booktitle = {International Conference on Machine Learning},
year = {2023},
pages = {17784-17807},
month = jul,
url = {all_papers/KricheneJSSTZ23.pdf}
}
@inproceedings{NagarajKANJ23,
title = {Multi-user reinforcement learning with low rank rewards},
author = {Dheeraj Mysore Nagaraj, Suhas S Kowshik, Naman Agarwal, Praneeth Netrapalli, Prateek Jain},
abstract = {We consider collaborative multi-user reinforcement learning, where multiple users have the same state-action space and transition probabilities but different rewards. Under the assumption that the reward matrix of the users has a low-rank structure–a standard and practically successful assumption in the collaborative filtering setting–we design algorithms with significantly lower sample complexity compared to the ones that learn the MDP individually for each user. Our main contribution is an algorithm which explores rewards collaboratively with user-specific MDPs and can learn rewards efficiently in two key settings: tabular MDPs and linear MDPs. When is large and the rank is constant, the sample complexity per MDP depends logarithmically over the size of the state-space, which represents an exponential reduction (in the state-space size) when compared to the standard “non-collaborative” algorithms. Our main technical contribution is a method to construct policies which obtain data such that low rank matrix completion is possible (without a generative model). This goes beyond the regular RL framework and is closely related to mean field limits of multi-agent RL.},
booktitle = {International Conference on Machine Learning},
year = {2023},
pages = {25627-25659},
month = jul,
url = {all_papers/NagarajKANJ23.pdf}
}
@inproceedings{PalSSJ23,
title = {Optimal algorithms for latent bandits with cluster structure},
author = {Soumyabrata Pal, Arun Sai Suggala, Karthikeyan Shanmugam, Prateek Jain},
abstract = {We consider the problem of latent bandits with cluster structure where there are multiple users, each with an associated multi-armed bandit problem. These users are grouped into latent clusters such that the mean reward vectors of users within the same cluster are identical. At each round, a user, selected uniformly at random, pulls an arm and observes a corresponding noisy reward. The goal of the users is to maximize their cumulative rewards. This problem is central to practical recommendation systems and has received wide attention of late Gentile et al.(2014), Maillard and Mannor (2014). Now, if each user acts independently, then they would have to explore each arm independently and a regret of is unavoidable, where M, N are the number of arms and users, respectively. Instead, we propose LATTICE (Latent bAndiTs via maTrIx ComplEtion) which allows exploration of the latent cluster structure to provide the minimax optimal regret of when the number of clusters is . This is the first algorithm to guarantee such strong regret bound. LATTICE is based on a careful exploitation of arm information within a cluster while simultaneously clustering users. Furthermore, it is computationally efficient and requires only calls to an offline matrix completion oracle across all T rounds.},
booktitle = {International Conference on Artificial Intelligence and Statistics},
year = {2023},
pages = {7540-7577},
month = apr,
url = {all_papers/PalSSJ23.pdf}
}
@inproceedings{KusupatiBRWSRHCKJF22,
title = {Matryoshka representation learning},
author = {Aditya Kusupati, Gantavya Bhatt, Aniket Rege, Matthew Wallingford, Aditya Sinha, Vivek Ramanujan, William Howard-Snyder, Kaifeng Chen, Sham Kakade, Prateek Jain, Ali Farhadi},
abstract = {Learned representations are a central component in modern ML systems, serving a multitude of downstream tasks. When training such representations, it is often the case that computational and statistical constraints for each downstream task are unknown. In this context rigid, fixed capacity representations can be either over or under-accommodating to the task at hand. This leads us to ask: can we design a flexible representation that can adapt to multiple downstream tasks with varying computational resources? Our main contribution is Matryoshka Representation Learning (MRL) which encodes information at different granularities and allows a single embedding to adapt to the computational constraints of downstream tasks. MRL minimally modifies existing representation learning pipelines and imposes no additional cost during inference and deployment. MRL learns coarse-to-fine representations that are at least as accurate and rich as independently trained low-dimensional representations. The flexibility within the learned Matryoshka Representations offer:(a) up to smaller embedding size for ImageNet-1K classification at the same level of accuracy;(b) up to real-world speed-ups for large-scale retrieval on ImageNet-1K and 4K; and (c) up to accuracy improvements for long-tail few-shot classification, all while being as robust as the original representations. Finally, we show that MRL extends seamlessly to web-scale datasets (ImageNet, JFT) across various modalities--vision (ViT, ResNet), vision+ language (ALIGN) and language (BERT). MRL code and pretrained models are open-sourced at https://github. com/RAIVNLab/MRL.},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
year = {2022},
pages = {30233-30249},
month = dec,
url = {all_papers/KusupatiBRWSRHCKJF22.pdf}
}
@inproceedings{DevvritSDJ22,
title = {S3GC: scalable self-supervised graph clustering},
author = {Fnu Devvrit, Aditya Sinha, Inderjit Dhillon, Prateek Jain},
abstract = {We study the problem of clustering graphs with additional side-information of node features. The problem is extensively studied, and several existing methods exploit Graph Neural Networks to learn node representations. However, most of the existing methods focus on generic representations instead of their cluster-ability or do not scale to large scale graph datasets. In this work, we propose S3GC which uses contrastive learning along with Graph Neural Networks and node features to learn clusterable features. We empirically demonstrate that S3GC is able to learn the correct cluster structure even when graph information or node features are individually not informative enough to learn correct clusters. Finally, using extensive evaluation on a variety of benchmarks, we demonstrate that S3GC is able to significantly outperform state-of-the-art methods in terms of clustering accuracy--with as much as 5% gain in NMI--while being scalable to graphs of size 100M.},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
year = {2022},
pages = {3248-3261},
month = dec,
url = {all_papers/DevvritSDJ22.pdf}
}
@inproceedings{AhnJJKNS22,
title = {Reproducibility in optimization: Theoretical framework and limits},
author = {Kwangjun Ahn, Prateek Jain, Ziwei Ji, Satyen Kale, Praneeth Netrapalli, Gil I Shamir},
abstract = {We initiate a formal study of reproducibility in optimization. We define a quantitative measure of reproducibility of optimization procedures in the face of noisy or error-prone operations such as inexact or stochastic gradient computations or inexact initialization. We then analyze several convex optimization settings of interest such as smooth, non-smooth, and strongly-convex objective functions and establish tight bounds on the limits of reproducibility in each setting. Our analysis reveals a fundamental trade-off between computation and reproducibility: more computation is necessary (and sufficient) for better reproducibility.},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
year = {2022},
pages = {18022-18033},
month = dec,
url = {all_papers/AhnJJKNS22.pdf}
}
@inproceedings{AddepalliNRNJ22,
title = {Feature reconstruction from outputs can mitigate simplicity bias in neural networks},
author = {Sravanti Addepalli, Anshul Nasery, Venkatesh Babu Radhakrishnan, Praneeth Netrapalli, Prateek Jain},
abstract = {Deep Neural Networks are known to be brittle to even minor distribution shifts compared to the training distribution. While one line of work has demonstrated that \emph{Simplicity Bias} (SB) of DNNs -- bias towards learning only the simplest features -- is a key reason for this brittleness, another recent line of work has surprisingly found that diverse/ complex features are indeed learned by the backbone, and their brittleness is due to the linear classification head relying primarily on the simplest features. To bridge the gap between these two lines of work, we first hypothesize and verify that while SB may not altogether preclude learning complex features, it amplifies simpler features over complex ones. Namely, simple features are replicated several times in the learned representations while complex features might not be replicated. This phenomenon, we term \emph{Feature Replication Hypothesis}, coupled with the \emph{Implicit Bias} of SGD to converge to maximum margin solutions in the feature space, leads the models to rely mostly on the simple features for classification. To mitigate this bias, we propose \emph{Feature Reconstruction Regularizer (FRR)} to ensure that the learned features can be reconstructed back from the logits. The use of \emph{FRR} in linear layer training (\emph{FRR-L}) encourages the use of more diverse features for classification. We further propose to finetune the full network by freezing the weights of the linear layer trained using \emph{FRR-L}, to refine the learned features, making them more suitable for classification. Using this simple solution, we demonstrate up to 15\% gains in OOD accuracy on the recently …},
booktitle = {The Eleventh International Conference on Learning Representations},
year = {2022},
pages = {},
month = sep,
url = {all_papers/AddepalliNRNJ22.pdf}
}
@inproceedings{NaseryANJ22,
title = {DAFT: Distilling Adversarially Fine-tuned teachers for OOD Robustness},
author = {Anshul Nasery, Sravanti Addepalli, Praneeth Netrapalli, Prateek Jain},
abstract = {We consider the problem of OOD generalization,where the goal is to train a model that performs well on test distributions that are different from the training distribution. Deep learning models are known to be fragile to such shifts and can suffer large accuracy drops even for slightly different test distributions (Hendrycks & Dietterich, 2019).We propose a new method –DAFT– based on the intuition that adversarially robust combination of a large number of rich features should provide OOD robustness. Our method carefully distills the model from a powerful teacher that learns several discriminative features using standard training while combining them using adversarial training. The standard adversarial training procedure is modified to produce teachers which can guide the student better. We evaluate DAFT on standard benchmarks in the DomainBed framework, and find that DAFT consistently out-performs well-tuned ERM and distillation baselines by up to 6%, with more pronounced gains for smaller networks},
booktitle = {ICML 2022: Workshop on Spurious Correlations, Invariance and Stability},
year = {2022},
pages = {},
month = jul,
url = {all_papers/NaseryANJ22.pdf}
}
@inproceedings{VarshneyTJ22,
title = {(Nearly) Optimal Private Linear Regression for Sub-Gaussian Data via Adaptive Clipping},
author = {Prateek Varshney, Abhradeep Thakurta, Prateek Jain},
abstract = {We study the problem of differentially private linear regression where each of the data point is sampled from a fixed sub-Gaussian style distribution. We propose and analyze a one-pass mini-batch stochastic gradient descent method (DP-AMBSSGD) where points in each iteration are sampled without replacement. Noise is added for DP but the noise standard deviation is estimated online. Compared to existing -DP techniques which have sub-optimal error bounds, DP-AMBSSGD is able to provide nearly optimal error bounds in terms of key parameters like dimensionality , number of points , and the standard deviation\sigma of the noise in observations. For example, when the -dimensional covariates are sampled iid from the normal distribution, then the excess error of DP-AMBSSGD due to privacy is , ie, the error is meaningful when number of samples N\geq d\log d which is the standard operative regime for linear regression. In contrast, error bounds for existing efficient methods in this setting are: , even for . That is, for constant , the existing techniques require to provide a non-trivial result.},
booktitle = {Conference on Learning Theory},
year = {2022},
pages = {1126-1166},
month = jun,
url = {all_papers/VarshneyTJ22.pdf}
}
@inproceedings{AcharyaHJSDT22,
title = {Robust training in high dimensions via block coordinate geometric median descent},
author = {Anish Acharya, Abolfazl Hashemi, Prateek Jain, Sujay Sanghavi, Inderjit S Dhillon, Ufuk Topcu},
abstract = {Geometric median (GM) is a classical method in statistics for achieving robust estimation of the uncorrupted data; under gross corruption, it achieves the optimal breakdown point of 1/2. However, its computational complexity makes it infeasible for robustifying stochastic gradient descent (SGD) in high-dimensional optimization problems. In this paper, we show that by applying GM to only a judiciously chosen block of coordinates at a time and using a memory mechanism, one can retain the breakdown point of 1/2 for smooth non-convex problems, with non-asymptotic convergence rates comparable to the SGD with GM while resulting in significant speedup in training. We further validate the run-time and robustness of our approach empirically on several popular deep learning tasks. Code available at: https://github. com/anishacharya/BGMD},
booktitle = {International Conference on Artificial Intelligence and Statistics},
year = {2022},
pages = {11145-11168},
month = may,
url = {all_papers/AcharyaHJSDT22.pdf}
}
@inproceedings{KowshikNJN21,
title = {Streaming linear system identification with reverse experience replay},
author = {Suhas Kowshik, Dheeraj Nagaraj, Prateek Jain, Praneeth Netrapalli},
abstract = {We consider the problem of estimating a linear time-invariant (LTI) dynamical system from a single trajectory via streaming algorithms, which is encountered in several applications including reinforcement learning (RL) and time-series analysis. While the LTI system estimation problem is well-studied in the {\em offline} setting, the practically important streaming/online setting has received little attention. Standard streaming methods like stochastic gradient descent (SGD) are unlikely to work since streaming points can be highly correlated. In this work, we propose a novel streaming algorithm, SGD with Reverse Experience Replay (SGD-RER), that is inspired by the experience replay (ER) technique popular in the RL literature. SGD-RER divides data into small buffers and runs SGD backwards on the data stored in the individual buffers. We show that this algorithm exactly deconstructs the dependency structure and obtains information theoretically optimal guarantees for both parameter error and prediction error. Thus, we provide the first--to the best of our knowledge--optimal SGD-style algorithm for the classical problem of linear system identification with a first order oracle. Furthermore, SGD-RER can be applied to more general settings like sparse LTI identification with known sparsity pattern, and non-linear dynamical systems. Our work demonstrates that the knowledge of data dependency structure can aid us in designing statistically and computationally efficient algorithms which can``decorrelate''streaming samples.},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
year = {2021},
pages = {30140-30152},
month = dec,
url = {all_papers/KowshikNJN21.pdf}
}
@inproceedings{ThekumparampilJNO21,
title = {Statistically and computationally efficient linear meta-representation learning},
author = {Kiran K Thekumparampil, Prateek Jain, Praneeth Netrapalli, Sewoong Oh},
abstract = {In typical few-shot learning, each task is not equipped with enough data to be learned in isolation. To cope with such data scarcity, meta-representation learning methods train across many related tasks to find a shared (lower-dimensional) representation of the data where all tasks can be solved accurately. It is hypothesized that any new arriving tasks can be rapidly trained on this low-dimensional representation using only a few samples. Despite the practical successes of this approach, its statistical and computational properties are less understood. Moreover, the prescribed algorithms in these studies have little resemblance to those used in practice or they are computationally intractable. To understand and explain the success of popular meta-representation learning approaches such as ANIL, MetaOptNet, R2D2, and OML, we study a alternating gradient-descent minimization (AltMinGD) method (and its variant alternating minimization (AltMin)) which underlies the aforementioned methods. For a simple but canonical setting of shared linear representations, we show that AltMinGD achieves nearly-optimal estimation error, requiring only samples per task. This agrees with the observed efficacy of this algorithm in the practical few-shot learning scenarios.},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
year = {2021},
pages = {18487-18500},
month = dec,
url = {all_papers/ThekumparampilJNO21.pdf}
}
@inproceedings{KowshikNJN21,
title = {Near-optimal offline and streaming algorithms for learning non-linear dynamical systems},
author = {Suhas Kowshik, Dheeraj Nagaraj, Prateek Jain, Praneeth Netrapalli},
abstract = {We consider the setting of vector valued non-linear dynamical systems , where is unbiased noise and is a known link function that satisfies certain {\em expansivity property}. The goal is to learn from a single trajectory of {\em dependent or correlated} samples. While the problem is well-studied in the linear case, where is identity, with optimal error rates even for non-mixing systems, existing results in the non-linear case hold only for mixing systems. In this work, we improve existing results for learning nonlinear systems in a number of ways: a) we provide the first offline algorithm that can learn non-linear dynamical systems without the mixing assumption, b) we significantly improve upon the sample complexity of existing results for mixing systems, c) in the much harder one-pass, streaming setting we study a SGD with Reverse Experience Replay (SGD-RER) method, and demonstrate that for mixing systems, it achieves the same sample complexity as our offline algorithm, d) we justify the expansivity assumption by showing that for the popular ReLU link function---a non-expansive but easy to learn link function with iid samples---any method would require exponentially many samples (with respect to dimension of ) from the dynamical system. We validate our results via. simulations and demonstrate that a naive application of SGD can be highly sub-optimal. Indeed, our work demonstrates that for correlated data, specialized methods designed for the dependency structure in data can significantly outperform standard SGD based methods.},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
year = {2021},
pages = {8518-8531},
month = dec,
url = {all_papers/KowshikNJN21.pdf}
}
@inproceedings{ShahJN21,
title = {Do input gradients highlight discriminative features?},
author = {Harshay Shah, Prateek Jain, Praneeth Netrapalli},
abstract = {Post-hoc gradient-based interpretability methods [Simonyan et al., 2013, Smilkov et al., 2017] that provide instance-specific explanations of model predictions are often based on assumption (A): magnitude of input gradients—gradients of logits with respect to input—noisily highlight discriminative task-relevant features. In this work, we test the validity of assumption (A) using a three-pronged approach: 1. We develop an evaluation framework, DiffROAR, to test assumption (A) on four image classification benchmarks. Our results suggest that (i) input gradients of standard models (ie, trained on original data) may grossly violate (A), whereas (ii) input gradients of adversarially robust models satisfy (A). 2. We then introduce BlockMNIST, an MNIST-based semi-real dataset, that by design encodes a priori knowledge of discriminative features. Our analysis on BlockMNIST leverages this information to validate as well as characterize differences between input gradient attributions of standard and robust models. 3. Finally, we theoretically prove that our empirical findings hold on a simplified version of the BlockMNIST dataset. Specifically, we prove that input gradients of standard one-hidden-layer MLPs trained on this dataset do not highlight instance-specific signal coordinates, thus grossly violating assumption (A). Our findings motivate the need to formalize and test common assumptions in interpretability in a falsifiable manner [Leavitt and Morcos, 2020]. We believe that the DiffROAR evaluation framework and BlockMNIST-based datasets can serve as sanity checks to audit instance-specific interpretability methods; code and data available at https://github …},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
year = {2021},
pages = {2046-2059},
month = dec,
url = {all_papers/ShahJN21.pdf}
}
@inproceedings{ChienJKRSTZ21,
title = {Private alternating least squares: Practical private matrix completion with tighter rates},
author = {Steve Chien, Prateek Jain, Walid Krichene, Steffen Rendle, Shuang Song, Abhradeep Thakurta, Li Zhang},
abstract = {We study the problem of differentially private (DP) matrix completion under user-level privacy. We design a joint differentially private variant of the popular Alternating-Least-Squares (ALS) method that achieves: i)(nearly) optimal sample complexity for matrix completion (in terms of number of items, users), and ii) the best known privacy/utility trade-off both theoretically, as well as on benchmark data sets. In particular, we provide the first global convergence analysis of ALS with noise introduced to ensure DP, and show that, in comparison to the best known alternative (the Private Frank-Wolfe algorithm by Jain et al.(2018)), our error bounds scale significantly better with respect to the number of items and users, which is critical in practical problems. Extensive validation on standard benchmarks demonstrate that the algorithm, in combination with carefully designed sampling procedures, is significantly more accurate than existing techniques, thus promising to be the first practical DP embedding model.},
booktitle = {International Conference on Machine Learning},
year = {2021},
pages = {1877-1887},
month = jul,
url = {all_papers/ChienJKRSTZ21.pdf}
}
@inproceedings{SahaNNJ21,
title = {Optimal regret algorithm for pseudo-1d bandit convex optimization},
author = {Aadirupa Saha, Nagarajan Natarajan, Praneeth Netrapalli, Prateek Jain},
abstract = {We study online learning with bandit feedback (ie learner has access to only zeroth-order oracle) where cost/reward functions $\f_t $ admit a" pseudo-1d" structure, ie $\f_t (\w)=\loss_t (\pred_t (\w)) $ where the output of $\pred_t $ is one-dimensional. At each round, the learner observes context $\x_t $, plays prediction $\pred_t (\w_t;\x_t) $(eg $\pred_t (\cdot)=⟨\x_t,\cdot⟩ $) for some $\w_t\in\mathbb {R}^ d $ and observes loss $\loss_t (\pred_t (\w_t)) $ where $\loss_t $ is a convex Lipschitz-continuous function. The goal is to minimize the standard regret metric. This pseudo-1d bandit convex optimization problem (\SBCO) arises frequently in domains such as online decision-making or parameter-tuning in large systems. For this problem, we first show a regret lower bound of for any algorithm, where is the number of rounds. We propose a new algorithm\sbcalg that combines randomized online gradient descent with a kernelized exponential weights method to exploit the pseudo-1d structure effectively, guaranteeing the {\em optimal} regret bound mentioned above, up to additional logarithmic factors. In contrast, applying state-of-the-art online convex optimization methods leads to regret, that is significantly suboptimal in terms of .},
booktitle = {International Conference on Machine Learning},
year = {2021},
pages = {9255-9264},
month = jul,
url = {all_papers/SahaNNJ21.pdf}
}
@inproceedings{HiranandaniVKJ20,
title = {Optimization and Analysis of the pAp@ k Metric for Recommender Systems},
author = {Gaurush Hiranandani, Warut Vijitbenjaronk, Sanmi Koyejo, Prateek Jain},
abstract = {Modern recommendation and notification systems must be robust to data imbalance, limitations on the number of recommendations/notifications, and heterogeneous engagement profiles across users. The pAp@ k metric, which combines the partial-AUC and the precision@ k metrics, was recently proposed to evaluate such recommendation systems and has been used in real-world deployments. Conceptually, pAp@ k measures the probability of correctly ranking a top-ranked positive instance over top-ranked negative instances. Due to the combinatorial aspect surfaced by top-ranked points, little is known about the characteristics and optimization methods of pAp@ k. In this paper, we analyze the learning-theoretic properties of pAp@ k, particularly its benefits in evaluating modern recommender systems, and propose novel surrogates that are consistent under certain data regularity conditions. We then provide gradient descent based algorithms to optimize the surrogates directly. Our analysis and experimental evaluation suggest that pAp@ k indeed exhibits a certain dual behavior with respect to partial-AUC and precision@ k. Moreover, the proposed methods outperform all the baselines in various applications. Taken together, our results motivate the use of pAp@ k for large-scale recommender systems with heterogeneous user-engagement.},
booktitle = {International Conference on Machine Learning},
year = {2020},
pages = {4260-4270},
month = nov,
url = {all_papers/HiranandaniVKJ20.pdf}
}
@inproceedings{GoyalRJSJ20,
title = {DROCC: Deep robust one-class classification},
author = {Sachin Goyal, Aditi Raghunathan, Moksh Jain, Harsha Vardhan Simhadri, Prateek Jain},
abstract = {Classical approaches for one-class problems such as one-class SVM and isolation forest require careful feature engineering when applied to structured domains like images. State-of-the-art methods aim to leverage deep learning to learn appropriate features via two main approaches. The first approach based on predicting transformations (Golan & El-Yaniv, 2018; Hendrycks et al., 2019a) while successful in some domains, crucially depends on an appropriate domain-specific set of transformations that are hard to obtain in general. The second approach of minimizing a classical one-class loss on the learned final layer representations, eg, DeepSVDD (Ruff et al., 2018) suffers from the fundamental drawback of representation collapse. In this work, we propose Deep Robust One Class Classification (DROCC) that is both applicable to most standard domains without requiring any side-information and robust to representation collapse. DROCC is based on the assumption that the points from the class of interest lie on a well-sampled, locally linear low dimensional manifold. Empirical evaluation demonstrates that DROCC is highly effective in two different one-class problem settings and on a range of real-world datasets across different domains: tabular data, images (CIFAR and ImageNet), audio, and time-series, offering up to 20% increase in accuracy over the state-of-the-art in anomaly detection. Code is available at https://github. com/microsoft/EdgeML},
booktitle = {International conference on machine learning},
year = {2020},
pages = {3711-3721},
month = nov,
url = {all_papers/GoyalRJSJ20.pdf}
}
@inproceedings{KusupatiRSWJKF20,
title = {Soft threshold weight reparameterization for learnable sparsity},
author = {Aditya Kusupati, Vivek Ramanujan, Raghav Somani, Mitchell Wortsman, Prateek Jain, Sham Kakade, Ali Farhadi},
abstract = {Sparsity in Deep Neural Networks (DNNs) is studied extensively with the focus of maximizing prediction accuracy given an overall parameter budget. Existing methods rely on uniform or heuristic non-uniform sparsity budgets which have sub-optimal layer-wise parameter allocation resulting in a) lower prediction accuracy or b) higher inference cost (FLOPs). This work proposes Soft Threshold Reparameterization (STR), a novel use of the soft-threshold operator on DNN weights. STR smoothly induces sparsity while learning pruning thresholds thereby obtaining a non-uniform sparsity budget. Our method achieves state-of-the-art accuracy for unstructured sparsity in CNNs (ResNet50 and MobileNetV1 on ImageNet-1K), and, additionally, learns non-uniform budgets that empirically reduce the FLOPs by up to 50%. Notably, STR boosts the accuracy over existing results by up to 10% in the ultra sparse (99%) regime and can also be used to induce low-rank (structured sparsity) in RNNs. In short, STR is a simple mechanism which learns effective sparsity budgets that contrast with popular heuristics. Code, pretrained models and sparsity budgets are at https://github. com/RAIVNLab/STR.},
booktitle = {International Conference on Machine Learning},
year = {2020},
pages = {5544-5555},
month = nov,
url = {all_papers/KusupatiRSWJKF20.pdf}
}
@inproceedings{BhatiaPNSJ20,
title = {OASIS: ILP-guided synthesis of loop invariants},
author = {Sahil Bhatia, Saswat Padhi, Nagarajan Natarajan, Rahul Sharma, Prateek Jain},
abstract = {Automated synthesis of inductive invariants is an important problem in software verification. We propose a novel technique that is able to solve complex loop invariant synthesis problems involving large number of variables. We reduce the problem of synthesizing invariants to a set of integer linear programming (ILP) problems. We instantiate our techniques in the tool Oasis that outperforms state-of-the-art systems on benchmarks from the invariant synthesis track of the Syntax Guided Synthesis competition.},
booktitle = {NeurIPS 2020 Workshop on Computer-Assisted Programming},
year = {2020},
pages = {},
month = oct,
url = {all_papers/BhatiaPNSJ20.pdf}
}
@inproceedings{RoyBJ20,
title = {A topic-aligned multilingual corpus of Wikipedia articles for studying information asymmetry in low resource languages},
author = {Dwaipayan Roy, Sumit Bhatia, Prateek Jain},
abstract = {Wikipedia is the largest web-based open encyclopedia covering more than three hundred languages. However, different language editions of Wikipedia differ significantly in terms of their information coverage. We present a systematic comparison of information coverage in English Wikipedia (most exhaustive) and Wikipedias in eight other widely spoken languages (Arabic, German, Hindi, Korean, Portuguese, Russian, Spanish and Turkish). We analyze the content present in the respective Wikipedias in terms of the coverage of topics as well as the depth of coverage of topics included in these Wikipedias. Our analysis quantifies and provides useful insights about the information gap that exists between different language editions of Wikipedia and offers a roadmap for the IR community to bridge this gap.},
booktitle = {Proceedings of the Twelfth Language Resources and Evaluation Conference},
year = {2020},
pages = {2373-2380},
month = may,
url = {all_papers/RoyBJ20.pdf}
}
@inproceedings{ThekumparampilJNO20,
title = {Projection efficient subgradient method and optimal nonsmooth frank-wolfe method},
author = {Kiran K Thekumparampil, Prateek Jain, Praneeth Netrapalli, Sewoong Oh},
abstract = {We consider the classical setting of optimizing a nonsmooth Lipschitz continuous convex function over a convex constraint set, when having access to a (stochastic) first-order oracle (FO) for the function and a projection oracle (PO) for the constraint set. It is well known that to achieve -suboptimality in high-dimensions, FO calls are necessary. This is achieved by the projected subgradient method (PGD). However, PGD also entails PO calls, which may be computationally costlier than FO calls (eg nuclear norm constraints). Improving this PO calls complexity of PGD is largely unexplored, despite the fundamental nature of this problem and extensive literature. We present first such improvement. This only requires a mild assumption that the objective function, when extended to a slightly larger neighborhood of the constraint set, still remains Lipschitz and accessible via FO. In particular, we introduce MOPES method, which carefully combines Moreau-Yosida smoothing and accelerated first-order schemes. This is guaranteed to find a feasible -suboptimal solution using only PO calls and optimal FO calls. Further, instead of a PO if we only have a linear minimization oracle (LMO, a la Frank-Wolfe) to access the constraint set, an extension of our method, MOLES, finds a feasible -suboptimal solution using LMO calls and FO calls---both match known lower bounds, resolving a question left open since White (1993). Our experiments confirm that these methods achieve significant speedups over the state-of-the-art, for a problem with costly PO and LMO calls.},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
year = {2020},
pages = {12211-12224},
month = dec,
url = {all_papers/ThekumparampilJNO20.pdf}
}
@inproceedings{ShahTRJN20,
title = {The pitfalls of simplicity bias in neural networks},
author = {Harshay Shah, Kaustav Tamuly, Aditi Raghunathan, Prateek Jain, Praneeth Netrapalli},
abstract = {Several works have proposed Simplicity Bias (SB)---the tendency of standard training procedures such as Stochastic Gradient Descent (SGD) to find simple models---to justify why neural networks generalize well [Arpit et al. 2017, Nakkiran et al. 2019, Valle-Perez et al. 2019]. However, the precise notion of simplicity remains vague. Furthermore, previous settings [Soudry et al. 2018, Gunasekar et al. 2018] that use SB to theoretically justify why neural networks generalize well do not simultaneously capture the non-robustness of neural networks---a widely observed phenomenon in practice [Goodfellow et al. 2014, Jo and Bengio 2017]. We attempt to reconcile SB and the superior standard generalization of neural networks with the non-robustness observed in practice by introducing piecewise-linear and image-based datasets, which (a) incorporate a precise notion of simplicity,(b) comprise multiple predictive features with varying levels of simplicity, and (c) capture the non-robustness of neural networks trained on real data. Using theory and empirics on these datasets, we make four observations:(i) SB of SGD and variants can be extreme: neural networks can exclusively rely on the simplest feature and remain invariant to all predictive complex features.(ii) The extreme aspect of SB could explain why seemingly benign distribution shifts and small adversarial perturbations significantly degrade model performance.(iii) Contrary to conventional wisdom, SB can also hurt generalization on the same data distribution, as SB persists even when the simplest feature has less predictive power than the more complex features.(iv) Common approaches to …},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
year = {2020},
pages = {9573-9585},
month = dec,
url = {all_papers/ShahTRJN20.pdf}
}
@inproceedings{SahaKSVJ20,
title = {RNNPool: Efficient non-linear pooling for RAM constrained inference},
author = {Oindrila Saha, Aditya Kusupati, Harsha Vardhan Simhadri, Manik Varma, Prateek Jain},
abstract = {Standard Convolutional Neural Networks (CNNs) designed for computer vision tasks tend to have large intermediate activation maps. These require large working memory and are thus unsuitable for deployment on resource-constrained devices typically used for inference on the edge. Aggressively downsampling the images via pooling or strided convolutions can address the problem but leads to a significant decrease in accuracy due to gross aggregation of the feature map by standard pooling operators. In this paper, we introduce RNNPool, a novel pooling operator based on Recurrent Neural Networks (RNNs), that efficiently aggregates features over large patches of an image and rapidly downsamples activation maps. Empirical evaluation indicates that an RNNPool layer can effectively replace multiple blocks in a variety of architectures such as MobileNets, DenseNet when applied to standard vision tasks like image classification and face detection. That is, RNNPool can significantly decrease computational complexity and peak memory usage for inference while retaining comparable accuracy. We use RNNPool with the standard S3FD architecture to construct a face detection method that achieves state-of-the-art MAP for tiny ARM Cortex-M4 class microcontrollers with under 256 KB of RAM. Code is released at https://github. com/Microsoft/EdgeML.},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
year = {2020},
pages = {20473-20484},
month = dec,
url = {all_papers/SahaKSVJ20.pdf}
}
@inproceedings{GeJKKNN19,
title = {Open Problem: Do Good Algorithms Necessarily Query Bad Points?},
author = {Rong Ge, Prateek Jain, Sham M Kakade, Rahul Kidambi, Dheeraj M Nagaraj, Praneeth Netrapalli},
abstract = {Folklore results in the theory of Stochastic Approximation indicates the (minimax) optimality of Stochastic Gradient Descent (SGD)(Robbins and Monro, 1951) with polynomially decaying stepsizes and iterate averaging (Ruppert, 1988; Polyak and Juditsky, 1992) for classes of stochastic convex optimization. Basing of these folkore results and some recent developments, this manuscript considers a more subtle question: does any algorithm necessarily (information theoretically) have to query iterates that are sub-optimal infinitely often?},
booktitle = {Conference on Learning Theory},
year = {2019},
pages = {3190-3193},
month = jun,
url = {all_papers/GeJKKNN19.pdf}
}
@inproceedings{JainNN19,
title = {Making the last iterate of sgd information theoretically optimal},
author = {Prateek Jain, Dheeraj Nagaraj, Praneeth Netrapalli},
abstract = {Stochastic gradient descent (SGD) is one of the most widely used algorithms for large scale optimization problems. While classical theoretical analysis of SGD for convex problems studies (suffix)\emph {averages} of iterates and obtains information theoretically optimal bounds on suboptimality, the\emph {last point} of SGD is, by far, the most preferred choice in practice. The best known results for last point of SGD (Shamir and Zhang, 2013) however, are suboptimal compared to information theoretic lower bounds by a factor, where is the number of iterations. Harvey et. al (2018) shows that in fact, this additional factor is tight for standard step size sequences of $\OTheta {\frac {1}{\sqrt {t}}} $ and $\OTheta {\frac {1}{t}} $ for non-strongly convex and strongly convex settings, respectively. Similarly, even for subgradient descent (GD) when applied to non-smooth, convex functions, the best known step-size sequences still lead to -suboptimal convergence rates (on the final iterate). The main contribution of this work is to design new step size sequences that enjoy information theoretically optimal bounds on the suboptimality of\emph {last point} of SGD as well as GD. We achieve this by designing a modification scheme, that converts one sequence of step sizes to another so that the last point of SGD/GD with modified sequence has the same suboptimality guarantees as the average of SGD/GD with original sequence. We also show that our result holds with high-probability. We validate our results through simulations which demonstrate that the new step size sequence indeed improves the final iterate significantly compared to the …},
booktitle = {Conference on Learning Theory},
year = {2019},
pages = {1752-1755},
month = jun,
url = {all_papers/JainNN19.pdf}
}
@inproceedings{SuggalaBRJ19,
title = {Adaptive hard thresholding for near-optimal consistent robust regression},
author = {Arun Sai Suggala, Kush Bhatia, Pradeep Ravikumar, Prateek Jain},
abstract = {We study the problem of robust linear regression with response variable corruptions. We consider the oblivious adversary model, where the adversary corrupts a fraction of the responses in complete ignorance of the data. We provide a nearly linear time estimator which consistently estimates the true regression vector, even with fraction of corruptions. Existing results in this setting either don’t guarantee consistent estimates or can only handle a small fraction of corruptions. We also extend our estimator to robust sparse linear regression and show that similar guarantees hold in this setting. Finally, we apply our estimator to the problem of linear regression with heavy-tailed noise and show that our estimator consistently estimates the regression vector even when the noise has unbounded variance (eg, Cauchy distribution), for which most existing results don’t even apply. Our estimator is based on a novel variant of outlier removal via hard thresholding in which the threshold is chosen adaptively and crucially relies on randomness to escape bad fixed points of the non-convex hard thresholding operation.},
booktitle = {Conference on Learning Theory},
year = {2019},
pages = {2892-2897},
month = jun,
url = {all_papers/SuggalaBRJ19.pdf}
}
@inproceedings{NagarajJN19,
title = {Sgd without replacement: Sharper rates for general smooth convex functions},
author = {Dheeraj Nagaraj, Prateek Jain, Praneeth Netrapalli},
abstract = {We study stochastic gradient descent without replacement (SGDo) for smooth convex functions. SGDo is widely observed to converge faster than true SGD where each sample is drawn independently with replacement (Bottou, 2009) and hence, is more popular in practice. But it’s convergence properties are not well understood as sampling without replacement leads to coupling between iterates and gradients. By using method of exchangeable pairs to bound Wasserstein distance, we provide the first non-asymptotic results for SGDo when applied to general smooth, strongly-convex functions. In particular, we show that SGDo converges at a rate of while SGD is known to converge at rate, where denotes the number of passes over data and is required to be large enough. Existing results for SGDo in this setting require additional Hessian Lipschitz assumption (Gurbuzbalaban et al, 2015; HaoChen and Sra 2018). For small , we show SGDo can achieve same convergence rate as SGD for general smooth strongly-convex functions. Existing results in this setting require and hold only for generalized linear models (Shamir, 2016). In addition, by careful analysis of the coupling, for both large and small , we obtain better dependence on problem dependent parameters like condition number.},
booktitle = {International Conference on Machine Learning},
year = {2019},
pages = {4703-4711},
month = may,
url = {all_papers/NagarajJN19.pdf}
}