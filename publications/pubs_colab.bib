
@inproceedings{PalVMJTASS24,
title = {Sample-Efficient Personalization: Modeling User Parameters as Low Rank Plus Sparse Components},
author = {Soumyabrata Pal, Prateek Varshney, Gagan Madan, Prateek Jain, Abhradeep Thakurta, Gaurav Aggarwal, Pradeep Shenoy, Gaurav Srivastava},
abstract = {Personalization of machine learning (ML) predictions for individual users/domains/enterprises is critical for practical recommendation systems. Standard personalization approaches involve learning a user/domain specific\emph {embedding} that is fed into a fixed global model which can be limiting. On the other hand, personalizing/fine-tuning model itself for each user/domain—aka meta-learning—has high storage/infrastructure cost. Moreover, rigorous theoretical studies of scalable personalization approaches have been very limited. To address the above issues, we propose a novel meta-learning style approach that models network weights as a sum of low-rank and sparse components. This captures common information from multiple individuals/users together in the low-rank part while sparse part captures user-specific idiosyncrasies. We then study the framework in the linear setting, where the problem reduces to that of estimating the sum of a rank- and a -column sparse matrix using a small number of linear measurements. We propose a computationally efficient alternating minimization method with iterative hard thresholding—AMHT-LRS—to learn the low-rank and sparse part. Theoretically, for the realizable Gaussian data setting, we show that AMHT-LRS solves the problem efficiently with nearly optimal sample complexity. Finally, a significant challenge in personalization is ensuring privacy of each user’s sensitive data. We alleviate this problem by proposing a differentially private variant of our method that also is equipped with strong generalization guarantees.},
booktitle = {International Conference on Artificial Intelligence and Statistics},
year = {2024},
pages = {1702-1710},
url = {allpapers/PalVMJTASS24.pdf}
}
@article{LeeDRCCCHBKDLDASGKJJCN24,
title = {Gecko: Versatile text embeddings distilled from large language models},
author = {Jinhyuk Lee, Zhuyun Dai, Xiaoqi Ren, Blair Chen, Daniel Cer, Jeremy R Cole, Kai Hui, Michael Boratko, Rajvi Kapadia, Wen Ding, Yi Luan, Sai Meher Karthik Duddu, Gustavo Hernandez Abrego, Weiqiang Shi, Nithi Gupta, Aditya Kusupati, Prateek Jain, Siddhartha Reddy Jonnalagadda, Ming-Wei Chang, Iftekhar Naim},
abstract = {We present Gecko, a compact and versatile text embedding model. Gecko achieves strong retrieval performance by leveraging a key idea: distilling knowledge from large language models (LLMs) into a retriever. Our two-step distillation process begins with generating diverse, synthetic paired data using an LLM. Next, we further refine the data quality by retrieving a set of candidate passages for each query, and relabeling the positive and hard negative passages using the same LLM. The effectiveness of our approach is demonstrated by the compactness of the Gecko. On the Massive Text Embedding Benchmark (MTEB), Gecko with 256 embedding dimensions outperforms all existing entries with 768 embedding size. Gecko with 768 embedding dimensions achieves an average score of 66.31, competing with 7x larger models and 5x higher dimensional embeddings.},
journal = {arXiv preprint arXiv:2403.20327},
year = {2024},
pages = {},
url = {allpapers/LeeDRCCCHBKDLDASGKJJCN24.pdf}
}
@article{ThakurtaZJSRCKM24,
title = {Privacy-enhanced training and deployment of machine learning models using client-side and server-side data},
author = {Abhradeep Guha Thakurta, Li Zhang, Prateek Jain, Shuang Song, Steffen Rendle, Steve Shaw-Tang Chien, Walid Krichene, Yarong Mu},
abstract = {Computer-implemented systems and methods for training a decentralized model for making a personalized recommendation. In one aspect, the method comprising: obtaining, using user activity data, client-side training data that includes features and training labels; and training, by the client device, a decentralized model in training rounds, wherein training, in each training round comprises: receiving, first data including a current server-side embedding generated by the server-side machine learning model, wherein the first data received from the server does not include any server-side data used in generating the current server-side embedding; generating, using the client-side machine learning model, a client-side embedding based on the client-side training data; updating, using the client-side embedding and the current server-side embedding and based on the training labels, the client-side machine learning …},
journal = {},
year = {2024},
pages = {},
url = {allpapers/ThakurtaZJSRCKM24.pdf}
}
@article{YerramYBKJN24,
title = {HiRE: High Recall Approximate Top- Estimation for Efficient LLM Inference},
author = {Varun Yerram, Chong You, Srinadh Bhojanapalli, Sanjiv Kumar, Prateek Jain, Praneeth Netrapalli},
abstract = {Autoregressive decoding with generative Large Language Models (LLMs) on accelerators (GPUs/TPUs) is often memory-bound where most of the time is spent on transferring model parameters from high bandwidth memory (HBM) to cache. On the other hand, recent works show that LLMs can maintain quality with significant sparsity/redundancy in the feedforward (FFN) layers by appropriately training the model to operate on a top- fraction of rows/columns (where ), there by suggesting a way to reduce the transfer of model parameters, and hence latency. However, exploiting this sparsity for improving latency is hindered by the fact that identifying top rows/columns is data-dependent and is usually performed using full matrix operations, severely limiting potential gains. To address these issues, we introduce HiRE (High Recall Approximate Top-k Estimation). HiRE comprises of two novel components: (i) a compression scheme to cheaply predict top- rows/columns with high recall, followed by full computation restricted to the predicted subset, and (ii) DA-TOP-: an efficient multi-device approximate top- operator. We demonstrate that on a one billion parameter model, HiRE applied to both the softmax as well as feedforward layers, achieves almost matching pretraining and downstream accuracy, and speeds up inference latency by on a single TPUv5e device.},
journal = {arXiv preprint arXiv:2402.09360},
year = {2024},
pages = {},
url = {allpapers/YerramYBKJN24.pdf}
}
@article{NairSBKJN24,
title = {Tandem Transformers for Inference Efficient LLMs},
author = {Pranav Ajit Nair, Yashas Samaga, Toby Boyd, Sanjiv Kumar, Prateek Jain, Praneeth Netrapalli},
abstract = {The autoregressive nature of conventional large language models (LLMs) inherently limits inference speed, as tokens are generated sequentially. While speculative and parallel decoding techniques attempt to mitigate this, they face limitations: either relying on less accurate smaller models for generation or failing to fully leverage the base LLM's representations. We introduce a novel architecture, Tandem transformers, to address these issues. This architecture uniquely combines (1) a small autoregressive model and (2) a large model operating in block mode (processing multiple tokens simultaneously). The small model's predictive accuracy is substantially enhanced by granting it attention to the large model's richer representations. On the PaLM2 pretraining dataset, a tandem of PaLM2-Bison and PaLM2-Gecko demonstrates a 3.3% improvement in next-token prediction accuracy over a standalone PaLM2-Gecko, offering a 1.16x speedup compared to a PaLM2-Otter model with comparable downstream performance. We further incorporate the tandem model within the speculative decoding (SPEED) framework where the large model validates tokens from the small model. This ensures that the Tandem of PaLM2-Bison and PaLM2-Gecko achieves substantial speedup (around 1.14x faster than using vanilla PaLM2-Gecko in SPEED) while maintaining identical downstream task accuracy.},
journal = {arXiv preprint arXiv:2402.08644},
year = {2024},
pages = {},
url = {allpapers/NairSBKJN24.pdf}
}
@article{LiuJKOS24,
title = {Label Robust and Differentially Private Linear Regression: Computational and Statistical Efficiency},
author = {Xiyang Liu, Prateek Jain, Weihao Kong, Sewoong Oh, Arun Suggala},
abstract = {We study the canonical problem of linear regression under -differential privacy when the datapoints are sampled iid~ from a distribution and a fraction of response variables are adversarially corrupted. We provide the first provably efficient--both computationally and statistically--method for this problem, assuming standard assumptions on the data distribution. Our algorithm is a variant of the popular differentially private stochastic gradient descent (DP-SGD) algorithm with two key innovations: a full-batch gradient descent to improve sample complexity and a novel adaptive clipping to guarantee robustness. Our method requires only linear time in input size, and still matches the information theoretical optimal sample complexity up to a data distribution dependent condition number factor. Interestingly, the same algorithm, when applied to a setting where there is no adversarial corruption, still improves upon the existing state-of-the-art and achieves a near optimal sample complexity.},
journal = {Advances in Neural Information Processing Systems},
year = {2024},
pages = {},
url = {allpapers/LiuJKOS24.pdf}
}
@article{PalSSJ24,
title = {Blocked Collaborative Bandits: Online Collaborative Filtering with Per-Item Budget Constraints},
author = {Soumyabrata Pal, Arun Suggala, Karthikeyan Shanmugam, Prateek Jain},
abstract = {We consider the problem of\emph {blocked} collaborative bandits where there are multiple users, each with an associated multi-armed bandit problem. These users are grouped into\emph {latent} clusters such that the mean reward vectors of users within the same cluster are identical. Our goal is to design algorithms that maximize the cumulative reward accrued by all the users over time, under the\emph {constraint} that no arm of a user is pulled more than times. This problem has been originally considered by\cite {Bresler: 2014}, and designing regret-optimal algorithms for it has since remained an open problem. In this work, we propose an algorithm called B-LATTICE (Blocked Latent bAndiTs via maTrIx ComplEtion) that collaborates across users, while simultaneously satisfying the budget constraints, to maximize their cumulative rewards. Theoretically, under certain reasonable assumptions on the latent structure, with users, arms, rounds per user, and latent clusters, B-LATTICE achieves a per-user regret of under a budget constraint of . These are the first sub-linear regret bounds for this problem, and match the minimax regret bounds when . Empirically, we demonstrate that our algorithm has superior performance over baselines even when . B-LATTICE is a phased algorithm where in each phase it clusters users into groups and collaborates across users within a group to quickly learn their reward models.},
journal = {Advances in Neural Information Processing Systems},
year = {2024},
pages = {},
url = {allpapers/PalSSJ24.pdf}
}
@article{RegeKFCKJF24,
title = {Adanns: A framework for adaptive semantic search},
author = {Aniket Rege, Aditya Kusupati, Alan Fan, Qingqing Cao, Sham Kakade, Prateek Jain, Ali Farhadi},
abstract = {Web-scale search systems learn an encoder to embed a given query which is then hooked into an approximate nearest neighbor search (ANNS) pipeline to retrieve similar data points. To accurately capture tail queries and data points, learned representations typically are _rigid, high-dimensional_ vectors that are generally used as-is in the entire ANNS pipeline and can lead to computationally expensive retrieval. In this paper, we argue that instead of rigid representations, different stages of ANNS can leverage _adaptive representations_ of varying capacities to achieve significantly better accuracy-compute trade-offs, ie, stages of ANNS that can get away with more approximate computation should use a lower-capacity representation of the same data point. To this end, we introduce AdANNS, a novel ANNS design framework that explicitly leverages the flexibility of Matryoshka Representations. We demonstrate state-of-the-art accuracy-compute trade-offs using novel AdANNS-based key ANNS building blocks like search data structures (AdANNS-IVF) and quantization (AdANNS-OPQ). For example on ImageNet retrieval, AdANNS-IVF is up to % more accurate than the rigid representations-based IVF at the same compute budget; and matches accuracy while being up to faster in _wall-clock time_. For Natural Questions, -byte AdANNS-OPQ matches the accuracy of the -byte OPQ baseline constructed using rigid representations--_same accuracy at half the cost! _ We further show that the gains from AdANNS translate to modern-day composite ANNS indices that combine search structures and quantization. Finally, we demonstrate that …},
journal = {Advances in Neural Information Processing Systems},
year = {2024},
pages = {},
url = {allpapers/RegeKFCKJF24.pdf}
}
@article{MorwaniBJN24,
title = {Simplicity bias in 1-hidden layer neural networks},
author = {Depen Morwani, Jatin Batra, Prateek Jain, Praneeth Netrapalli},
abstract = {Recent works have demonstrated that neural networks exhibit extreme* simplicity bias*(SB). That is, they learn* only the simplest* features to solve a task at hand, even in the presence of other, more robust but more complex features. Due to the lack of a general and rigorous definition of* features*, these works showcase SB on* semi-synthetic* datasets such as Color-MNIST, MNIST-CIFAR where defining features is relatively easier. In this work, we rigorously define as well as thoroughly establish SB for* one hidden layer* neural networks in the infinite width regime. More concretely,(i) we define SB as the network essentially being a function of a low dimensional projection of the inputs (ii) theoretically, we show that when the data is linearly separable, the network primarily depends on only the linearly separable (-dimensional) subspace even in the presence of an arbitrarily large number of other, more complex features which could have led to a significantly more robust classifier,(iii) empirically, we show that models trained on* real* datasets such as Imagenet and Waterbirds-Landbirds indeed depend on a low dimensional projection of the inputs, thereby demonstrating SB on these datasets, iv) finally, we present a natural ensemble approach that encourages diversity in models by training successive models on features not used by earlier models, and demonstrate that it yields models that are significantly more robust to Gaussian noise.},
journal = {Advances in Neural Information Processing Systems},
year = {2024},
pages = {},
url = {allpapers/MorwaniBJN24.pdf}
}
@article{NairSBKJN24,
title = {Tandem Transformers for Inference Efficient LLMs},
author = {Pranav Ajit Nair, Yashas Samaga, Toby Boyd, Sanjiv Kumar, Prateek Jain, Praneeth Netrapalli},
abstract = {The autoregressive nature of conventional large language models (LLMs) inherently limits inference speed, as tokens are generated sequentially. While speculative and parallel decoding techniques attempt to mitigate this, they face limitations: either relying on less accurate smaller models for generation or failing to fully leverage the base LLM's representations. We introduce a novel architecture, Tandem transformers, to address these issues. This architecture uniquely combines (1) a small autoregressive model and (2) a large model operating in block mode (processing multiple tokens simultaneously). The small model's predictive accuracy is substantially enhanced by granting it attention to the large model's richer representations. On the PaLM2 pretraining dataset, a tandem of PaLM2-Bison and PaLM2-Gecko demonstrates a 3.3% improvement in next-token prediction accuracy over a standalone PaLM2-Gecko …},
journal = {arXiv e-prints},
year = {2024},
pages = {arXiv: 2402.08644},
url = {allpapers/NairSBKJN24.pdf}
}
@article{MenonBHJSCYMLDD24,
title = {Multiple-entity-based recommendation system},
author = {Lekshmi Menon, Amar Budhiraja, Gaurush Hiranandani, Prateek Jain, Darshatkumar Anandji Shah, Ayush Choure, Navya Yarrabelly, Anurag Mishra, Mohammad Luqman, Shivangi Dhakad, Juhi Dua},
abstract = {Systems and methods for entity recommendation can make use of rich data by allowing the items to be recommended and the recipients of the recommendation (eg, users) to be modeled as “complex entities” composed of one or more static sub-entities and/or a dynamic component, and by utilizing information about multiple relationships between the sub-entities as reflected in bipartite graphs. Generating recommendations from such information may involve creating vector representations of the sub-entities based on the bipartite graphs (eg, using graph-based convolutional networks), and combining these vector representations into representations of the items and users (or other recipients) to be fed into a classifier model.},
journal = {},
year = {2024},
pages = {},
url = {allpapers/MenonBHJSCYMLDD24.pdf}
}
@article{BansalSDGVGBJT24,
title = {Llm augmented llms: Expanding capabilities through composition},
author = {Rachit Bansal, Bidisha Samanta, Siddharth Dalmia, Nitish Gupta, Shikhar Vashishth, Sriram Ganapathy, Abhishek Bapna, Prateek Jain, Partha Talukdar},
abstract = {Foundational models with billions of parameters which have been trained on large corpora of data have demonstrated non-trivial skills in a variety of domains. However, due to their monolithic structure, it is challenging and expensive to augment them or impart new skills. On the other hand, due to their adaptation abilities, several new instances of these models are being trained towards new domains and tasks. In this work, we study the problem of efficient and practical composition of existing foundation models with more specific models to enable newer capabilities. To this end, we propose CALM -- Composition to Augment Language Models -- which introduces cross-attention between models to compose their representations and enable new capabilities. Salient features of CALM are: (i) Scales up LLMs on new tasks by 're-using' existing LLMs along with a few additional parameters and data, (ii) Existing model weights are kept intact, and hence preserves existing capabilities, and (iii) Applies to diverse domains and settings. We illustrate that augmenting PaLM2-S with a smaller model trained on low-resource languages results in an absolute improvement of up to 13\% on tasks like translation into English and arithmetic reasoning for low-resource languages. Similarly, when PaLM2-S is augmented with a code-specific model, we see a relative improvement of 40\% over the base model for code generation and explanation tasks -- on-par with fully fine-tuned counterparts.},
journal = {arXiv preprint arXiv:2401.02412},
year = {2024},
pages = {},
url = {allpapers/BansalSDGVGBJT24.pdf}
}
@article{JainMJB24,
title = {Attention neural networks with tree attention mechanisms},
author = {Himanshu Jain, Lovish Madaan, Prateek Jain, Venkata Sesha Pavana Srinadh Bhojanapalli},
abstract = {Systems and methods for processing inputs using attention neural networks with tree attention layers. Each tree attention layer includes one or more tree attention sub-layers that are each configured to: process query vectors using a decision tree model for the tree attention sub-layer to determine a respective tree path for each query vector; process key vectors using the decision tree model to determine a respective tree path for each key vector; and generate an attended input sequence comprising a respective attended input at each of the plurality of input positions, comprising: generating, for each particular input position, the respective attended input at the particular input position based on (i) the tree path for the query vector at the particular input position (ii) the respective tree paths for the key vectors at each of the plurality of input positions and (iii) the value vectors at a subset of the input positions.},
journal = {},
year = {2024},
pages = {},
url = {allpapers/JainMJB24.pdf}
}
@inproceedings{DevvritKKDCDTHKFJ23,
title = {Matformer: Nested transformer for elastic inference},
author = {Fnu Devvrit, Sneha Kudugunta, Aditya Kusupati, Tim Dettmers, Kaifeng Chen, Inderjit S Dhillon, Yulia Tsvetkov, Hannaneh Hajishirzi, Sham M Kakade, Ali Farhadi, Prateek Jain},
abstract = {Transformer models are deployed in a wide range of settings, from multi-accelerator clusters to standalone mobile phones. The diverse inference constraints in these scenarios necessitate practitioners to train foundation models such as PaLM 2, Llama, & ViTs as a series of models of varying sizes. Due to significant training costs, only a select few model sizes are trained and supported, limiting more fine-grained control over relevant tradeoffs, including latency, cost, and accuracy. This work introduces MatFormer, a nested Transformer architecture designed to offer elasticity in a variety of deployment constraints. Each Feed Forward Network (FFN) block of a MatFormer model is jointly optimized with a few nested smaller FFN blocks. This training procedure allows for the Mix'n'Match of model granularities across layers -- i.e., a trained universal MatFormer model enables extraction of hundreds of accurate smaller models, which were never explicitly optimized. We empirically demonstrate MatFormer's effectiveness across different model classes (decoders & encoders), modalities (language & vision), and scales (up to 2.6B parameters). We find that a 2.6B decoder-only MatFormer language model (MatLM) allows us to extract smaller models spanning from 1.5B to 2.6B, each exhibiting comparable validation loss and one-shot downstream evaluations to their independently trained counterparts. Furthermore, we observe that smaller encoders extracted from a universal MatFormer-based ViT (MatViT) encoder preserve the metric-space structure for adaptive large-scale retrieval. Finally, we showcase that speculative decoding with the accurate and …},
booktitle = {Workshop on Advancing Neural Network Training: Computational Efficiency, Scalability, and Resource Optimization (WANT@ NeurIPS 2023)},
year = {2023},
pages = {},
url = {allpapers/DevvritKKDCDTHKFJ23.pdf}
}
@article{GuptaKRBJD23,
title = {Efficacy of dual-encoders for extreme multi-label classification},
author = {Nilesh Gupta, Devvrit Khatri, Ankit S Rawat, Srinadh Bhojanapalli, Prateek Jain, Inderjit S Dhillon},
abstract = {Dual-encoder models have demonstrated significant success in dense retrieval tasks for open-domain question answering that mostly involves zero-shot and few-shot scenarios. However, their performance in many-shot retrieval problems where training data is abundant, such as extreme multi-label classification (XMC), remains under-explored. Existing empirical evidence suggests that, for such problems, the dual-encoder method's accuracies lag behind the performance of state-of-the-art (SOTA) extreme classification methods that grow the number of learnable parameters linearly with the number of classes. As a result, some recent extreme classification techniques use a combination of dual-encoders and a learnable classification head for each class to excel on these tasks. In this paper, we investigate the potential of "pure" DE models in XMC tasks. Our findings reveal that when trained correctly standard dual-encoders can match or outperform SOTA extreme classification methods by up to 2% at Precision@1 even on the largest XMC datasets while being 20x smaller in terms of the number of trainable parameters. We further propose a differentiable topk error-based loss function, which can be used to specifically optimize for Recall@k metrics. We include our PyTorch implementation along with other resources for reproducing the results in the supplementary material.},
journal = {arXiv preprint arXiv:2310.10636},
year = {2023},
pages = {},
url = {allpapers/GuptaKRBJD23.pdf}
}
@article{KumarMGKDJ23,
title = {EHI: End-to-end Learning of Hierarchical Index for Efficient Dense Retrieval},
author = {Ramnath Kumar, Anshul Mittal, Nilesh Gupta, Aditya Kusupati, Inderjit Dhillon, Prateek Jain},
abstract = {Dense embedding-based retrieval is now the industry standard for semantic search and ranking problems, like obtaining relevant web documents for a given query. Such techniques use a two-stage process: (a) contrastive learning to train a dual encoder to embed both the query and documents and (b) approximate nearest neighbor search (ANNS) for finding similar documents for a given query. These two stages are disjoint; the learned embeddings might be ill-suited for the ANNS method and vice-versa, leading to suboptimal performance. In this work, we propose End-to-end Hierarchical Indexing -- EHI -- that jointly learns both the embeddings and the ANNS structure to optimize retrieval performance. EHI uses a standard dual encoder model for embedding queries and documents while learning an inverted file index (IVF) style tree structure for efficient ANNS. To ensure stable and efficient learning of discrete tree-based ANNS structure, EHI introduces the notion of dense path embedding that captures the position of a query/document in the tree. We demonstrate the effectiveness of EHI on several benchmarks, including de-facto industry standard MS MARCO (Dev set and TREC DL19) datasets. For example, with the same compute budget, EHI outperforms state-of-the-art (SOTA) in by 0.6% (MRR@10) on MS MARCO dev set and by 4.2% (nDCG@10) on TREC DL19 benchmarks.},
journal = {arXiv preprint arXiv:2310.08891},
year = {2023},
pages = {},
url = {allpapers/KumarMGKDJ23.pdf}
}
@article{KuduguntaKDCDTHKFJ23,
title = {MatFormer: Nested Transformer for Elastic Inference},
author = {Sneha Kudugunta, Aditya Kusupati, Tim Dettmers, Kaifeng Chen, Inderjit Dhillon, Yulia Tsvetkov, Hannaneh Hajishirzi, Sham Kakade, Ali Farhadi, Prateek Jain},
abstract = {Transformer models are deployed in a wide range of settings, from multi-accelerator clusters to standalone mobile phones. The diverse inference constraints in these scenarios necessitate practitioners to train foundation models such as PaLM 2, Llama, & ViTs as a series of models of varying sizes. Due to significant training costs, only a select few model sizes are trained and supported, limiting more fine-grained control over relevant tradeoffs, including latency, cost, and accuracy. This work introduces MatFormer, a nested Transformer architecture designed to offer elasticity in a variety of deployment constraints. Each Feed Forward Network (FFN) block of a MatFormer model is jointly optimized with a few nested smaller FFN blocks. This training procedure allows for the Mix'n'Match of model granularities across layers -- i.e., a trained universal MatFormer model enables extraction of hundreds of accurate smaller models, which were never explicitly optimized. We empirically demonstrate MatFormer's effectiveness across different model classes (decoders & encoders), modalities (language & vision), and scales (up to 2.6B parameters). We find that a 2.6B decoder-only MatFormer language model (MatLM) allows us to extract smaller models spanning from 1.5B to 2.6B, each exhibiting comparable validation loss and one-shot downstream evaluations to their independently trained counterparts. Furthermore, we observe that smaller encoders extracted from a universal MatFormer-based ViT (MatViT) encoder preserve the metric-space structure for adaptive large-scale retrieval. Finally, we showcase that speculative decoding with the accurate and …},
journal = {arXiv preprint arXiv:2310.07707},
year = {2023},
pages = {},
url = {allpapers/KuduguntaKDCDTHKFJ23.pdf}
}
@inproceedings{KricheneJSSTZ23,
title = {Multi-task differential privacy under distribution skew},
author = {Walid Krichene, Prateek Jain, Shuang Song, Mukund Sundararajan, Abhradeep Guha Thakurta, Li Zhang},
abstract = {We study the problem of multi-task learning under user-level differential privacy, in which n users contribute data to m tasks, each involving a subset of users. One important aspect of the problem, that can significantly impact quality, is the distribution skew among tasks. Tasks that have much fewer data samples than others are more susceptible to the noise added for privacy. It is natural to ask whether algorithms can adapt to this skew to improve the overall utility. We give a systematic analysis of the problem, by studying how to optimally allocate a user’s privacy budget among tasks. We propose a generic algorithm, based on an adaptive reweighting of the empirical loss, and show that in the presence of distribution skew, this gives a quantifiable improvement of excess empirical risk. Experimental studies on recommendation problems that exhibit a long tail of small tasks, demonstrate that our methods significantly improve utility, achieving the state of the art on two standard benchmarks.},
booktitle = {International Conference on Machine Learning},
year = {2023},
pages = {17784-17807},
url = {allpapers/KricheneJSSTZ23.pdf}
}
@inproceedings{NagarajKANJ23,
title = {Multi-user reinforcement learning with low rank rewards},
author = {Dheeraj Mysore Nagaraj, Suhas S Kowshik, Naman Agarwal, Praneeth Netrapalli, Prateek Jain},
abstract = {We consider collaborative multi-user reinforcement learning, where multiple users have the same state-action space and transition probabilities but different rewards. Under the assumption that the reward matrix of the users has a low-rank structure–a standard and practically successful assumption in the collaborative filtering setting–we design algorithms with significantly lower sample complexity compared to the ones that learn the MDP individually for each user. Our main contribution is an algorithm which explores rewards collaboratively with user-specific MDPs and can learn rewards efficiently in two key settings: tabular MDPs and linear MDPs. When is large and the rank is constant, the sample complexity per MDP depends logarithmically over the size of the state-space, which represents an exponential reduction (in the state-space size) when compared to the standard “non-collaborative” algorithms. Our main technical contribution is a method to construct policies which obtain data such that low rank matrix completion is possible (without a generative model). This goes beyond the regular RL framework and is closely related to mean field limits of multi-agent RL.},
booktitle = {International Conference on Machine Learning},
year = {2023},
pages = {25627-25659},
url = {allpapers/NagarajKANJ23.pdf}
}
@article{NaserySSJ23,
title = {End-to-End Neural Network Compression via Regularized Latency Surrogates},
author = {Anshul Nasery, Hardik Shah, Arun Sai Suggala, Prateek Jain},
abstract = {Neural network (NN) compression via techniques such as pruning, quantization requires setting compression hyperparameters (e.g., number of channels to be pruned, bitwidths for quantization) for each layer either manually or via neural architecture search (NAS) which can be computationally expensive. We address this problem by providing an end-to-end technique that optimizes for model's Floating Point Operations (FLOPs) or for on-device latency via a novel latency surrogate. Our algorithm is versatile and can be used with many popular compression methods including pruning, low-rank factorization, and quantization. Crucially, it is fast and runs in almost the same amount of time as single model training; which is a significant training speed-up over standard NAS methods. For BERT compression on GLUE fine-tuning tasks, we achieve reduction in FLOPs with only drop in performance. For compressing MobileNetV3 on ImageNet-1K, we achieve reduction in FLOPs, and reduction in on-device latency without drop in accuracy, while still requiring less training compute than SOTA compression techniques. Finally, for transfer learning on smaller datasets, our technique identifies - cheaper architectures than standard MobileNetV3, EfficientNet suite of architectures at almost the same training cost and accuracy.},
journal = {arXiv preprint arXiv:2306.05785},
year = {2023},
pages = {},
url = {allpapers/NaserySSJ23.pdf}
}
@inproceedings{PalSSJ23,
title = {Optimal algorithms for latent bandits with cluster structure},
author = {Soumyabrata Pal, Arun Sai Suggala, Karthikeyan Shanmugam, Prateek Jain},
abstract = {We consider the problem of latent bandits with cluster structure where there are multiple users, each with an associated multi-armed bandit problem. These users are grouped into latent clusters such that the mean reward vectors of users within the same cluster are identical. At each round, a user, selected uniformly at random, pulls an arm and observes a corresponding noisy reward. The goal of the users is to maximize their cumulative rewards. This problem is central to practical recommendation systems and has received wide attention of late Gentile et al.(2014), Maillard and Mannor (2014). Now, if each user acts independently, then they would have to explore each arm independently and a regret of is unavoidable, where M, N are the number of arms and users, respectively. Instead, we propose LATTICE (Latent bAndiTs via maTrIx ComplEtion) which allows exploration of the latent cluster structure to provide the minimax optimal regret of when the number of clusters is . This is the first algorithm to guarantee such strong regret bound. LATTICE is based on a careful exploitation of arm information within a cluster while simultaneously clustering users. Furthermore, it is computationally efficient and requires only calls to an offline matrix completion oracle across all T rounds.},
booktitle = {International Conference on Artificial Intelligence and Statistics},
year = {2023},
pages = {7540-7577},
url = {allpapers/PalSSJ23.pdf}
}
@article{LiuJKOS23,
title = {Near optimal private and robust linear regression},
author = {Xiyang Liu, Prateek Jain, Weihao Kong, Sewoong Oh, Arun Sai Suggala},
abstract = {We study the canonical statistical estimation problem of linear regression from i.i.d.~examples under -differential privacy when some response variables are adversarially corrupted. We propose a variant of the popular differentially private stochastic gradient descent (DP-SGD) algorithm with two innovations: a full-batch gradient descent to improve sample complexity and a novel adaptive clipping to guarantee robustness. When there is no adversarial corruption, this algorithm improves upon the existing state-of-the-art approach and achieves a near optimal sample complexity. Under label-corruption, this is the first efficient linear regression algorithm to guarantee both -DP and robustness. Synthetic experiments confirm the superiority of our approach.},
journal = {arXiv preprint arXiv:2301.13273},
year = {2023},
pages = {},
url = {allpapers/LiuJKOS23.pdf}
}
@article{NaseryNJA23,
title = {LEARNING AN INVERTIBLE OUTPUT MAPPING CAN MITIGATE SIMPLICITY BIAS IN NEURAL NETWORKS},
author = {Anshul Nasery, Praneeth Netrapalli, Prateek Jain, Sravanti Addepalli},
abstract = {Deep Neural Networks (DNNs) are known to be brittle to even minor distribution shifts compared to the training distribution. Simplicity Bias (SB) of DNNs–bias towards learning a small number of simplest features–has been demonstrated to be a key reason for this brittleness. Prior works have shown that the effect of Simplicity Bias is extreme–even when the features learned are diverse, training the classification head again selects only few of the simplest features, leading to similarly brittle models. In this work, we introduce Feature Reconstruction Regularizer (FRR) in the linear classification head, with the aim of reducing Simplicity Bias, thereby improving Out-Of-Distribution (OOD) robustness. The proposed regularizer when used during linear layer training, termed as FRR-L, enforces that the features can be reconstructed back from the logit layer, ensuring that diverse features participate in the classification task …},
journal = {},
year = {2023},
pages = {},
url = {allpapers/NaseryNJA23.pdf}
}
@article{KusupatiBRWSRHCKJF22,
title = {Matryoshka representation learning},
author = {Aditya Kusupati, Gantavya Bhatt, Aniket Rege, Matthew Wallingford, Aditya Sinha, Vivek Ramanujan, William Howard-Snyder, Kaifeng Chen, Sham Kakade, Prateek Jain, Ali Farhadi},
abstract = {Learned representations are a central component in modern ML systems, serving a multitude of downstream tasks. When training such representations, it is often the case that computational and statistical constraints for each downstream task are unknown. In this context rigid, fixed capacity representations can be either over or under-accommodating to the task at hand. This leads us to ask: can we design a flexible representation that can adapt to multiple downstream tasks with varying computational resources? Our main contribution is Matryoshka Representation Learning (MRL) which encodes information at different granularities and allows a single embedding to adapt to the computational constraints of downstream tasks. MRL minimally modifies existing representation learning pipelines and imposes no additional cost during inference and deployment. MRL learns coarse-to-fine representations that are at least as accurate and rich as independently trained low-dimensional representations. The flexibility within the learned Matryoshka Representations offer:(a) up to smaller embedding size for ImageNet-1K classification at the same level of accuracy;(b) up to real-world speed-ups for large-scale retrieval on ImageNet-1K and 4K; and (c) up to accuracy improvements for long-tail few-shot classification, all while being as robust as the original representations. Finally, we show that MRL extends seamlessly to web-scale datasets (ImageNet, JFT) across various modalities--vision (ViT, ResNet), vision+ language (ALIGN) and language (BERT). MRL code and pretrained models are open-sourced at https://github. com/RAIVNLab/MRL.},
journal = {Advances in Neural Information Processing Systems},
year = {2022},
pages = {30233-30249},
url = {allpapers/KusupatiBRWSRHCKJF22.pdf}
}
@article{DevvritSDJ22,
title = {S3GC: scalable self-supervised graph clustering},
author = {Fnu Devvrit, Aditya Sinha, Inderjit Dhillon, Prateek Jain},
abstract = {We study the problem of clustering graphs with additional side-information of node features. The problem is extensively studied, and several existing methods exploit Graph Neural Networks to learn node representations. However, most of the existing methods focus on generic representations instead of their cluster-ability or do not scale to large scale graph datasets. In this work, we propose S3GC which uses contrastive learning along with Graph Neural Networks and node features to learn clusterable features. We empirically demonstrate that S3GC is able to learn the correct cluster structure even when graph information or node features are individually not informative enough to learn correct clusters. Finally, using extensive evaluation on a variety of benchmarks, we demonstrate that S3GC is able to significantly outperform state-of-the-art methods in terms of clustering accuracy--with as much as 5% gain in NMI--while being scalable to graphs of size 100M.},
journal = {Advances in Neural Information Processing Systems},
year = {2022},
pages = {3248-3261},
url = {allpapers/DevvritSDJ22.pdf}
}
@article{LiuKJO22,
title = {DP-PCA: Statistically optimal and differentially private PCA},
author = {Xiyang Liu, Weihao Kong, Prateek Jain, Sewoong Oh},
abstract = {We study the canonical statistical task of computing the principal component from iid~ data under differential privacy. Although extensively studied in literature, existing solutions fall short on two key aspects:() even for Gaussian data, existing private algorithms require the number of samples to scale super-linearly with , ie, , to obtain non-trivial results while non-private PCA requires only , and () existing techniques suffer from a large error even when the variance in each data point is small. We propose DP-PCA method that uses a single-pass minibatch gradient descent style algorithm to overcome the above limitations. For sub-Gaussian data, we provide nearly optimal statistical error rates even for .},
journal = {Advances in neural information processing systems},
year = {2022},
pages = {29929-29943},
url = {allpapers/LiuKJO22.pdf}
}
@article{AhnJJKNS22,
title = {Reproducibility in optimization: Theoretical framework and limits},
author = {Kwangjun Ahn, Prateek Jain, Ziwei Ji, Satyen Kale, Praneeth Netrapalli, Gil I Shamir},
abstract = {We initiate a formal study of reproducibility in optimization. We define a quantitative measure of reproducibility of optimization procedures in the face of noisy or error-prone operations such as inexact or stochastic gradient computations or inexact initialization. We then analyze several convex optimization settings of interest such as smooth, non-smooth, and strongly-convex objective functions and establish tight bounds on the limits of reproducibility in each setting. Our analysis reveals a fundamental trade-off between computation and reproducibility: more computation is necessary (and sufficient) for better reproducibility.},
journal = {Advances in Neural Information Processing Systems},
year = {2022},
pages = {18022-18033},
url = {allpapers/AhnJJKNS22.pdf}
}
@article{NandySJCRR22,
title = {Domain-agnostic contrastive representations for learning from label proportions},
author = {Jay Nandy, Rishi Saket, Prateek Jain, Jatin Chauhan, Balaraman Ravindran, Aravindan Raghuveer},
abstract = {We study the weak supervision learning problem of Learning from Label Proportions (LLP) where the goal is to learn an instance-level classifier using proportions of various class labels in a bag -- a collection of input instances that often can be highly correlated. While representation learning for weakly-supervised tasks is found to be effective, they often require domain knowledge. To the best of our knowledge, representation learning for tabular data (unstructured data containing both continuous and categorical features) are not studied. In this paper, we propose to learn diverse representations of instances within the same bags to effectively utilize the weak bag-level supervision. We propose a domain agnostic LLP method, called "Self Contrastive Representation Learning for LLP" (SelfCLR-LLP) that incorporates a novel self--contrastive function as an auxiliary loss to learn representations on tabular data for LLP …},
journal = {},
year = {2022},
pages = {1542-1551},
url = {allpapers/NandySJCRR22.pdf}
}
@article{AddepalliNBNJ22,
title = {Learning an invertible output mapping can mitigate simplicity bias in neural networks},
author = {Sravanti Addepalli, Anshul Nasery, R Venkatesh Babu, Praneeth Netrapalli, Prateek Jain},
abstract = {Deep Neural Networks are known to be brittle to even minor distribution shifts compared to the training distribution. While one line of work has demonstrated that Simplicity Bias (SB) of DNNs - bias towards learning only the simplest features - is a key reason for this brittleness, another recent line of work has surprisingly found that diverse/ complex features are indeed learned by the backbone, and their brittleness is due to the linear classification head relying primarily on the simplest features. To bridge the gap between these two lines of work, we first hypothesize and verify that while SB may not altogether preclude learning complex features, it amplifies simpler features over complex ones. Namely, simple features are replicated several times in the learned representations while complex features might not be replicated. This phenomenon, we term Feature Replication Hypothesis, coupled with the Implicit Bias of SGD to converge to maximum margin solutions in the feature space, leads the models to rely mostly on the simple features for classification. To mitigate this bias, we propose Feature Reconstruction Regularizer (FRR) to ensure that the learned features can be reconstructed back from the logits. The use of {\em FRR} in linear layer training (FRR-L) encourages the use of more diverse features for classification. We further propose to finetune the full network by freezing the weights of the linear layer trained using FRR-L, to refine the learned features, making them more suitable for classification. Using this simple solution, we demonstrate up to 15% gains in OOD accuracy on the recently introduced semi-synthetic datasets with extreme …},
journal = {arXiv preprint arXiv:2210.01360},
year = {2022},
pages = {},
url = {allpapers/AddepalliNBNJ22.pdf}
}
@inproceedings{AddepalliNRNJ22,
title = {Feature reconstruction from outputs can mitigate simplicity bias in neural networks},
author = {Sravanti Addepalli, Anshul Nasery, Venkatesh Babu Radhakrishnan, Praneeth Netrapalli, Prateek Jain},
abstract = {Deep Neural Networks are known to be brittle to even minor distribution shifts compared to the training distribution. While one line of work has demonstrated that \emph{Simplicity Bias} (SB) of DNNs -- bias towards learning only the simplest features -- is a key reason for this brittleness, another recent line of work has surprisingly found that diverse/ complex features are indeed learned by the backbone, and their brittleness is due to the linear classification head relying primarily on the simplest features. To bridge the gap between these two lines of work, we first hypothesize and verify that while SB may not altogether preclude learning complex features, it amplifies simpler features over complex ones. Namely, simple features are replicated several times in the learned representations while complex features might not be replicated. This phenomenon, we term \emph{Feature Replication Hypothesis}, coupled with the \emph{Implicit Bias} of SGD to converge to maximum margin solutions in the feature space, leads the models to rely mostly on the simple features for classification. To mitigate this bias, we propose \emph{Feature Reconstruction Regularizer (FRR)} to ensure that the learned features can be reconstructed back from the logits. The use of \emph{FRR} in linear layer training (\emph{FRR-L}) encourages the use of more diverse features for classification. We further propose to finetune the full network by freezing the weights of the linear layer trained using \emph{FRR-L}, to refine the learned features, making them more suitable for classification. Using this simple solution, we demonstrate up to 15\% gains in OOD accuracy on the recently …},
booktitle = {The Eleventh International Conference on Learning Representations},
year = {2022},
pages = {},
url = {allpapers/AddepalliNRNJ22.pdf}
}
@article{PalVMTASSJ22,
title = {Private and Efficient Meta-Learning with Low Rank and Sparse decomposition},
author = {Soumyabrata Pal, Prateek Varshney, Gagan Madan, Abhradeep Guha Thakurta, Gaurav Aggarwal, Pradeep Shenoy, Gaurav Srivastava, Prateek Jain},
abstract = {Meta-learning is critical for a variety of practical ML systems -- like personalized recommendations systems -- that are required to generalize to new tasks despite a small number of task-specific training points. Existing meta-learning techniques use two complementary approaches of either learning a low-dimensional representation of points for all tasks, or task-specific fine-tuning of a global model trained using all the tasks. In this work, we propose a novel meta-learning framework that combines both the techniques to enable handling of a large number of data-starved tasks. Our framework models network weights as a sum of low-rank and sparse matrices. This allows us to capture information from multiple domains together in the low-rank part while still allowing task specific personalization using the sparse part. We instantiate and study the framework in the linear setting, where the problem reduces to that of estimating the sum of a rank- and a -column sparse matrix using a small number of linear measurements. We propose an alternating minimization method with hard thresholding -- AMHT-LRS -- to learn the low-rank and sparse part effectively and efficiently. For the realizable, Gaussian data setting, we show that AMHT-LRS indeed solves the problem efficiently with nearly optimal samples. We extend AMHT-LRS to ensure that it preserves privacy of each individual user in the dataset, while still ensuring strong generalization with nearly optimal number of samples. Finally, on multiple datasets, we demonstrate that the framework allows personalized models to obtain superior performance in the data-scarce regime.},
journal = {},
year = {2022},
pages = {},
url = {allpapers/PalVMTASSJ22.pdf}
}
@article{JainP22,
title = {Online low rank matrix completion},
author = {Prateek Jain, Soumyabrata Pal},
abstract = {We study the problem of {\em online} low-rank matrix completion with users, items and rounds. In each round, the algorithm recommends one item per user, for which it gets a (noisy) reward sampled from a low-rank user-item preference matrix. The goal is to design a method with sub-linear regret (in ) and nearly optimal dependence on and . The problem can be easily mapped to the standard multi-armed bandit problem where each item is an {\em independent} arm, but that leads to poor regret as the correlation between arms and users is not exploited. On the other hand, exploiting the low-rank structure of reward matrix is challenging due to non-convexity of the low-rank manifold. We first demonstrate that the low-rank structure can be exploited using a simple explore-then-commit (ETC) approach that ensures a regret of . That is, roughly only item recommendations are required per user to get a non-trivial solution. We then improve our result for the rank- setting which in itself is quite challenging and encapsulates some of the key issues. Here, we propose \textsc{OCTAL} (Online Collaborative filTering using iterAtive user cLustering) that guarantees nearly optimal regret of . OCTAL is based on a novel technique of clustering users that allows iterative elimination of items and leads to a nearly optimal minimax rate.},
journal = {arXiv preprint arXiv:2209.03997},
year = {2022},
pages = {},
url = {allpapers/JainP22.pdf}
}
@article{NaseryANJ22,
title = {DAFT: Distilling Adversarially Fine-tuned Models for Better OOD Generalization},
author = {Anshul Nasery, Sravanti Addepalli, Praneeth Netrapalli, Prateek Jain},
abstract = {We consider the problem of OOD generalization, where the goal is to train a model that performs well on test distributions that are different from the training distribution. Deep learning models are known to be fragile to such shifts and can suffer large accuracy drops even for slightly different test distributions. We propose a new method - DAFT - based on the intuition that adversarially robust combination of a large number of rich features should provide OOD robustness. Our method carefully distills the knowledge from a powerful teacher that learns several discriminative features using standard training while combining them using adversarial training. The standard adversarial training procedure is modified to produce teachers which can guide the student better. We evaluate DAFT on standard benchmarks in the DomainBed framework, and demonstrate that DAFT achieves significant improvements over the current state-of-the-art OOD generalization methods. DAFT consistently out-performs well-tuned ERM and distillation baselines by up to 6%, with more pronounced gains for smaller networks.},
journal = {arXiv preprint arXiv:2208.09139},
year = {2022},
pages = {},
url = {allpapers/NaseryANJ22.pdf}
}
@article{MadaanBJJ22,
title = {Treeformer: Dense gradient trees for efficient attention computation},
author = {Lovish Madaan, Srinadh Bhojanapalli, Himanshu Jain, Prateek Jain},
abstract = {Standard inference and training with transformer based architectures scale quadratically with input sequence length. This is prohibitively large for a variety of applications especially in web-page translation, query-answering etc. Consequently, several approaches have been developed recently to speedup attention computation by enforcing different attention structures such as sparsity, low-rank, approximating attention using kernels. In this work, we view attention computation as that of nearest neighbor retrieval, and use decision tree based hierarchical navigation to reduce the retrieval cost per query token from linear in sequence length to nearly logarithmic. Based on such hierarchical navigation, we design Treeformer which can use one of two efficient attention layers -- TF-Attention and TC-Attention. TF-Attention computes the attention in a fine-grained style, while TC-Attention is a coarse attention layer which also ensures that the gradients are "dense". To optimize such challenging discrete layers, we propose a two-level bootstrapped training method. Using extensive experiments on standard NLP benchmarks, especially for long-sequences, we demonstrate that our Treeformer architecture can be almost as accurate as baseline Transformer while using 30x lesser FLOPs in the attention layer. Compared to Linformer, the accuracy can be as much as 12% higher while using similar FLOPs in the attention layer.},
journal = {arXiv preprint arXiv:2208.09015},
year = {2022},
pages = {},
url = {allpapers/MadaanBJJ22.pdf}
}
@inproceedings{NaseryANJ22,
title = {DAFT: Distilling Adversarially Fine-tuned teachers for OOD Robustness},
author = {Anshul Nasery, Sravanti Addepalli, Praneeth Netrapalli, Prateek Jain},
abstract = {We consider the problem of OOD generalization,where the goal is to train a model that performs well on test distributions that are different from the training distribution. Deep learning models are known to be fragile to such shifts and can suffer large accuracy drops even for slightly different test distributions (Hendrycks & Dietterich, 2019).We propose a new method –DAFT– based on the intuition that adversarially robust combination of a large number of rich features should provide OOD robustness. Our method carefully distills the model from a powerful teacher that learns several discriminative features using standard training while combining them using adversarial training. The standard adversarial training procedure is modified to produce teachers which can guide the student better. We evaluate DAFT on standard benchmarks in the DomainBed framework, and find that DAFT consistently out-performs well-tuned ERM and distillation baselines by up to 6%, with more pronounced gains for smaller networks},
booktitle = {ICML 2022: Workshop on Spurious Correlations, Invariance and Stability},
year = {2022},
pages = {},
url = {allpapers/NaseryANJ22.pdf}
}
@article{VarshneyTJ22,
title = {(Nearly) Optimal Private Linear Regression via Adaptive Clipping},
author = {Prateek Varshney, Abhradeep Thakurta, Prateek Jain},
abstract = {We study the problem of differentially private linear regression where each data point is sampled from a fixed sub-Gaussian style distribution. We propose and analyze a one-pass mini-batch stochastic gradient descent method (DP-AMBSSGD) where points in each iteration are sampled without replacement. Noise is added for DP but the noise standard deviation is estimated online. Compared to existing -DP techniques which have sub-optimal error bounds, DP-AMBSSGD is able to provide nearly optimal error bounds in terms of key parameters like dimensionality , number of points , and the standard deviation of the noise in observations. For example, when the -dimensional covariates are sampled i.i.d. from the normal distribution, then the excess error of DP-AMBSSGD due to privacy is , i.e., the error is meaningful when number of samples which is the standard operative regime for linear regression. In contrast, error bounds for existing efficient methods in this setting are: , even for . That is, for constant , the existing techniques require to provide a non-trivial result.},
journal = {arXiv preprint arXiv:2207.04686},
year = {2022},
pages = {},
url = {allpapers/VarshneyTJ22.pdf}
}
@inproceedings{VarshneyTJ22,
title = {(Nearly) Optimal Private Linear Regression for Sub-Gaussian Data via Adaptive Clipping},
author = {Prateek Varshney, Abhradeep Thakurta, Prateek Jain},
abstract = {We study the problem of differentially private linear regression where each of the data point is sampled from a fixed sub-Gaussian style distribution. We propose and analyze a one-pass mini-batch stochastic gradient descent method (DP-AMBSSGD) where points in each iteration are sampled without replacement. Noise is added for DP but the noise standard deviation is estimated online. Compared to existing -DP techniques which have sub-optimal error bounds, DP-AMBSSGD is able to provide nearly optimal error bounds in terms of key parameters like dimensionality , number of points , and the standard deviation\sigma of the noise in observations. For example, when the -dimensional covariates are sampled iid from the normal distribution, then the excess error of DP-AMBSSGD due to privacy is , ie, the error is meaningful when number of samples N\geq d\log d which is the standard operative regime for linear regression. In contrast, error bounds for existing efficient methods in this setting are: , even for . That is, for constant , the existing techniques require to provide a non-trivial result.},
booktitle = {Conference on Learning Theory},
year = {2022},
pages = {1126-1166},
url = {allpapers/VarshneyTJ22.pdf}
}
@article{MajmundarGNJ22,
title = {Met: Masked encoding for tabular data},
author = {Kushal Majmundar, Sachin Goyal, Praneeth Netrapalli, Prateek Jain},
abstract = {We consider the task of self-supervised representation learning (SSL) for tabular data: tabular-SSL. Typical contrastive learning based SSL methods require instance-wise data augmentations which are difficult to design for unstructured tabular data. Existing tabular-SSL methods design such augmentations in a relatively ad-hoc fashion and can fail to capture the underlying data manifold. Instead of augmentations based approaches for tabular-SSL, we propose a new reconstruction based method, called Masked Encoding for Tabular Data (MET), that does not require augmentations. MET is based on the popular MAE approach for vision-SSL [He et al., 2021] and uses two key ideas: (i) since each coordinate in a tabular dataset has a distinct meaning, we need to use separate representations for all coordinates, and (ii) using an adversarial reconstruction loss in addition to the standard one. Empirical results on five diverse tabular datasets show that MET achieves a new state of the art (SOTA) on all of these datasets and improves up to 9% over current SOTA methods. We shed more light on the working of MET via experiments on carefully designed simple datasets.},
journal = {arXiv preprint arXiv:2206.08564},
year = {2022},
pages = {},
url = {allpapers/MajmundarGNJ22.pdf}
}
@inproceedings{AcharyaHJSDT22,
title = {Robust training in high dimensions via block coordinate geometric median descent},
author = {Anish Acharya, Abolfazl Hashemi, Prateek Jain, Sujay Sanghavi, Inderjit S Dhillon, Ufuk Topcu},
abstract = {Geometric median (GM) is a classical method in statistics for achieving robust estimation of the uncorrupted data; under gross corruption, it achieves the optimal breakdown point of 1/2. However, its computational complexity makes it infeasible for robustifying stochastic gradient descent (SGD) in high-dimensional optimization problems. In this paper, we show that by applying GM to only a judiciously chosen block of coordinates at a time and using a memory mechanism, one can retain the breakdown point of 1/2 for smooth non-convex problems, with non-asymptotic convergence rates comparable to the SGD with GM while resulting in significant speedup in training. We further validate the run-time and robustness of our approach empirically on several popular deep learning tasks. Code available at: https://github. com/anishacharya/BGMD},
booktitle = {International Conference on Artificial Intelligence and Statistics},
year = {2022},
pages = {11145-11168},
url = {allpapers/AcharyaHJSDT22.pdf}
}
@article{SuhasSJN22,
title = {Multivariate Time Series Forecasting},
author = {S Suhas, Arun Suggala, Prateek Jain, Praneeth Nethrapalli},
abstract = {Time series forecasting is an interesting problem with industrial applications in stock price predictions, retail demand forecasting and traffic prediction. In particular, forecasting high dimensional data using highly correlated time series has proven to be difficult. Traditional time series forecasting methods operate only on individual time series. These methods include AR, ARIMA and exponential smoothing [3]. However these models fail to capture even simple relationships between time series. In recent times, the M5 competition [5] shed light on the ability of Deep Learning techniques to do well in the forecasting domain. We explore the problem of “Multivariate Time Series Forecasting” between time series that have very strong explicit correlations (eg traffic data) in this draft.},
journal = {},
year = {2022},
pages = {},
url = {allpapers/SuhasSJN22.pdf}
}
@article{KusupatiBRWSRHCKJF22,
title = {Matryoshka representations for adaptive deployment},
author = {Aditya Kusupati, Gantavya Bhatt, Aniket Rege, Matthew Wallingford, Aditya Sinha, Vivek Ramanujan, William Howard-Snyder, Kaifeng Chen, Sham Kakade, Prateek Jain, Ali Farhadi},
abstract = {Learned representations are a central component in modern ML systems, serving a multitude of downstream tasks. When training such representations, it is often the case that computational and statistical constraints for each downstream task are unknown. In this context, rigid fixed-capacity representations can be either over or under-accommodating to the task at hand. This leads us to ask: can we design a flexible representation that can adapt to multiple downstream tasks with varying computational resources? Our main contribution is Matryoshka Representation Learning (MRL) which encodes information at different granularities and allows a single embedding to adapt to the computational constraints of downstream tasks. MRL minimally modifies existing representation learning pipelines and imposes no additional cost during inference and deployment. MRL learns coarse-to-fine representations that are at least as accurate and rich as independently trained low-dimensional representations. The flexibility within the learned Matryoshka Representations offer:(a) up to 14× smaller embedding size for ImageNet-1K classification at the same level of accuracy;(b) up to 14× real-world speed-ups for large-scale retrieval on ImageNet-1K and 4K; and (c) up to 2% accuracy improvements for long-tail few-shot classification, all while being as robust as the original representations. Finally, we show that MRL extends seamlessly to web-scale datasets (ImageNet, JFT) across various modalities–vision (ViT, ResNet), vision+ language (ALIGN) and language (BERT). MRL code and pretrained models are open-sourced at https://github. com/RAIVNLab/MRL.},
journal = {arXiv preprint arXiv:2205.13147},
year = {2022},
pages = {},
url = {allpapers/KusupatiBRWSRHCKJF22.pdf}
}
@article{GulwaniJPPP21,
title = {Syntactic profiling of alphanumeric strings},
author = {Sumit Gulwani, Prateek Jain, Daniel Adam Perelman, Saswat Padhi, Oleksandr Polozov},
abstract = {A computing device includes a storage machine holding instructions executable by a logic machine to generate multi-string clusters, each containing alphanumeric strings of a dataset. Further multi-string clusters are generated via iterative performance of a combination operation in which a hierarchically-superior cluster is generated from a set of multi-string clusters. The combination operation includes, for candidate pairs of multi-string clusters, generating syn tactic profiles describing an alphanumeric string from each multi-string cluster of the candidate pair. For each of the candidate pairs, a cost factor is determined for at least one of its syntactic profiles. Based on the cost factors determined for the syntactic profiles, one of the candidate pairs is selected. The multi-string clusters from the selected candi date pair are combined to generate the hierarchically-supe rior cluster including all of the alphanumeric strings …},
journal = {},
year = {2021},
pages = {},
url = {allpapers/GulwaniJPPP21.pdf}
}
@article{SmithJRST21,
title = {Differentially private model personalization},
author = {Adam Smith, Prateek Jain, Keith Rush, Shuang Song, Abhradeep G Thakurta},
abstract = {We study personalization of supervised learning with user-level differential privacy. Consider a setting with many users, each of whom has a training data set drawn from their own distribution Pi . Assuming some shared structure among the problems Pi, can users collectively learn the shared structure---and solve their tasks better than they could individually---while preserving the privacy of their data? We formulate this question using joint, user-level differential privacy---that is, we control what is leaked about each user's entire data set. We provide algorithms that exploit popular non-private approaches in this domain like the Almost-No-Inner-Loop (ANIL) method, and give strong user-level privacy guarantees for our general approach. When the problems Pi are linear regression problems with each user's regression vector lying in a common, unknown low-dimensional subspace, we show that our efficient algorithms satisfy nearly optimal estimation error guarantees. We also establish a general, information-theoretic upper bound via an exponential mechanism-based algorithm.},
journal = {},
year = {2021},
pages = {},
url = {allpapers/SmithJRST21.pdf}
}
@article{KowshikNJN21,
title = {Streaming linear system identification with reverse experience replay},
author = {Suhas Kowshik, Dheeraj Nagaraj, Prateek Jain, Praneeth Netrapalli},
abstract = {We consider the problem of estimating a linear time-invariant (LTI) dynamical system from a single trajectory via streaming algorithms, which is encountered in several applications including reinforcement learning (RL) and time-series analysis. While the LTI system estimation problem is well-studied in the {\em offline} setting, the practically important streaming/online setting has received little attention. Standard streaming methods like stochastic gradient descent (SGD) are unlikely to work since streaming points can be highly correlated. In this work, we propose a novel streaming algorithm, SGD with Reverse Experience Replay (SGD-RER), that is inspired by the experience replay (ER) technique popular in the RL literature. SGD-RER divides data into small buffers and runs SGD backwards on the data stored in the individual buffers. We show that this algorithm exactly deconstructs the dependency structure and obtains information theoretically optimal guarantees for both parameter error and prediction error. Thus, we provide the first--to the best of our knowledge--optimal SGD-style algorithm for the classical problem of linear system identification with a first order oracle. Furthermore, SGD-RER can be applied to more general settings like sparse LTI identification with known sparsity pattern, and non-linear dynamical systems. Our work demonstrates that the knowledge of data dependency structure can aid us in designing statistically and computationally efficient algorithms which can``decorrelate''streaming samples.},
journal = {Advances in Neural Information Processing Systems},
year = {2021},
pages = {30140-30152},
url = {allpapers/KowshikNJN21.pdf}
}
@article{ThekumparampilJNO21,
title = {Statistically and computationally efficient linear meta-representation learning},
author = {Kiran K Thekumparampil, Prateek Jain, Praneeth Netrapalli, Sewoong Oh},
abstract = {In typical few-shot learning, each task is not equipped with enough data to be learned in isolation. To cope with such data scarcity, meta-representation learning methods train across many related tasks to find a shared (lower-dimensional) representation of the data where all tasks can be solved accurately. It is hypothesized that any new arriving tasks can be rapidly trained on this low-dimensional representation using only a few samples. Despite the practical successes of this approach, its statistical and computational properties are less understood. Moreover, the prescribed algorithms in these studies have little resemblance to those used in practice or they are computationally intractable. To understand and explain the success of popular meta-representation learning approaches such as ANIL, MetaOptNet, R2D2, and OML, we study a alternating gradient-descent minimization (AltMinGD) method (and its variant alternating minimization (AltMin)) which underlies the aforementioned methods. For a simple but canonical setting of shared linear representations, we show that AltMinGD achieves nearly-optimal estimation error, requiring only samples per task. This agrees with the observed efficacy of this algorithm in the practical few-shot learning scenarios.},
journal = {Advances in Neural Information Processing Systems},
year = {2021},
pages = {18487-18500},
url = {allpapers/ThekumparampilJNO21.pdf}
}
@article{JainRSST21,
title = {Differentially private model personalization},
author = {Prateek Jain, John Rush, Adam Smith, Shuang Song, Abhradeep Guha Thakurta},
abstract = {We study personalization of supervised learning with user-level differential privacy. Consider a setting with many users, each of whom has a training data set drawn from their own distribution . Assuming some shared structure among the problems , can users collectively learn the shared structure---and solve their tasks better than they could individually---while preserving the privacy of their data? We formulate this question using joint, user-level differential privacy---that is, we control what is leaked about each user's entire data set. We provide algorithms that exploit popular non-private approaches in this domain like the Almost-No-Inner-Loop (ANIL) method, and give strong user-level privacy guarantees for our general approach. When the problems are linear regression problems with each user's regression vector lying in a common, unknown low-dimensional subspace, we show that our efficient algorithms satisfy nearly optimal estimation error guarantees. We also establish a general, information-theoretic upper bound via an exponential mechanism-based algorithm.},
journal = {Advances in neural information processing systems},
year = {2021},
pages = {29723-29735},
url = {allpapers/JainRSST21.pdf}
}
@article{KusupatiWRSPPJKF21,
title = {Llc: Accurate, multi-purpose learnt low-dimensional binary codes},
author = {Aditya Kusupati, Matthew Wallingford, Vivek Ramanujan, Raghav Somani, Jae Sung Park, Krishna Pillutla, Prateek Jain, Sham Kakade, Ali Farhadi},
abstract = {Learning binary representations of instances and classes is a classical problem with several high potential applications. In modern settings, the compression of high-dimensional neural representations to low-dimensional binary codes is a challenging task and often require large bit-codes to be accurate. In this work, we propose a novel method for $\textbf {L} $ earning $\textbf {L} $ ow-dimensional binary $\textbf {C} $ odes $(\textbf {LLC}) $ for instances as well as classes. Our method does ${\textit {not}} $ require any side-information, like annotated attributes or label meta-data, and learns extremely low-dimensional binary codes ($\approx 20$ bits for ImageNet-1K). The learnt codes are super-efficient while still ensuring $\textit {nearly optimal} $ classification accuracy for ResNet50 on ImageNet-1K. We demonstrate that the learnt codes capture intrinsically important features in the data, by discovering an intuitive taxonomy over classes. We further quantitatively measure the quality of our codes by applying it to the efficient image retrieval as well as out-of-distribution (OOD) detection problems. For ImageNet-100 retrieval problem, our learnt binary codes outperform $16 $ bit HashNet using only $10 $ bits and also are as accurate as $10 $ dimensional real representations. Finally, our learnt binary codes can perform OOD detection, out-of-the-box, as accurately as a baseline that needs $\approx3000 $ samples to tune its threshold, while we require ${\textit {none}} $. Code is open-sourced at https://github. com/RAIVNLab/LLC.},
journal = {Advances in neural information processing systems},
year = {2021},
pages = {23900-23913},
url = {allpapers/KusupatiWRSPPJKF21.pdf}
}
@article{KowshikNJN21,
title = {Near-optimal offline and streaming algorithms for learning non-linear dynamical systems},
author = {Suhas Kowshik, Dheeraj Nagaraj, Prateek Jain, Praneeth Netrapalli},
abstract = {We consider the setting of vector valued non-linear dynamical systems , where is unbiased noise and is a known link function that satisfies certain {\em expansivity property}. The goal is to learn from a single trajectory of {\em dependent or correlated} samples. While the problem is well-studied in the linear case, where is identity, with optimal error rates even for non-mixing systems, existing results in the non-linear case hold only for mixing systems. In this work, we improve existing results for learning nonlinear systems in a number of ways: a) we provide the first offline algorithm that can learn non-linear dynamical systems without the mixing assumption, b) we significantly improve upon the sample complexity of existing results for mixing systems, c) in the much harder one-pass, streaming setting we study a SGD with Reverse Experience Replay (SGD-RER) method, and demonstrate that for mixing systems, it achieves the same sample complexity as our offline algorithm, d) we justify the expansivity assumption by showing that for the popular ReLU link function---a non-expansive but easy to learn link function with iid samples---any method would require exponentially many samples (with respect to dimension of ) from the dynamical system. We validate our results via. simulations and demonstrate that a naive application of SGD can be highly sub-optimal. Indeed, our work demonstrates that for correlated data, specialized methods designed for the dependency structure in data can significantly outperform standard SGD based methods.},
journal = {Advances in Neural Information Processing Systems},
year = {2021},
pages = {8518-8531},
url = {allpapers/KowshikNJN21.pdf}
}
@article{ShahJN21,
title = {Do input gradients highlight discriminative features?},
author = {Harshay Shah, Prateek Jain, Praneeth Netrapalli},
abstract = {Post-hoc gradient-based interpretability methods [Simonyan et al., 2013, Smilkov et al., 2017] that provide instance-specific explanations of model predictions are often based on assumption (A): magnitude of input gradients—gradients of logits with respect to input—noisily highlight discriminative task-relevant features. In this work, we test the validity of assumption (A) using a three-pronged approach: 1. We develop an evaluation framework, DiffROAR, to test assumption (A) on four image classification benchmarks. Our results suggest that (i) input gradients of standard models (ie, trained on original data) may grossly violate (A), whereas (ii) input gradients of adversarially robust models satisfy (A). 2. We then introduce BlockMNIST, an MNIST-based semi-real dataset, that by design encodes a priori knowledge of discriminative features. Our analysis on BlockMNIST leverages this information to validate as well as characterize differences between input gradient attributions of standard and robust models. 3. Finally, we theoretically prove that our empirical findings hold on a simplified version of the BlockMNIST dataset. Specifically, we prove that input gradients of standard one-hidden-layer MLPs trained on this dataset do not highlight instance-specific signal coordinates, thus grossly violating assumption (A). Our findings motivate the need to formalize and test common assumptions in interpretability in a falsifiable manner [Leavitt and Morcos, 2020]. We believe that the DiffROAR evaluation framework and BlockMNIST-based datasets can serve as sanity checks to audit instance-specific interpretability methods; code and data available at https://github …},
journal = {Advances in Neural Information Processing Systems},
year = {2021},
pages = {2046-2059},
url = {allpapers/ShahJN21.pdf}
}
@article{DaigavaneMSTAJ21,
title = {Node-level differentially private graph neural networks},
author = {Ameya Daigavane, Gagan Madan, Aditya Sinha, Abhradeep Guha Thakurta, Gaurav Aggarwal, Prateek Jain},
abstract = {Graph Neural Networks (GNNs) are a popular technique for modelling graph-structured data and computing node-level representations via aggregation of information from the neighborhood of each node. However, this aggregation implies an increased risk of revealing sensitive information, as a node can participate in the inference for multiple nodes. This implies that standard privacy-preserving machine learning techniques, such as differentially private stochastic gradient descent (DP-SGD) - which are designed for situations where each data point participates in the inference for one point only - either do not apply, or lead to inaccurate models. In this work, we formally define the problem of learning GNN parameters with node-level privacy, and provide an algorithmic solution with a strong differential privacy guarantee. We employ a careful sensitivity analysis and provide a non-trivial extension of the privacy-by-amplification technique to the GNN setting. An empirical evaluation on standard benchmark datasets demonstrates that our method is indeed able to learn accurate privacy-preserving GNNs which outperform both private and non-private methods that completely ignore graph information.},
journal = {arXiv preprint arXiv:2111.15521},
year = {2021},
pages = {},
url = {allpapers/DaigavaneMSTAJ21.pdf}
}
@article{MenonBHJSCYMLDD21,
title = {Message recommendation system},
author = {Lekshmi Menon, Amar Budhiraja, Gaurush Hiranandani, Prateek Jain, Darshatkumar Anandji Shah, Ayush Choure, Navya Yarrabelly, Anurag Mishra, Mohammad Luqman, Shivangi Dhakad, Juhi Dua},
abstract = {Systems and methods for entity recommendation can make use of rich data by allowing the items to be recommended and the recipients of the recommendation (eg, users) to be modeled as “complex entities” composed of one or more static sub-entities and/or a dynamic component, and by utilizing information about multiple relationships between the sub-entities as reflected in bipartite graphs. Generating recommendations from such information may involve creating vector representations of the sub-entities based on the bipartite graphs (eg, using graph-based convolutional networks), and combining these vector representations into representations of the items and users (or other recipients) to be fed into a classifier model.},
journal = {},
year = {2021},
pages = {},
url = {allpapers/MenonBHJSCYMLDD21.pdf}
}
@article{AgarwalCJNN21,
title = {Online target q-learning with reverse experience replay: Efficiently finding the optimal policy for linear mdps},
author = {Naman Agarwal, Syomantak Chaudhuri, Prateek Jain, Dheeraj Nagaraj, Praneeth Netrapalli},
abstract = {Q-learning is a popular Reinforcement Learning (RL) algorithm which is widely used in practice with function approximation (Mnih et al., 2015). In contrast, existing theoretical results are pessimistic about Q-learning. For example, (Baird, 1995) shows that Q-learning does not converge even with linear function approximation for linear MDPs. Furthermore, even for tabular MDPs with synchronous updates, Q-learning was shown to have sub-optimal sample complexity (Li et al., 2021;Azar et al., 2013). The goal of this work is to bridge the gap between practical success of Q-learning and the relatively pessimistic theoretical results. The starting point of our work is the observation that in practice, Q-learning is used with two important modifications: (i) training with two networks, called online network and target network simultaneously (online target learning, or OTL) , and (ii) experience replay (ER) (Mnih et al., 2015). While they have been observed to play a significant role in the practical success of Q-learning, a thorough theoretical understanding of how these two modifications improve the convergence behavior of Q-learning has been missing in literature. By carefully combining Q-learning with OTL and reverse experience replay (RER) (a form of experience replay), we present novel methods Q-Rex and Q-RexDaRe (Q-Rex + data reuse). We show that Q-Rex efficiently finds the optimal policy for linear MDPs (or more generally for MDPs with zero inherent Bellman error with linear approximation (ZIBEL)) and provide non-asymptotic bounds on sample complexity -- the first such result for a Q-learning method for this class of MDPs under standard …},
journal = {arXiv preprint arXiv:2110.08440},
year = {2021},
pages = {},
url = {allpapers/AgarwalCJNN21.pdf}
}
@article{NarayananSJKS21,
title = {Iglu: Efficient GCN training via lazy updates},
author = {S Deepak Narayanan, Aditya Sinha, Prateek Jain, Purushottam Kar, Sundararajan Sellamanickam},
abstract = {Training multi-layer Graph Convolution Networks (GCN) using standard SGD techniques scales poorly as each descent step ends up updating node embeddings for a large portion of the graph. Recent attempts to remedy this sub-sample the graph that reduces compute but introduce additional variance and may offer suboptimal performance. This paper develops the IGLU method that caches intermediate computations at various GCN layers thus enabling lazy updates that significantly reduce the compute cost of descent. IGLU introduces bounded bias into the gradients but nevertheless converges to a first-order saddle point under standard assumptions such as objective smoothness. Benchmark experiments show that IGLU offers up to 1.2% better accuracy despite requiring up to 88% less compute.},
journal = {arXiv preprint arXiv:2109.13995},
year = {2021},
pages = {},
url = {allpapers/NarayananSJKS21.pdf}
}
@article{PolozovGJVM21,
title = {Neural-guided deductive search for program synthesis},
author = {Oleksandr Polozov, Sumit Gulwani, Prateek Jain, Ashwin Kalyan Vijayakumar, Abhishek Mohta},
abstract = {Systems, methods, and computer-executable instructions for guiding program synthesis includes receiving a specification that includes an input and output example. Programs are synthesized that meet the specification. During synthesizing each of the programs includes branching decisions. Each branching decision includes a plurality of paths. Synthesizing the programs comprises includes selecting a first score model, for a first branching decision. Each of the programs is scored using the first score model. The paths of the first branching decision are pared based on the score. One the paths is selected. A synthesized program that meets the specification is returned. The synthesized program includes the one of the paths.},
journal = {},
year = {2021},
pages = {},
url = {allpapers/PolozovGJVM21.pdf}
}
@inproceedings{ChienJKRSTZ21,
title = {Private alternating least squares: Practical private matrix completion with tighter rates},
author = {Steve Chien, Prateek Jain, Walid Krichene, Steffen Rendle, Shuang Song, Abhradeep Thakurta, Li Zhang},
abstract = {We study the problem of differentially private (DP) matrix completion under user-level privacy. We design a joint differentially private variant of the popular Alternating-Least-Squares (ALS) method that achieves: i)(nearly) optimal sample complexity for matrix completion (in terms of number of items, users), and ii) the best known privacy/utility trade-off both theoretically, as well as on benchmark data sets. In particular, we provide the first global convergence analysis of ALS with noise introduced to ensure DP, and show that, in comparison to the best known alternative (the Private Frank-Wolfe algorithm by Jain et al.(2018)), our error bounds scale significantly better with respect to the number of items and users, which is critical in practical problems. Extensive validation on standard benchmarks demonstrate that the algorithm, in combination with carefully designed sampling procedures, is significantly more accurate than existing techniques, thus promising to be the first practical DP embedding model.},
booktitle = {International Conference on Machine Learning},
year = {2021},
pages = {1877-1887},
url = {allpapers/ChienJKRSTZ21.pdf}
}
@inproceedings{SahaNNJ21,
title = {Optimal regret algorithm for pseudo-1d bandit convex optimization},
author = {Aadirupa Saha, Nagarajan Natarajan, Praneeth Netrapalli, Prateek Jain},
abstract = {We study online learning with bandit feedback (ie learner has access to only zeroth-order oracle) where cost/reward functions $\f_t $ admit a" pseudo-1d" structure, ie $\f_t (\w)=\loss_t (\pred_t (\w)) $ where the output of $\pred_t $ is one-dimensional. At each round, the learner observes context $\x_t $, plays prediction $\pred_t (\w_t;\x_t) $(eg $\pred_t (\cdot)=⟨\x_t,\cdot⟩ $) for some $\w_t\in\mathbb {R}^ d $ and observes loss $\loss_t (\pred_t (\w_t)) $ where $\loss_t $ is a convex Lipschitz-continuous function. The goal is to minimize the standard regret metric. This pseudo-1d bandit convex optimization problem (\SBCO) arises frequently in domains such as online decision-making or parameter-tuning in large systems. For this problem, we first show a regret lower bound of for any algorithm, where is the number of rounds. We propose a new algorithm\sbcalg that combines randomized online gradient descent with a kernelized exponential weights method to exploit the pseudo-1d structure effectively, guaranteeing the {\em optimal} regret bound mentioned above, up to additional logarithmic factors. In contrast, applying state-of-the-art online convex optimization methods leads to regret, that is significantly suboptimal in terms of .},
booktitle = {International Conference on Machine Learning},
year = {2021},
pages = {9255-9264},
url = {allpapers/SahaNNJ21.pdf}
}
@article{ThekumparampilJNO21,
title = {Sample efficient linear meta-learning by alternating minimization},
author = {Kiran Koshy Thekumparampil, Prateek Jain, Praneeth Netrapalli, Sewoong Oh},
abstract = {Meta-learning synthesizes and leverages the knowledge from a given set of tasks to rapidly learn new tasks using very little data. Meta-learning of linear regression tasks, where the regressors lie in a low-dimensional subspace, is an extensively-studied fundamental problem in this domain. However, existing results either guarantee highly suboptimal estimation errors, or require samples per task (where is the data dimensionality) thus providing little gain over separately learning each task. In this work, we study a simple alternating minimization method (MLLAM), which alternately learns the low-dimensional subspace and the regressors. We show that, for a constant subspace dimension MLLAM obtains nearly-optimal estimation error, despite requiring only samples per task. However, the number of samples required per task grows logarithmically with the number of tasks. To remedy this in the low-noise regime, we propose a novel task subset selection scheme that ensures the same strong statistical guarantee as MLLAM, even with bounded number of samples per task for arbitrarily large number of tasks.},
journal = {arXiv preprint arXiv:2105.08306},
year = {2021},
pages = {},
url = {allpapers/ThekumparampilJNO21.pdf}
}
@article{ThekumparampilJNO21,
title = {Sample Efficient Linear Meta-Learning by Alternating Minimization},
author = {Kiran Koshy Thekumparampil, Prateek Jain, Praneeth Netrapalli, Sewoong Oh},
abstract = {Meta-learning synthesizes and leverages the knowledge from a given set of tasks to rapidly learn new tasks using very little data. Meta-learning of linear regression tasks, where the regressors lie in a low-dimensional subspace, is an extensively-studied fundamental problem in this domain. However, existing results either guarantee highly suboptimal estimation errors, or require samples per task (where is the data dimensionality) thus providing little gain over separately learning each task. In this work, we study a simple alternating minimization method (MLLAM), which alternately learns the low-dimensional subspace and the regressors. We show that, for a constant subspace dimension MLLAM obtains nearly-optimal estimation error, despite requiring only samples per task. However, the number of samples required per task grows logarithmically with the number of tasks. To remedy this in the low …},
journal = {arXiv e-prints},
year = {2021},
pages = {arXiv: 2105.08306},
url = {allpapers/ThekumparampilJNO21.pdf}
}
@article{JainKNN21,
title = {Streaming linear system identification with reverse experience replay},
author = {Prateek Jain, Suhas S Kowshik, Dheeraj Nagaraj, Praneeth Netrapalli},
abstract = {We consider the problem of estimating a linear time-invariant (LTI) dynamical system from a single trajectory via streaming algorithms, which is encountered in several applications including reinforcement learning (RL) and time-series analysis. While the LTI system estimation problem is well-studied in the {\em offline} setting, the practically important streaming/online setting has received little attention. Standard streaming methods like stochastic gradient descent (SGD) are unlikely to work since streaming points can be highly correlated. In this work, we propose a novel streaming algorithm, SGD with Reverse Experience Replay (), that is inspired by the experience replay (ER) technique popular in the RL literature. divides data into small buffers and runs SGD backwards on the data stored in the individual buffers. We show that this algorithm exactly deconstructs the dependency structure and obtains information theoretically optimal guarantees for both parameter error and prediction error. Thus, we provide the first -- to the best of our knowledge -- optimal SGD-style algorithm for the classical problem of linear system identification with a first order oracle. Furthermore, can be applied to more general settings like sparse LTI identification with known sparsity pattern, and non-linear dynamical systems. Our work demonstrates that the knowledge of data dependency structure can aid us in designing statistically and computationally efficient algorithms which can "decorrelate" streaming samples.},
journal = {arXiv preprint arXiv:2103.05896},
year = {2021},
pages = {},
url = {allpapers/JainKNN21.pdf}
}
@article{KarthikeyanJNJ21,
title = {Learning accurate decision trees with bandit feedback via quantized gradient descent},
author = {Ajaykrishna Karthikeyan, Naman Jain, Nagarajan Natarajan, Prateek Jain},
abstract = {Decision trees provide a rich family of highly non-linear but efficient models, due to which they continue to be the go-to family of predictive models by practitioners across domains. But learning trees is challenging due to their discrete decision boundaries. The state-of-the-art (SOTA) techniques resort to (a) learning \textit{soft} trees thereby losing logarithmic inference time; or (b) using methods tailored to specific supervised learning settings, requiring access to labeled examples and loss function. In this work, by leveraging techniques like overparameterization and straight-through estimators, we propose a unified method that enables accurate end-to-end gradient based tree training and can be deployed in a variety of settings like offline supervised learning and online learning with bandit feedback. Using extensive validation on standard benchmarks, we demonstrate that our method provides best of both worlds, i.e., it is competitive to, and in some cases more accurate than methods designed \textit{specifically} for the supervised settings; and in bandit settings, where most existing tree learning techniques are not applicable, our models are still accurate and significantly outperform the applicable SOTA methods.},
journal = {arXiv preprint arXiv:2102.07567},
year = {2021},
pages = {},
url = {allpapers/KarthikeyanJNJ21.pdf}
}
@article{JainNN21,
title = {Making the last iterate of sgd information theoretically optimal},
author = {Prateek Jain, Dheeraj M Nagaraj, Praneeth Netrapalli},
abstract = {Stochastic gradient descent (SGD) is one of the most widely used algorithms for large-scale optimization problems. While classical theoretical analysis of SGD for convex problems studies (suffix) averages of iterates and obtains information theoretically optimal bounds on suboptimality, the last point of SGD is, by far, the most preferred choice in practice. The best known results for the last point of SGD [O. Shamir and T. Zhang, Proceedings of the 30th International Conference on Machine Learning, 2013, pp. 71--79] however, are suboptimal compared to information theoretic lower bounds by a factor, where is the number of iterations. Harvey, Liaw, Plan, and Randhawa [Conference on Learning Theory, PMLR, 2019, pp. 1579--1613] shows that in fact, this additional factor is tight for standard step size sequences of and for non-strongly convex and strongly convex settings, respectively. Similarly …},
journal = {SIAM Journal on Optimization},
year = {2021},
pages = {1108-1130},
url = {allpapers/JainNN21.pdf}
}
@article{NagarajNJK21,
title = {Near-optimal Offline and Streaming Algorithms for Learning Non-Linear Dynamical Systems},
author = {Dheeraj Nagaraj, Praneeth Netrapalli, Prateek Jain, Suhas Kowshik},
abstract = {We study the problem of learning vector valued non-linear dynamical systems from a single trajectory of {\em dependent or correlated} points. We assume a known link function that satisfy a certain {\em expansivity property}. While the problem is well-studied in the linear case with strong learning guarantees even for non-mixing systems, the results in non-linear case hold only for mixing systems and even then the error rates are significantly sub-optimal. In this work, we bridge this gap in a variety of settings: a) we provide first optimal offline algorithm that can learn non-linear dynamical systems without mixing assumption, b) in the much harder one-pass, streaming setting we study a SGD with Reverse Experience Replay ($\sgdber $) method, and demonstrate that for mixing systems, it achieves nearly optimal performance even for heavy-tailed noise, c) we justify the expansivity assumption by showing that when …},
journal = {},
year = {2021},
pages = {},
url = {allpapers/NagarajNJK21.pdf}
}
@article{NagarajNJK21,
title = {Streaming Linear System Identification with Reverse Experience Replay},
author = {Dheeraj Nagaraj, Praneeth Netrapalli, Prateek Jain, Suhas Kowshik},
abstract = {We consider the problem of estimating a stochastic linear time-invariant (LTI) dynamical system from a single trajectory via streaming algorithms. The problem is equivalent to estimating the parameters of vector auto-regressive ($\var $) models encountered in time series analysis (\cite {hamilton2020time}). A recent sequence of papers\citep {faradonbeh2018finite, simchowitz2018learning, sarkar2019near} show that ordinary least squares (OLS) regression can be used to provide optimal finite time estimator for the problem. However, such techniques apply for {\em offline} setting where the optimal solution of OLS is available {\em apriori}. But, in many problems of interest as encountered in reinforcement learning (RL), it is important to estimate the parameters on the go using gradient oracle. This task is challenging since standard methods like SGD might not perform well when using stochastic gradients from …},
journal = {},
year = {2021},
pages = {},
url = {allpapers/NagarajNJK21.pdf}
}
@article{ThakurtaSRJS21,
title = {Differentially Private Model Personalization},
author = {Abhradeep Guha Thakurta, Adam Smith, Keith Rush, Prateek Jain, Shuang Song},
abstract = {We study personalization of supervised learning with user-level differential privacy. Consider a setting with many users, each of whom has a training data set drawn from their own distribution . Assuming some shared structure among the problems , can users collectively learn the shared structure---and solve their tasks better than they could individually---while preserving the privacy of their data? We formulate this question using joint,\textit {user-level} differential privacy---that is, we control what is leaked about each user's entire data set.},
journal = {},
year = {2021},
pages = {},
url = {allpapers/ThakurtaSRJS21.pdf}
}
@article{ThekumparampilNJO21,
title = {Sample Efficient Linear Meta-Learning},
author = {Kiran Koshy Thekumparampil, Praneeth Netrapalli, Prateek Jain, Sewoong Oh},
abstract = {Meta-learning algorithms synthesizes and leverages the knowledge from a given set of tasks to rapidly learn new tasks using very little data. While methods like ANIL\cite {raghu2019rapid} have been demonstrated to be effective in practical meta-learning problems, their statistical and computational properties are ill-understood. Recent theoretical studies of meta-learning problem in a simple linear/non-linear regression setting still do not explain practical success of the meta-learning approach. For example, existing results either guarantee highly suboptimal estimation errors\cite {tripuraneni2020provable} or require relatively large number of samples per task\cite {}-- samples where is the data dimensionality--which runs counter to practical settings. Additionally, the prescribed algorithms are inefficient and typically are not used in practice.% to achieve these sample complexity are high inefficient. Similar to the …},
journal = {},
year = {2021},
pages = {},
url = {allpapers/ThekumparampilNJO21.pdf}
}
@article{ThakurtaZJSRK21,
title = {Private Alternating Least Squares:(Nearly) OptimalPrivacy/Utility Trade-off for Matrix Completion},
author = {Abhradeep Guha Thakurta, Li Zhang, Prateek Jain, Shuang Song, Steffen Rendle, Walid Krichene},
abstract = {We study the problem of differentially private (DP) matrix completion under user-level privacy. We design an -joint differentially private variant of the popular Alternating-Least-Squares (ALS) method that achieves: i)(nearly) optimal sample complexity for matrix completion (in terms of number of items, users), and ii) best known privacy/utility trade-off both theoretically, as well as on benchmark data sets.},
journal = {},
year = {2021},
pages = {},
url = {allpapers/ThakurtaZJSRK21.pdf}
}
@inproceedings{HiranandaniVKJ20,
title = {Optimization and Analysis of the pAp@ k Metric for Recommender Systems},
author = {Gaurush Hiranandani, Warut Vijitbenjaronk, Sanmi Koyejo, Prateek Jain},
abstract = {Modern recommendation and notification systems must be robust to data imbalance, limitations on the number of recommendations/notifications, and heterogeneous engagement profiles across users. The pAp@ k metric, which combines the partial-AUC and the precision@ k metrics, was recently proposed to evaluate such recommendation systems and has been used in real-world deployments. Conceptually, pAp@ k measures the probability of correctly ranking a top-ranked positive instance over top-ranked negative instances. Due to the combinatorial aspect surfaced by top-ranked points, little is known about the characteristics and optimization methods of pAp@ k. In this paper, we analyze the learning-theoretic properties of pAp@ k, particularly its benefits in evaluating modern recommender systems, and propose novel surrogates that are consistent under certain data regularity conditions. We then provide gradient descent based algorithms to optimize the surrogates directly. Our analysis and experimental evaluation suggest that pAp@ k indeed exhibits a certain dual behavior with respect to partial-AUC and precision@ k. Moreover, the proposed methods outperform all the baselines in various applications. Taken together, our results motivate the use of pAp@ k for large-scale recommender systems with heterogeneous user-engagement.},
booktitle = {International Conference on Machine Learning},
year = {2020},
pages = {4260-4270},
url = {allpapers/HiranandaniVKJ20.pdf}
}
@inproceedings{GoyalRJSJ20,
title = {DROCC: Deep robust one-class classification},
author = {Sachin Goyal, Aditi Raghunathan, Moksh Jain, Harsha Vardhan Simhadri, Prateek Jain},
abstract = {Classical approaches for one-class problems such as one-class SVM and isolation forest require careful feature engineering when applied to structured domains like images. State-of-the-art methods aim to leverage deep learning to learn appropriate features via two main approaches. The first approach based on predicting transformations (Golan & El-Yaniv, 2018; Hendrycks et al., 2019a) while successful in some domains, crucially depends on an appropriate domain-specific set of transformations that are hard to obtain in general. The second approach of minimizing a classical one-class loss on the learned final layer representations, eg, DeepSVDD (Ruff et al., 2018) suffers from the fundamental drawback of representation collapse. In this work, we propose Deep Robust One Class Classification (DROCC) that is both applicable to most standard domains without requiring any side-information and robust to representation collapse. DROCC is based on the assumption that the points from the class of interest lie on a well-sampled, locally linear low dimensional manifold. Empirical evaluation demonstrates that DROCC is highly effective in two different one-class problem settings and on a range of real-world datasets across different domains: tabular data, images (CIFAR and ImageNet), audio, and time-series, offering up to 20% increase in accuracy over the state-of-the-art in anomaly detection. Code is available at https://github. com/microsoft/EdgeML},
booktitle = {International conference on machine learning},
year = {2020},
pages = {3711-3721},
url = {allpapers/GoyalRJSJ20.pdf}
}
@inproceedings{KusupatiRSWJKF20,
title = {Soft threshold weight reparameterization for learnable sparsity},
author = {Aditya Kusupati, Vivek Ramanujan, Raghav Somani, Mitchell Wortsman, Prateek Jain, Sham Kakade, Ali Farhadi},
abstract = {Sparsity in Deep Neural Networks (DNNs) is studied extensively with the focus of maximizing prediction accuracy given an overall parameter budget. Existing methods rely on uniform or heuristic non-uniform sparsity budgets which have sub-optimal layer-wise parameter allocation resulting in a) lower prediction accuracy or b) higher inference cost (FLOPs). This work proposes Soft Threshold Reparameterization (STR), a novel use of the soft-threshold operator on DNN weights. STR smoothly induces sparsity while learning pruning thresholds thereby obtaining a non-uniform sparsity budget. Our method achieves state-of-the-art accuracy for unstructured sparsity in CNNs (ResNet50 and MobileNetV1 on ImageNet-1K), and, additionally, learns non-uniform budgets that empirically reduce the FLOPs by up to 50%. Notably, STR boosts the accuracy over existing results by up to 10% in the ultra sparse (99%) regime and can also be used to induce low-rank (structured sparsity) in RNNs. In short, STR is a simple mechanism which learns effective sparsity budgets that contrast with popular heuristics. Code, pretrained models and sparsity budgets are at https://github. com/RAIVNLab/STR.},
booktitle = {International Conference on Machine Learning},
year = {2020},
pages = {5544-5555},
url = {allpapers/KusupatiRSWJKF20.pdf}
}
@inproceedings{BhatiaPNSJ20,
title = {OASIS: ILP-guided synthesis of loop invariants},
author = {Sahil Bhatia, Saswat Padhi, Nagarajan Natarajan, Rahul Sharma, Prateek Jain},
abstract = {Automated synthesis of inductive invariants is an important problem in software verification. We propose a novel technique that is able to solve complex loop invariant synthesis problems involving large number of variables. We reduce the problem of synthesizing invariants to a set of integer linear programming (ILP) problems. We instantiate our techniques in the tool Oasis that outperforms state-of-the-art systems on benchmarks from the invariant synthesis track of the Syntax Guided Synthesis competition.},
booktitle = {NeurIPS 2020 Workshop on Computer-Assisted Programming},
year = {2020},
pages = {},
url = {allpapers/BhatiaPNSJ20.pdf}
}
@article{ThekumparampilJNO20,
title = {Projection Efficient Subgradient Method and Optimal Nonsmooth Frank-Wolfe Method},
author = {Kiran Koshy Thekumparampil, Prateek Jain, Praneeth Netrapalli, Sewoong Oh},
abstract = {We consider the classical setting of optimizing a nonsmooth Lipschitz continuous convex function over a convex constraint set, when having access to a (stochastic) first-order oracle (FO) for the function and a projection oracle (PO) for the constraint set. It is well known that to achieve -suboptimality in high-dimensions, FO calls are necessary. This is achieved by the projected subgradient method (PGD). However, PGD also entails PO calls, which may be computationally costlier than FO calls (eg nuclear norm constraints). Improving this PO calls complexity of PGD is largely unexplored, despite the fundamental nature of this problem and extensive literature. We present first such improvement. This only requires a mild assumption that the objective function, when extended to a slightly larger neighborhood of the constraint set, still remains Lipschitz and accessible via FO. In particular, we introduce MOPES method, which …},
journal = {arXiv e-prints},
year = {2020},
pages = {arXiv: 2010.01848},
url = {allpapers/ThekumparampilJNO20.pdf}
}
@article{SoJMS20,
title = {Nonconvex Optimization for Signal Processing and Machine Learning [From the Guest Editors]},
author = {Anthony Man-Cho So, Prateek Jain, Wing-Kin Ma, Gesualdo Scutari},
abstract = {The articles in this special section focus on nonconvex optimization for signal processing and machine learning. Optimization is now widely recognized as an indispensable tool in signal processing (SP) and machine learning (ML). Indeed, many of the advances in these fields rely crucially on the formulation of suitable optimization models and deployment of efficient numerical optimization algorithms. In the early 2000s, there was a heavy focus on the use of convex optimization techniques to tackle SP and ML applications. This is largely due to the fact that convex optimization problems often possess favorable theoretical and computational properties and that many problems of practical interest have been shown to admit convex formulations or good convex approximations.},
journal = {IEEE Signal Processing Magazine},
year = {2020},
pages = {15-17},
url = {allpapers/SoJMS20.pdf}
}
@article{NatarajanKJRRGG20,
title = {Programming by rewards},
author = {Nagarajan Natarajan, Ajaykrishna Karthikeyan, Prateek Jain, Ivan Radicek, Sriram Rajamani, Sumit Gulwani, Johannes Gehrke},
abstract = {We formalize and study ``programming by rewards'' (PBR), a new approach for specifying and synthesizing subroutines for optimizing some quantitative metric such as performance, resource utilization, or correctness over a benchmark. A PBR specification consists of (1) input features , and (2) a reward function , modeled as a black-box component (which we can only run), that assigns a reward for each execution. The goal of the synthesizer is to synthesize a "decision function" which transforms the features to a decision value for the black-box component so as to maximize the expected reward for executing decisions for various values of . We consider a space of decision functions in a DSL of loop-free if-then-else programs, which can branch on linear functions of the input features in a tree-structure and compute a linear function of the inputs in the leaves of the tree. We find that this DSL captures decision functions that are manually written in practice by programmers. Our technical contribution is the use of continuous-optimization techniques to perform synthesis of such decision functions as if-then-else programs. We also show that the framework is theoretically-founded ---in cases when the rewards satisfy nice properties, the synthesized code is optimal in a precise sense. We have leveraged PBR to synthesize non-trivial decision functions related to search and ranking heuristics in the PROSE codebase (an industrial strength program synthesis framework) and achieve competitive results to manually written procedures over multiple man years of tuning. We present empirical evaluation against other baseline techniques over …},
journal = {arXiv preprint arXiv:2007.06835},
year = {2020},
pages = {},
url = {allpapers/NatarajanKJRRGG20.pdf}
}
@article{MukhotyGJK20,
title = {Globally-convergent iteratively reweighted least squares for robust regression problems},
author = {Bhaskar Mukhoty, Govind Gopakumar, Prateek Jain, Purushottam Kar},
abstract = {We provide the first global model recovery results for the IRLS (iteratively reweighted least squares) heuristic for robust regression problems. IRLS is known to offer excellent performance, despite bad initializations and data corruption, for several parameter estimation problems. Existing analyses of IRLS frequently require careful initialization, thus offering only local convergence guarantees. We remedy this by proposing augmentations to the basic IRLS routine that not only offer guaranteed global recovery, but in practice also outperform state-of-the-art algorithms for robust regression. Our routines are more immune to hyperparameter misspecification in basic regression tasks, as well as applied tasks such as linear-armed bandit problems. Our theoretical analyses rely on a novel extension of the notions of strong convexity and smoothness to weighted strong convexity and smoothness, and establishing that sub-Gaussian designs offer bounded weighted condition numbers. These notions may be useful in analyzing other algorithms as well.},
journal = {arXiv preprint arXiv:2006.14211},
year = {2020},
pages = {},
url = {allpapers/MukhotyGJK20.pdf}
}
@inproceedings{RoyBJ20,
title = {A topic-aligned multilingual corpus of Wikipedia articles for studying information asymmetry in low resource languages},
author = {Dwaipayan Roy, Sumit Bhatia, Prateek Jain},
abstract = {Wikipedia is the largest web-based open encyclopedia covering more than three hundred languages. However, different language editions of Wikipedia differ significantly in terms of their information coverage. We present a systematic comparison of information coverage in English Wikipedia (most exhaustive) and Wikipedias in eight other widely spoken languages (Arabic, German, Hindi, Korean, Portuguese, Russian, Spanish and Turkish). We analyze the content present in the respective Wikipedias in terms of the coverage of topics as well as the depth of coverage of topics included in these Wikipedias. Our analysis quantifies and provides useful insights about the information gap that exists between different language editions of Wikipedia and offers a roadmap for the IR community to bridge this gap.},
booktitle = {Proceedings of the Twelfth Language Resources and Evaluation Conference},
year = {2020},
pages = {2373-2380},
url = {allpapers/RoyBJ20.pdf}
}
@article{BiswasBJM20,
title = {COVID-19: strategies for allocation of test kits},
author = {Arpita Biswas, Shruthi Bannur, Prateek Jain, Srujana Merugu},
abstract = {With the increasing spread of COVID-19, it is important to systematically test more and more people. The current strategy for test-kit allocation is mostly rule-based, focusing on individuals having (a) symptoms for COVID-19, (b) travel history or (c) contact history with confirmed COVID-19 patients. Such testing strategy may miss out on detecting asymptomatic individuals who got infected via community spread. Thus, it is important to allocate a separate budget of test-kits per day targeted towards preventing community spread and detecting new cases early on. In this report, we consider the problem of allocating test-kits and discuss some solution approaches. We believe that these approaches will be useful to contain community spread and detect new cases early on. Additionally, these approaches would help in collecting unbiased data which can then be used to improve the accuracy of machine learning models trained to predict COVID-19 infections.},
journal = {arXiv preprint arXiv:2004.01740},
year = {2020},
pages = {},
url = {allpapers/BiswasBJM20.pdf}
}
@article{JainVLG20,
title = {Homomorphic factorization encryption},
author = {Prateek Jain, Ramarathnam Venkatesan, Jonathan Lee, Kartik Gupta},
abstract = {Systems, methods, and computer-executable instructions for secure data analysis using encrypted data. An encryption key and a decryption key are created. The security of encryption using the encryption key and the decryption key are based upon factoring. A computation key is created based upon the encryption key. Data is encrypted using the encryption key. The encrypted data and the computation key are provided to a remote system. The remote system is requested to perform data analysis on the encrypted data. An encrypted result of the data analysis is received from the remote system. The encrypted result of the data analysis is decrypted with the decryption key.},
journal = {},
year = {2020},
pages = {},
url = {allpapers/JainVLG20.pdf}
}
@article{BudhirajaHCSYCKJ20,
title = {Rich-Item Recommendations for Rich-Users: Exploiting Dynamic and Static Side Information},
author = {Amar Budhiraja, Gaurush Hiranandani, Darshak Chhatbar, Aditya Sinha, Navya Yarrabelly, Ayush Choure, Oluwasanmi Koyejo, Prateek Jain},
abstract = {In this paper, we study the problem of recommendation system where the users and items to be recommended are rich data structures with multiple entity types and with multiple sources of side-information in the form of graphs. We provide a general formulation for the problem that captures the complexities of modern real-world recommendations and generalizes many existing formulations. In our formulation, each user/document that requires a recommendation and each item or tag that is to be recommended, both are modeled by a set of static entities and a dynamic component. The relationships between entities are captured by several weighted bipartite graphs. To effectively exploit these complex interactions and learn the recommendation model, we propose MEDRES- a multiple graph-CNN based novel deep-learning architecture. MEDRES uses AL-GCN, a novel graph convolution network block, that harnesses strong representative features from the underlying graphs. Moreover, in order to capture highly heterogeneous engagement of different users with the system and constraints on the number of items to be recommended, we propose a novel ranking metric pAp@k along with a method to optimize the metric directly. We demonstrate effectiveness of our method on two benchmarks: a) citation data, b) Flickr data. In addition, we present two real-world case studies of our formulation and the MEDRES architecture. We show how our technique can be used to naturally model the message recommendation problem and the teams recommendation problem in the Microsoft Teams (MSTeams) product and demonstrate that it is 5-6% points more …},
journal = {arXiv preprint arXiv:2001.10495},
year = {2020},
pages = {},
url = {allpapers/BudhirajaHCSYCKJ20.pdf}
}
@article{ThekumparampilJNO20,
title = {Optimal nonsmooth Frank-Wolfe method for stochastic regret minimization},
author = {Kiran Koshy Thekumparampil, Prateek Jain, Praneeth Netrapalli, Sewoong Oh},
abstract = {The current best-known algorithm for convex constrained nonsmooth online stochastic regret minimization using a Linear Minimization Oracle (LMO, a la Frank-Wolfe) and a Stochastic First-order Oracle (SFO) achieves a regret of O (K3/4), where K is the number of iterations [26]. We provide two novel single-loop nonsmooth Frank-Wolfe methods, P-MOLES & PD-MOLES, which achieve the nearly-optimal online stochastic (non-adversarial) regret of O (},
journal = {12th OPT Workshop on Optimization for Machine Learning (OPT2020)},
year = {2020},
pages = {},
url = {allpapers/ThekumparampilJNO20.pdf}
}
@article{ThekumparampilJNO20,
title = {Projection efficient subgradient method and optimal nonsmooth frank-wolfe method},
author = {Kiran K Thekumparampil, Prateek Jain, Praneeth Netrapalli, Sewoong Oh},
abstract = {We consider the classical setting of optimizing a nonsmooth Lipschitz continuous convex function over a convex constraint set, when having access to a (stochastic) first-order oracle (FO) for the function and a projection oracle (PO) for the constraint set. It is well known that to achieve -suboptimality in high-dimensions, FO calls are necessary. This is achieved by the projected subgradient method (PGD). However, PGD also entails PO calls, which may be computationally costlier than FO calls (eg nuclear norm constraints). Improving this PO calls complexity of PGD is largely unexplored, despite the fundamental nature of this problem and extensive literature. We present first such improvement. This only requires a mild assumption that the objective function, when extended to a slightly larger neighborhood of the constraint set, still remains Lipschitz and accessible via FO. In particular, we introduce MOPES method, which carefully combines Moreau-Yosida smoothing and accelerated first-order schemes. This is guaranteed to find a feasible -suboptimal solution using only PO calls and optimal FO calls. Further, instead of a PO if we only have a linear minimization oracle (LMO, a la Frank-Wolfe) to access the constraint set, an extension of our method, MOLES, finds a feasible -suboptimal solution using LMO calls and FO calls---both match known lower bounds, resolving a question left open since White (1993). Our experiments confirm that these methods achieve significant speedups over the state-of-the-art, for a problem with costly PO and LMO calls.},
journal = {Advances in Neural Information Processing Systems},
year = {2020},
pages = {12211-12224},
url = {allpapers/ThekumparampilJNO20.pdf}
}
@article{NagarajWBJN20,
title = {Least squares regression with markovian data: Fundamental limits and algorithms},
author = {Dheeraj Nagaraj, Xian Wu, Guy Bresler, Prateek Jain, Praneeth Netrapalli},
abstract = {We study the problem of least squares linear regression where the datapoints are dependent and are sampled from a Markov chain. We establish sharp information theoretic minimax lower bounds for this problem in terms of $\tmix $, the mixing time of the underlying Markov chain, under different noise settings. Our results establish that in general, optimization with Markovian data is strictly harder than optimization with independent data and a trivial algorithm (SGD-DD) that works with only one in every $\tmix $ samples, which are approximately independent, is minimax optimal. In fact, it is strictly better than the popular Stochastic Gradient Descent (SGD) method with constant step-size which is otherwise minimax optimal in the regression with independent data setting. Beyond a worst case analysis, we investigate whether structured datasets seen in practice such as Gaussian auto-regressive dynamics can admit more efficient optimization schemes. Surprisingly, even in this specific and natural setting, Stochastic Gradient Descent (SGD) with constant step-size is still no better than SGD-DD. Instead, we propose an algorithm based on experience replay--a popular reinforcement learning technique--that achieves a significantly better error rate. Our improved rate serves as one of the first results where an algorithm outperforms SGD-DD on an interesting Markov chain and also provides one of the first theoretical analyses to support the use of experience replay in practice.},
journal = {Advances in neural information processing systems},
year = {2020},
pages = {16666-16676},
url = {allpapers/NagarajWBJN20.pdf}
}
@article{ShahTRJN20,
title = {The pitfalls of simplicity bias in neural networks},
author = {Harshay Shah, Kaustav Tamuly, Aditi Raghunathan, Prateek Jain, Praneeth Netrapalli},
abstract = {Several works have proposed Simplicity Bias (SB)---the tendency of standard training procedures such as Stochastic Gradient Descent (SGD) to find simple models---to justify why neural networks generalize well [Arpit et al. 2017, Nakkiran et al. 2019, Valle-Perez et al. 2019]. However, the precise notion of simplicity remains vague. Furthermore, previous settings [Soudry et al. 2018, Gunasekar et al. 2018] that use SB to theoretically justify why neural networks generalize well do not simultaneously capture the non-robustness of neural networks---a widely observed phenomenon in practice [Goodfellow et al. 2014, Jo and Bengio 2017]. We attempt to reconcile SB and the superior standard generalization of neural networks with the non-robustness observed in practice by introducing piecewise-linear and image-based datasets, which (a) incorporate a precise notion of simplicity,(b) comprise multiple predictive features with varying levels of simplicity, and (c) capture the non-robustness of neural networks trained on real data. Using theory and empirics on these datasets, we make four observations:(i) SB of SGD and variants can be extreme: neural networks can exclusively rely on the simplest feature and remain invariant to all predictive complex features.(ii) The extreme aspect of SB could explain why seemingly benign distribution shifts and small adversarial perturbations significantly degrade model performance.(iii) Contrary to conventional wisdom, SB can also hurt generalization on the same data distribution, as SB persists even when the simplest feature has less predictive power than the more complex features.(iv) Common approaches to …},
journal = {Advances in Neural Information Processing Systems},
year = {2020},
pages = {9573-9585},
url = {allpapers/ShahTRJN20.pdf}
}
@article{SahaKSVJ20,
title = {RNNPool: Efficient non-linear pooling for RAM constrained inference},
author = {Oindrila Saha, Aditya Kusupati, Harsha Vardhan Simhadri, Manik Varma, Prateek Jain},
abstract = {Standard Convolutional Neural Networks (CNNs) designed for computer vision tasks tend to have large intermediate activation maps. These require large working memory and are thus unsuitable for deployment on resource-constrained devices typically used for inference on the edge. Aggressively downsampling the images via pooling or strided convolutions can address the problem but leads to a significant decrease in accuracy due to gross aggregation of the feature map by standard pooling operators. In this paper, we introduce RNNPool, a novel pooling operator based on Recurrent Neural Networks (RNNs), that efficiently aggregates features over large patches of an image and rapidly downsamples activation maps. Empirical evaluation indicates that an RNNPool layer can effectively replace multiple blocks in a variety of architectures such as MobileNets, DenseNet when applied to standard vision tasks like image classification and face detection. That is, RNNPool can significantly decrease computational complexity and peak memory usage for inference while retaining comparable accuracy. We use RNNPool with the standard S3FD architecture to construct a face detection method that achieves state-of-the-art MAP for tiny ARM Cortex-M4 class microcontrollers with under 256 KB of RAM. Code is released at https://github. com/Microsoft/EdgeML.},
journal = {Advances in Neural Information Processing Systems},
year = {2020},
pages = {20473-20484},
url = {allpapers/SahaKSVJ20.pdf}
}
@article{BudhirajaHYCKJ20,
title = {Rich-item recommendations for rich-users via gcnn: Exploiting dynamic and static side information},
author = {Amarjit Budhiraja, Gaurush Hiranandani, Navya Yarrabelly, Ayush Choure, Oluwasanmi Koyejo, Prateek Jain},
abstract = {},
journal = {arXiv preprint arXiv:2001.10495},
year = {2020},
pages = {},
url = {allpapers/BudhirajaHYCKJ20.pdf}
}
@article{DennisAMSSSJ19,
title = {Shallow RNNs: A method for accurate time-series classification on tiny devices},
author = {Don Kurian Dennis, Durmus Alp Emre Acar, Vikram Mandikal, Vinu Sankar Sadasivan, Harsha Vardhan Simhadri, Venkatesh Saligrama, Prateek Jain},
abstract = {Recurrent Neural Networks (RNNs) capture long dependencies and context, and hence are the key component of typical sequential data based tasks. However, the sequential nature of RNNs dictates a large inference cost for long sequences even if the hardware supports parallelization. To induce long-term dependencies, and yet admit parallelization, we introduce novel shallow RNNs. In this architecture, the first layer splits the input sequence and runs several independent RNNs. The second layer consumes the output of the first layer using a second RNN thus capturing long dependencies. We provide theoretical justification for our architecture under weak assumptions that we verify on real-world benchmarks. Furthermore, we show that for time-series classification, our technique leads to substantially improved inference time over standard RNNs without compromising accuracy. For example, we can deploy audio-keyword classification on tiny Cortex M4 devices (100MHz processor, 256KB RAM, no DSP available) which was not possible using standard RNN models. Similarly, using ShaRNN in the popular Listen-Attend-Spell (LAS) architecture for phoneme classification [4], we can reduce the lag in phoneme classification by 10-12x while maintaining state-of-the-art accuracy.},
journal = {},
year = {2019},
pages = {12916-12926},
url = {allpapers/DennisAMSSSJ19.pdf}
}
@article{BhatiaPNSJ19,
title = {On Scaling Data-Driven Loop Invariant Inference},
author = {Sahil Bhatia, Saswat Padhi, Nagarajan Natarajan, Rahul Sharma, Prateek Jain},
abstract = {Automated synthesis of inductive invariants is an important problem in software verification. Once all the invariants have been specified, software verification reduces to checking of verification conditions. Although static analyses to infer invariants have been studied for over forty years, recent years have seen a flurry of data-driven invariant inference techniques which guess invariants from examples instead of analyzing program text. However, these techniques have been demonstrated to scale only to programs with a small number of variables. In this paper, we study these scalability issues and address them in our tool oasis that improves the scale of data-driven invariant inference and outperforms state-of-the-art systems on benchmarks from the invariant inference track of the Syntax Guided Synthesis competition.},
journal = {arXiv preprint arXiv:1911.11728},
year = {2019},
pages = {},
url = {allpapers/BhatiaPNSJ19.pdf}
}
@article{JainVLG19,
title = {Homomorphic data analysis},
author = {Prateek Jain, Ramarathnam Venkatesan, Jonathan Lee, Kartik Gupta},
abstract = {Systems, methods, and computer-executable instructions for homomorphic data analysis. Encrypted data is received, from a remote system, that has been encrypted with an encryption key. A number of iterations to iterate over the encrypted data is determined. A model is iterated over by the number of iterations to create an intermediate model. Each iteration updates the model, and the model and the intermediate model encrypted with the encryption key. The intermediate model is provided to the remote system. An updated model based upon the intermediate model is received from the remote system. The updated model is iterated over until a predetermined precision is reached to create a final model. The final model is provided to the remote system. The final model is encrypted with the encryption key.},
journal = {},
year = {2019},
pages = {},
url = {allpapers/JainVLG19.pdf}
}
@article{PatilDPSSSVJ19,
title = {Gesturepod: Enabling on-device gesture-based interaction for white cane users},
author = {Shishir G Patil, Don Kurian Dennis, Chirag Pabbaraju, Nadeem Shaheer, Harsha Vardhan Simhadri, Vivek Seshadri, Manik Varma, Prateek Jain},
abstract = {People using white canes for navigation find it challenging to concurrently access devices such as smartphones. Building on prior research on abandonment of specialized devices, we explore a new touch free mode of interaction wherein a person with visual impairment can perform gestures on their existing white cane to trigger tasks on their smartphone. We present GesturePod, an easy-to-integrate device that clips on to any white cane, and detects gestures performed with the cane. With GesturePod, a user can perform common tasks on their smartphone without touch or even removing the phone from their pocket or bag. We discuss the challenges in building the device and our design choices. We propose a novel, efficient machine learning pipeline to train and deploy the gesture recognition model. Our in-lab study shows that GesturePod achieves 92% gesture recognition accuracy and can help perform …},
journal = {},
year = {2019},
pages = {403-415},
url = {allpapers/PatilDPSSSVJ19.pdf}
}
@article{GulwaniJPPP19,
title = {Syntactic profiling of alphanumeric strings},
author = {Sumit Gulwani, Prateek Jain, Daniel Adam Perelman, Saswat Padhi, Oleksandr Polozov},
abstract = {A computing device includes a storage machine holding instructions executable by a logic machine to generate multi-string clusters, each containing alphanumeric strings of a dataset. Further multi-string clusters are generated via iterative performance of a combination operation in which a hierarchically-superior cluster is generated from a set of multi-string clusters. The combination operation includes, for candidate pairs of multi-string clusters, generating syntactic profiles describing an alphanumeric string from each multi-string cluster of the candidate pair. For each of the candidate pairs, a cost factor is determined for at least one of its syntactic profiles. Based on the cost factors determined for the syntactic profiles, one of the candidate pairs is selected. The multi-string clusters from the selected candidate pair are combined to generate the hierarchically-superior cluster including all of the alphanumeric strings from …},
journal = {},
year = {2019},
pages = {},
url = {allpapers/GulwaniJPPP19.pdf}
}
@article{GuptaWNKJR19,
title = {Distributional semantics meets multi-label learning},
author = {Vivek Gupta, Rahul Wadbude, Nagarajan Natarajan, Harish Karnick, Prateek Jain, Piyush Rai},
abstract = {We present a label embedding based approach to large-scale multi-label learning, drawing inspiration from ideas rooted in distributional semantics, specifically the Skip Gram Negative Sampling (SGNS) approach, widely used to learn word embeddings. Besides leading to a highly scalable model for multi-label learning, our approach highlights interesting connections between label embedding methods commonly used for multi-label learning and paragraph embedding methods commonly used for learning representations of text data. The framework easily extends to incorporating auxiliary information such as label-label correlations; this is crucial especially when many training instances are only partially annotated. To facilitate end-to-end learning, we develop a joint learning algorithm that can learn the embeddings as well as a regression model that predicts these embeddings for the new input to be annotated, via efficient gradient based methods. We demonstrate the effectiveness of our approach through an extensive set of experiments on a variety of benchmark datasets, and show that the proposed models perform favorably as compared to state-of-the-art methods for large-scale multi-label learning.},
journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
year = {2019},
pages = {3747-3754},
url = {allpapers/GuptaWNKJR19.pdf}
}
@article{PabbarajuJ19,
title = {Learning functions over sets via permutation adversarial networks},
author = {Chirag Pabbaraju, Prateek Jain},
abstract = {In this paper, we consider the problem of learning functions over sets, i.e., functions that are invariant to permutations of input set items. Recent approaches of pooling individual element embeddings can necessitate extremely large embedding sizes for challenging functions. We address this challenge by allowing standard neural networks like LSTMs to succinctly capture the function over the set. However, to ensure invariance with respect to permutations of set elements, we propose a novel architecture called SPAN that simultaneously learns the function as well as adversarial or worst-case permutations for each input set. The learning problem reduces to a min-max optimization problem that is solved via a simple alternating block coordinate descent technique. We conduct extensive experiments on a variety of set-learning tasks and demonstrate that SPAN learns nearly permutation-invariant functions while still ensuring accuracy on test data. On a variety of tasks sampled from the domains of statistics, graph functions and linear algebra, we show that our method can significantly outperform state-of-the-art methods such as DeepSets and Janossy Pooling. Finally, we present a case study of how learning set-functions can help extract powerful features for recommendation systems, and show that such a method can be as much as 2% more accurate than carefully hand-tuned features on a real-world recommendation system.},
journal = {arXiv preprint arXiv:1907.05638},
year = {2019},
pages = {},
url = {allpapers/PabbarajuJ19.pdf}
}
@article{ThekumparampilJNO19,
title = {Efficient Algorithms for Smooth Minimax Optimization},
author = {Kiran Koshy Thekumparampil, Prateek Jain, Praneeth Netrapalli, Sewoong Oh},
abstract = {This paper studies first order methods for solving smooth minimax optimization problems where is smooth and is concave for each . In terms of , we consider two settings--strongly convex and nonconvex--and improve upon the best known rates in both. For strongly-convex $ g (\cdot, y),\\forall y $, we propose a new algorithm combining Mirror-Prox and Nesterov's AGD, and show that it can find global optimum in iterations, improving over current state-of-the-art rate of . We use this result along with an inexact proximal point method to provide rate for finding stationary points in the nonconvex setting where can be nonconvex. This improves over current best-known rate of . Finally, we instantiate our result for finite nonconvex minimax problems, ie, , with nonconvex , to obtain convergence rate of total gradient …},
journal = {arXiv e-prints},
year = {2019},
pages = {arXiv: 1907.01543},
url = {allpapers/ThekumparampilJNO19.pdf}
}
@inproceedings{GeJKKNN19,
title = {Open Problem: Do Good Algorithms Necessarily Query Bad Points?},
author = {Rong Ge, Prateek Jain, Sham M Kakade, Rahul Kidambi, Dheeraj M Nagaraj, Praneeth Netrapalli},
abstract = {Folklore results in the theory of Stochastic Approximation indicates the (minimax) optimality of Stochastic Gradient Descent (SGD)(Robbins and Monro, 1951) with polynomially decaying stepsizes and iterate averaging (Ruppert, 1988; Polyak and Juditsky, 1992) for classes of stochastic convex optimization. Basing of these folkore results and some recent developments, this manuscript considers a more subtle question: does any algorithm necessarily (information theoretically) have to query iterates that are sub-optimal infinitely often?},
booktitle = {Conference on Learning Theory},
year = {2019},
pages = {3190-3193},
url = {allpapers/GeJKKNN19.pdf}
}
@inproceedings{JainNN19,
title = {Making the last iterate of sgd information theoretically optimal},
author = {Prateek Jain, Dheeraj Nagaraj, Praneeth Netrapalli},
abstract = {Stochastic gradient descent (SGD) is one of the most widely used algorithms for large scale optimization problems. While classical theoretical analysis of SGD for convex problems studies (suffix)\emph {averages} of iterates and obtains information theoretically optimal bounds on suboptimality, the\emph {last point} of SGD is, by far, the most preferred choice in practice. The best known results for last point of SGD (Shamir and Zhang, 2013) however, are suboptimal compared to information theoretic lower bounds by a factor, where is the number of iterations. Harvey et. al (2018) shows that in fact, this additional factor is tight for standard step size sequences of $\OTheta {\frac {1}{\sqrt {t}}} $ and $\OTheta {\frac {1}{t}} $ for non-strongly convex and strongly convex settings, respectively. Similarly, even for subgradient descent (GD) when applied to non-smooth, convex functions, the best known step-size sequences still lead to -suboptimal convergence rates (on the final iterate). The main contribution of this work is to design new step size sequences that enjoy information theoretically optimal bounds on the suboptimality of\emph {last point} of SGD as well as GD. We achieve this by designing a modification scheme, that converts one sequence of step sizes to another so that the last point of SGD/GD with modified sequence has the same suboptimality guarantees as the average of SGD/GD with original sequence. We also show that our result holds with high-probability. We validate our results through simulations which demonstrate that the new step size sequence indeed improves the final iterate significantly compared to the …},
booktitle = {Conference on Learning Theory},
year = {2019},
pages = {1752-1755},
url = {allpapers/JainNN19.pdf}
}
@inproceedings{SuggalaBRJ19,
title = {Adaptive hard thresholding for near-optimal consistent robust regression},
author = {Arun Sai Suggala, Kush Bhatia, Pradeep Ravikumar, Prateek Jain},
abstract = {We study the problem of robust linear regression with response variable corruptions. We consider the oblivious adversary model, where the adversary corrupts a fraction of the responses in complete ignorance of the data. We provide a nearly linear time estimator which consistently estimates the true regression vector, even with fraction of corruptions. Existing results in this setting either don’t guarantee consistent estimates or can only handle a small fraction of corruptions. We also extend our estimator to robust sparse linear regression and show that similar guarantees hold in this setting. Finally, we apply our estimator to the problem of linear regression with heavy-tailed noise and show that our estimator consistently estimates the regression vector even when the noise has unbounded variance (eg, Cauchy distribution), for which most existing results don’t even apply. Our estimator is based on a novel variant of outlier removal via hard thresholding in which the threshold is chosen adaptively and crucially relies on randomness to escape bad fixed points of the non-convex hard thresholding operation.},
booktitle = {Conference on Learning Theory},
year = {2019},
pages = {2892-2897},
url = {allpapers/SuggalaBRJ19.pdf}
}
@article{Jain19,
title = {Gradient Methods for Non-convex Optimization},
author = {Prateek Jain},
abstract = {Non-convex optimization forms bedrock of most modern machine learning (ML) techniques such as deep learning. While non-convex optimization problems have been studied for the past several decades, ML-based problems have significantly different characteristics and requirements due to large datasets and high-dimensional parameter spaces along with the statistical nature of the problem. Over the last few years, there has been a flurry of activity in non-convex optimization for such ML problems. This article surveys a few of the foundational approaches in this domain.},
journal = {},
year = {2019},
pages = {247-256},
url = {allpapers/Jain19.pdf}
}
@inproceedings{NagarajJN19,
title = {Sgd without replacement: Sharper rates for general smooth convex functions},
author = {Dheeraj Nagaraj, Prateek Jain, Praneeth Netrapalli},
abstract = {We study stochastic gradient descent without replacement (SGDo) for smooth convex functions. SGDo is widely observed to converge faster than true SGD where each sample is drawn independently with replacement (Bottou, 2009) and hence, is more popular in practice. But it’s convergence properties are not well understood as sampling without replacement leads to coupling between iterates and gradients. By using method of exchangeable pairs to bound Wasserstein distance, we provide the first non-asymptotic results for SGDo when applied to general smooth, strongly-convex functions. In particular, we show that SGDo converges at a rate of while SGD is known to converge at rate, where denotes the number of passes over data and is required to be large enough. Existing results for SGDo in this setting require additional Hessian Lipschitz assumption (Gurbuzbalaban et al, 2015; HaoChen and Sra 2018). For small , we show SGDo can achieve same convergence rate as SGD for general smooth strongly-convex functions. Existing results in this setting require and hold only for generalized linear models (Shamir, 2016). In addition, by careful analysis of the coupling, for both large and small , we obtain better dependence on problem dependent parameters like condition number.},
booktitle = {International Conference on Machine Learning},
year = {2019},
pages = {4703-4711},
url = {allpapers/NagarajJN19.pdf}
}
@article{SomaniGJN19,
title = {Universality Patterns in the Training of Neural Networks},
author = {Raghav Somani, Navin Goyal, Prateek Jain, Praneeth Netrapalli},
abstract = {This paper proposes and demonstrates a surprising pattern in the training of neural networks: there is a one to one relation between the values of any pair of losses (such as cross entropy, mean squared error, 0/1 error etc.) evaluated for a model arising at (any point of) a training run. This pattern is universal in the sense that this one to one relationship is identical across architectures (such as VGG, Resnet, Densenet etc.), algorithms (SGD and SGD with momentum) and training loss functions (cross entropy and mean squared error).},
journal = {},
year = {2019},
pages = {},
url = {allpapers/SomaniGJN19.pdf}
}
