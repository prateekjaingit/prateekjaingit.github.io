{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read Pub Results from SerpAPI\n",
    "import json\n",
    "f = open('pub_results.json',)\n",
    "# returns JSON object as\n",
    "# a dictionary\n",
    "results = json.load(f)\n",
    "\n",
    "results=results[0:98]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am here\n",
      "{'title': 'Privacy-enhanced training and deployment of machine learning models using client-side and server-side data', 'link': 'https://patents.google.com/patent/US20240054391A1/en', 'inventors': 'Abhradeep Guha Thakurta, Li Zhang, Prateek Jain, Shuang Song, Steffen Rendle, Steve Shaw-Tang Chien, Walid Krichene, Yarong Mu', 'publication_date': '2024/2/15', 'patent_office': 'US', 'application_number': '17928372', 'description': 'Computer-implemented systems and methods for training a decentralized model for making a personalized recommendation. In one aspect, the method comprising: obtaining, using user activity data, client-side training data that includes features and training labels; and training, by the client device, a decentralized model in training rounds, wherein training, in each training round comprises: receiving, first data including a current server-side embedding generated by the server-side machine learning model, wherein the first data received from the server does not include any server-side data used in generating the current server-side embedding; generating, using the client-side machine learning model, a client-side embedding based on the client-side training data; updating, using the client-side embedding and the current server-side embedding and based on the training labels, the client-side machine learning …', 'scholar_articles': [{'title': 'Privacy-enhanced training and deployment of machine learning models using client-side and server-side data', 'link': 'https://scholar.google.com/scholar?oi=bibs&cluster=1463710471403848759&btnI=1&hl=en', 'authors': 'AG Thakurta, L Zhang, P Jain, S Song, S Rendle… - US Patent App. 17/928,372, 2024', 'related_pages_link': {'link': 'https://scholar.google.com/scholar?oi=bibs&hl=en&q=related:N8QLLuskUBQJ:scholar.google.com/'}, 'versions': {'total': 2, 'link': 'https://scholar.google.com/scholar?oi=bibs&hl=en&cluster=1463710471403848759', 'serpapi_link': 'https://serpapi.com/search.json?cluster=1463710471403848759&engine=google_scholar&hl=en', 'cluster_id': '1463710471403848759'}}]}\n",
      "resource not found\n",
      "{'title': 'Privacy-enhanced training and deployment of machine learning models using client-side and server-side data', 'link': 'https://patents.google.com/patent/US20240054391A1/en', 'inventors': 'Abhradeep Guha Thakurta, Li Zhang, Prateek Jain, Shuang Song, Steffen Rendle, Steve Shaw-Tang Chien, Walid Krichene, Yarong Mu', 'publication_date': '2024/2/15', 'patent_office': 'US', 'application_number': '17928372', 'description': 'Computer-implemented systems and methods for training a decentralized model for making a personalized recommendation. In one aspect, the method comprising: obtaining, using user activity data, client-side training data that includes features and training labels; and training, by the client device, a decentralized model in training rounds, wherein training, in each training round comprises: receiving, first data including a current server-side embedding generated by the server-side machine learning model, wherein the first data received from the server does not include any server-side data used in generating the current server-side embedding; generating, using the client-side machine learning model, a client-side embedding based on the client-side training data; updating, using the client-side embedding and the current server-side embedding and based on the training labels, the client-side machine learning …', 'scholar_articles': [{'title': 'Privacy-enhanced training and deployment of machine learning models using client-side and server-side data', 'link': 'https://scholar.google.com/scholar?oi=bibs&cluster=1463710471403848759&btnI=1&hl=en', 'authors': 'AG Thakurta, L Zhang, P Jain, S Song, S Rendle… - US Patent App. 17/928,372, 2024', 'related_pages_link': {'link': 'https://scholar.google.com/scholar?oi=bibs&hl=en&q=related:N8QLLuskUBQJ:scholar.google.com/'}, 'versions': {'total': 2, 'link': 'https://scholar.google.com/scholar?oi=bibs&hl=en&cluster=1463710471403848759', 'serpapi_link': 'https://serpapi.com/search.json?cluster=1463710471403848759&engine=google_scholar&hl=en', 'cluster_id': '1463710471403848759'}}]}\n",
      "{'title': 'Tandem Transformers for Inference Efficient LLMs', 'link': 'https://ui.adsabs.harvard.edu/abs/2024arXiv240208644S/abstract', 'authors': 'Pranav Ajit Nair, Yashas Samaga, Toby Boyd, Sanjiv Kumar, Prateek Jain, Praneeth Netrapalli', 'publication_date': '2024/2', 'journal': 'arXiv e-prints', 'pages': 'arXiv: 2402.08644', 'description': \"The autoregressive nature of conventional large language models (LLMs) inherently limits inference speed, as tokens are generated sequentially. While speculative and parallel decoding techniques attempt to mitigate this, they face limitations: either relying on less accurate smaller models for generation or failing to fully leverage the base LLM's representations. We introduce a novel architecture, Tandem transformers, to address these issues. This architecture uniquely combines (1) a small autoregressive model and (2) a large model operating in block mode (processing multiple tokens simultaneously). The small model's predictive accuracy is substantially enhanced by granting it attention to the large model's richer representations. On the PaLM2 pretraining dataset, a tandem of PaLM2-Bison and PaLM2-Gecko demonstrates a 3.3% improvement in next-token prediction accuracy over a standalone PaLM2-Gecko …\", 'scholar_articles': [{'title': 'Tandem Transformers for Inference Efficient LLMs', 'link': 'https://scholar.google.com/scholar?oi=bibs&cluster=13324521967300059278&btnI=1&hl=en', 'authors': 'P Ajit Nair, Y Samaga, T Boyd, S Kumar, P Jain… - arXiv e-prints, 2024', 'related_pages_link': {'link': 'https://scholar.google.com/scholar?oi=bibs&hl=en&q=related:jiDPBiI26rgJ:scholar.google.com/'}}]}\n",
      "resource not found\n",
      "{'title': 'Tandem Transformers for Inference Efficient LLMs', 'link': 'https://ui.adsabs.harvard.edu/abs/2024arXiv240208644S/abstract', 'authors': 'Pranav Ajit Nair, Yashas Samaga, Toby Boyd, Sanjiv Kumar, Prateek Jain, Praneeth Netrapalli', 'publication_date': '2024/2', 'journal': 'arXiv e-prints', 'pages': 'arXiv: 2402.08644', 'description': \"The autoregressive nature of conventional large language models (LLMs) inherently limits inference speed, as tokens are generated sequentially. While speculative and parallel decoding techniques attempt to mitigate this, they face limitations: either relying on less accurate smaller models for generation or failing to fully leverage the base LLM's representations. We introduce a novel architecture, Tandem transformers, to address these issues. This architecture uniquely combines (1) a small autoregressive model and (2) a large model operating in block mode (processing multiple tokens simultaneously). The small model's predictive accuracy is substantially enhanced by granting it attention to the large model's richer representations. On the PaLM2 pretraining dataset, a tandem of PaLM2-Bison and PaLM2-Gecko demonstrates a 3.3% improvement in next-token prediction accuracy over a standalone PaLM2-Gecko …\", 'scholar_articles': [{'title': 'Tandem Transformers for Inference Efficient LLMs', 'link': 'https://scholar.google.com/scholar?oi=bibs&cluster=13324521967300059278&btnI=1&hl=en', 'authors': 'P Ajit Nair, Y Samaga, T Boyd, S Kumar, P Jain… - arXiv e-prints, 2024', 'related_pages_link': {'link': 'https://scholar.google.com/scholar?oi=bibs&hl=en&q=related:jiDPBiI26rgJ:scholar.google.com/'}}]}\n",
      "i am here\n",
      "i am here\n",
      "{'title': 'Attention neural networks with tree attention mechanisms', 'link': 'https://patents.google.com/patent/US20240005131A1/en', 'inventors': 'Himanshu Jain, Lovish Madaan, Prateek Jain, Venkata Sesha Pavana Srinadh Bhojanapalli', 'publication_date': '2024/1/4', 'patent_office': 'US', 'application_number': '18343723', 'description': 'Systems and methods for processing inputs using attention neural networks with tree attention layers. Each tree attention layer includes one or more tree attention sub-layers that are each configured to: process query vectors using a decision tree model for the tree attention sub-layer to determine a respective tree path for each query vector; process key vectors using the decision tree model to determine a respective tree path for each key vector; and generate an attended input sequence comprising a respective attended input at each of the plurality of input positions, comprising: generating, for each particular input position, the respective attended input at the particular input position based on (i) the tree path for the query vector at the particular input position (ii) the respective tree paths for the key vectors at each of the plurality of input positions and (iii) the value vectors at a subset of the input positions.', 'scholar_articles': [{'title': 'Attention neural networks with tree attention mechanisms', 'link': 'https://scholar.google.com/scholar?oi=bibs&cluster=2807722144324427988&btnI=1&hl=en', 'authors': 'H Jain, L Madaan, P Jain, VSPS Bhojanapalli - US Patent App. 18/343,723, 2024', 'related_pages_link': {'link': 'https://scholar.google.com/scholar?oi=bibs&hl=en&q=related:1KB0EGoI9yYJ:scholar.google.com/'}, 'versions': {'total': 2, 'link': 'https://scholar.google.com/scholar?oi=bibs&hl=en&cluster=2807722144324427988', 'serpapi_link': 'https://serpapi.com/search.json?cluster=2807722144324427988&engine=google_scholar&hl=en', 'cluster_id': '2807722144324427988'}}]}\n",
      "resource not found\n",
      "{'title': 'Attention neural networks with tree attention mechanisms', 'link': 'https://patents.google.com/patent/US20240005131A1/en', 'inventors': 'Himanshu Jain, Lovish Madaan, Prateek Jain, Venkata Sesha Pavana Srinadh Bhojanapalli', 'publication_date': '2024/1/4', 'patent_office': 'US', 'application_number': '18343723', 'description': 'Systems and methods for processing inputs using attention neural networks with tree attention layers. Each tree attention layer includes one or more tree attention sub-layers that are each configured to: process query vectors using a decision tree model for the tree attention sub-layer to determine a respective tree path for each query vector; process key vectors using the decision tree model to determine a respective tree path for each key vector; and generate an attended input sequence comprising a respective attended input at each of the plurality of input positions, comprising: generating, for each particular input position, the respective attended input at the particular input position based on (i) the tree path for the query vector at the particular input position (ii) the respective tree paths for the key vectors at each of the plurality of input positions and (iii) the value vectors at a subset of the input positions.', 'scholar_articles': [{'title': 'Attention neural networks with tree attention mechanisms', 'link': 'https://scholar.google.com/scholar?oi=bibs&cluster=2807722144324427988&btnI=1&hl=en', 'authors': 'H Jain, L Madaan, P Jain, VSPS Bhojanapalli - US Patent App. 18/343,723, 2024', 'related_pages_link': {'link': 'https://scholar.google.com/scholar?oi=bibs&hl=en&q=related:1KB0EGoI9yYJ:scholar.google.com/'}, 'versions': {'total': 2, 'link': 'https://scholar.google.com/scholar?oi=bibs&hl=en&cluster=2807722144324427988', 'serpapi_link': 'https://serpapi.com/search.json?cluster=2807722144324427988&engine=google_scholar&hl=en', 'cluster_id': '2807722144324427988'}}]}\n",
      "{'title': 'LEARNING AN INVERTIBLE OUTPUT MAPPING CAN MITIGATE SIMPLICITY BIAS IN NEURAL NETWORKS', 'link': 'http://research.google/pubs/learning-an-invertible-output-mapping-can-mitigate-simplicity-bias-in-neural-networks/', 'authors': 'Anshul Nasery, Praneeth Netrapalli, Prateek Jain, Sravanti Addepalli', 'publication_date': '2023', 'description': 'Deep Neural Networks (DNNs) are known to be brittle to even minor distribution shifts compared to the training distribution. Simplicity Bias (SB) of DNNs–bias towards learning a small number of simplest features–has been demonstrated to be a key reason for this brittleness. Prior works have shown that the effect of Simplicity Bias is extreme–even when the features learned are diverse, training the classification head again selects only few of the simplest features, leading to similarly brittle models. In this work, we introduce Feature Reconstruction Regularizer (FRR) in the linear classification head, with the aim of reducing Simplicity Bias, thereby improving Out-Of-Distribution (OOD) robustness. The proposed regularizer when used during linear layer training, termed as FRR-L, enforces that the features can be reconstructed back from the logit layer, ensuring that diverse features participate in the classification task …', 'scholar_articles': [{'title': 'LEARNING AN INVERTIBLE OUTPUT MAPPING CAN MITIGATE SIMPLICITY BIAS IN NEURAL NETWORKS', 'link': 'https://scholar.google.com/scholar?oi=bibs&cluster=10686118695432894192&btnI=1&hl=en', 'authors': 'A Nasery, P Netrapalli, P Jain, S Addepalli - 2023', 'related_pages_link': {'link': 'https://scholar.google.com/scholar?oi=bibs&hl=en&q=related:8DbwBFa4TJQJ:scholar.google.com/'}}]}\n",
      "resource not found\n",
      "{'title': 'LEARNING AN INVERTIBLE OUTPUT MAPPING CAN MITIGATE SIMPLICITY BIAS IN NEURAL NETWORKS', 'link': 'http://research.google/pubs/learning-an-invertible-output-mapping-can-mitigate-simplicity-bias-in-neural-networks/', 'authors': 'Anshul Nasery, Praneeth Netrapalli, Prateek Jain, Sravanti Addepalli', 'publication_date': '2023', 'description': 'Deep Neural Networks (DNNs) are known to be brittle to even minor distribution shifts compared to the training distribution. Simplicity Bias (SB) of DNNs–bias towards learning a small number of simplest features–has been demonstrated to be a key reason for this brittleness. Prior works have shown that the effect of Simplicity Bias is extreme–even when the features learned are diverse, training the classification head again selects only few of the simplest features, leading to similarly brittle models. In this work, we introduce Feature Reconstruction Regularizer (FRR) in the linear classification head, with the aim of reducing Simplicity Bias, thereby improving Out-Of-Distribution (OOD) robustness. The proposed regularizer when used during linear layer training, termed as FRR-L, enforces that the features can be reconstructed back from the logit layer, ensuring that diverse features participate in the classification task …', 'scholar_articles': [{'title': 'LEARNING AN INVERTIBLE OUTPUT MAPPING CAN MITIGATE SIMPLICITY BIAS IN NEURAL NETWORKS', 'link': 'https://scholar.google.com/scholar?oi=bibs&cluster=10686118695432894192&btnI=1&hl=en', 'authors': 'A Nasery, P Netrapalli, P Jain, S Addepalli - 2023', 'related_pages_link': {'link': 'https://scholar.google.com/scholar?oi=bibs&hl=en&q=related:8DbwBFa4TJQJ:scholar.google.com/'}}]}\n",
      "i am here\n",
      "{'title': 'Differentially private model personalization', 'link': 'https://open.bu.edu/handle/2144/44959', 'authors': 'Adam Smith, Prateek Jain, Keith Rush, Shuang Song, Abhradeep G Thakurta', 'publication_date': '2021/12/6', 'description': \"We study personalization of supervised learning with user-level differential privacy. Consider a setting with many users, each of whom has a training data set drawn from their own distribution Pi . Assuming some shared structure among the problems Pi, can users collectively learn the shared structure---and solve their tasks better than they could individually---while preserving the privacy of their data? We formulate this question using joint, user-level differential privacy---that is, we control what is leaked about each user's entire data set. We provide algorithms that exploit popular non-private approaches in this domain like the Almost-No-Inner-Loop (ANIL) method, and give strong user-level privacy guarantees for our general approach. When the problems Pi are linear regression problems with each user's regression vector lying in a common, unknown low-dimensional subspace, we show that our efficient algorithms satisfy nearly optimal estimation error guarantees. We also establish a general, information-theoretic upper bound via an exponential mechanism-based algorithm.\", 'scholar_articles': [{'title': 'Differentially private model personalization', 'link': 'https://scholar.google.com/scholar?oi=bibs&cluster=4321064999095321088&btnI=1&hl=en', 'authors': 'A Smith, P Jain, K Rush, S Song, AG Thakurta - 2021', 'related_pages_link': {'link': 'https://scholar.google.com/scholar?oi=bibs&hl=en&q=related:AEJzArmB9zsJ:scholar.google.com/'}, 'versions': {'total': 3, 'link': 'https://scholar.google.com/scholar?oi=bibs&hl=en&cluster=4321064999095321088', 'serpapi_link': 'https://serpapi.com/search.json?cluster=4321064999095321088&engine=google_scholar&hl=en', 'cluster_id': '4321064999095321088'}}]}\n",
      "resource not found\n",
      "{'title': 'Differentially private model personalization', 'link': 'https://open.bu.edu/handle/2144/44959', 'authors': 'Adam Smith, Prateek Jain, Keith Rush, Shuang Song, Abhradeep G Thakurta', 'publication_date': '2021/12/6', 'description': \"We study personalization of supervised learning with user-level differential privacy. Consider a setting with many users, each of whom has a training data set drawn from their own distribution Pi . Assuming some shared structure among the problems Pi, can users collectively learn the shared structure---and solve their tasks better than they could individually---while preserving the privacy of their data? We formulate this question using joint, user-level differential privacy---that is, we control what is leaked about each user's entire data set. We provide algorithms that exploit popular non-private approaches in this domain like the Almost-No-Inner-Loop (ANIL) method, and give strong user-level privacy guarantees for our general approach. When the problems Pi are linear regression problems with each user's regression vector lying in a common, unknown low-dimensional subspace, we show that our efficient algorithms satisfy nearly optimal estimation error guarantees. We also establish a general, information-theoretic upper bound via an exponential mechanism-based algorithm.\", 'scholar_articles': [{'title': 'Differentially private model personalization', 'link': 'https://scholar.google.com/scholar?oi=bibs&cluster=4321064999095321088&btnI=1&hl=en', 'authors': 'A Smith, P Jain, K Rush, S Song, AG Thakurta - 2021', 'related_pages_link': {'link': 'https://scholar.google.com/scholar?oi=bibs&hl=en&q=related:AEJzArmB9zsJ:scholar.google.com/'}, 'versions': {'total': 3, 'link': 'https://scholar.google.com/scholar?oi=bibs&hl=en&cluster=4321064999095321088', 'serpapi_link': 'https://serpapi.com/search.json?cluster=4321064999095321088&engine=google_scholar&hl=en', 'cluster_id': '4321064999095321088'}}]}\n",
      "i am here\n",
      "i am here\n",
      "{'title': 'Sample Efficient Linear Meta-Learning by Alternating Minimization', 'link': 'https://ui.adsabs.harvard.edu/abs/2021arXiv210508306K/abstract', 'authors': 'Kiran Koshy Thekumparampil, Prateek Jain, Praneeth Netrapalli, Sewoong Oh', 'publication_date': '2021/5', 'journal': 'arXiv e-prints', 'pages': 'arXiv: 2105.08306', 'description': 'Meta-learning synthesizes and leverages the knowledge from a given set of tasks to rapidly learn new tasks using very little data. Meta-learning of linear regression tasks, where the regressors lie in a low-dimensional subspace, is an extensively-studied fundamental problem in this domain. However, existing results either guarantee highly suboptimal estimation errors, or require samples per task (where is the data dimensionality) thus providing little gain over separately learning each task. In this work, we study a simple alternating minimization method (MLLAM), which alternately learns the low-dimensional subspace and the regressors. We show that, for a constant subspace dimension MLLAM obtains nearly-optimal estimation error, despite requiring only samples per task. However, the number of samples required per task grows logarithmically with the number of tasks. To remedy this in the low …', 'scholar_articles': [{'title': 'Sample Efficient Linear Meta-Learning by Alternating Minimization', 'link': 'https://scholar.google.com/scholar?oi=bibs&cluster=11873867299457948815&btnI=1&hl=en', 'authors': 'K Koshy Thekumparampil, P Jain, P Netrapalli, S Oh - arXiv e-prints, 2021', 'related_pages_link': {'link': 'https://scholar.google.com/scholar?oi=bibs&hl=en&q=related:j1xR_mVzyKQJ:scholar.google.com/'}}]}\n",
      "resource not found\n",
      "{'title': 'Sample Efficient Linear Meta-Learning by Alternating Minimization', 'link': 'https://ui.adsabs.harvard.edu/abs/2021arXiv210508306K/abstract', 'authors': 'Kiran Koshy Thekumparampil, Prateek Jain, Praneeth Netrapalli, Sewoong Oh', 'publication_date': '2021/5', 'journal': 'arXiv e-prints', 'pages': 'arXiv: 2105.08306', 'description': 'Meta-learning synthesizes and leverages the knowledge from a given set of tasks to rapidly learn new tasks using very little data. Meta-learning of linear regression tasks, where the regressors lie in a low-dimensional subspace, is an extensively-studied fundamental problem in this domain. However, existing results either guarantee highly suboptimal estimation errors, or require samples per task (where is the data dimensionality) thus providing little gain over separately learning each task. In this work, we study a simple alternating minimization method (MLLAM), which alternately learns the low-dimensional subspace and the regressors. We show that, for a constant subspace dimension MLLAM obtains nearly-optimal estimation error, despite requiring only samples per task. However, the number of samples required per task grows logarithmically with the number of tasks. To remedy this in the low …', 'scholar_articles': [{'title': 'Sample Efficient Linear Meta-Learning by Alternating Minimization', 'link': 'https://scholar.google.com/scholar?oi=bibs&cluster=11873867299457948815&btnI=1&hl=en', 'authors': 'K Koshy Thekumparampil, P Jain, P Netrapalli, S Oh - arXiv e-prints, 2021', 'related_pages_link': {'link': 'https://scholar.google.com/scholar?oi=bibs&hl=en&q=related:j1xR_mVzyKQJ:scholar.google.com/'}}]}\n",
      "{'title': 'Making the last iterate of sgd information theoretically optimal', 'link': 'https://epubs.siam.org/doi/abs/10.1137/19M128908X', 'authors': 'Prateek Jain, Dheeraj M Nagaraj, Praneeth Netrapalli', 'publication_date': '2021', 'journal': 'SIAM Journal on Optimization', 'volume': '31', 'issue': '2', 'pages': '1108-1130', 'publisher': 'Society for Industrial and Applied Mathematics', 'description': 'Stochastic gradient descent (SGD) is one of the most widely used algorithms for large-scale optimization problems. While classical theoretical analysis of SGD for convex problems studies (suffix) averages of iterates and obtains information theoretically optimal bounds on suboptimality, the last point of SGD is, by far, the most preferred choice in practice. The best known results for the last point of SGD [O. Shamir and T. Zhang, Proceedings of the 30th International Conference on Machine Learning, 2013, pp. 71--79] however, are suboptimal compared to information theoretic lower bounds by a factor, where is the number of iterations. Harvey, Liaw, Plan, and Randhawa [Conference on Learning Theory, PMLR, 2019, pp. 1579--1613] shows that in fact, this additional factor is tight for standard step size sequences of and for non-strongly convex and strongly convex settings, respectively. Similarly …', 'total_citations': {'cited_by': {'total': 17, 'link': 'https://scholar.google.com/scholar?oi=bibs&hl=en&cites=17804181748849386284&as_sdt=5', 'serpapi_link': 'https://serpapi.com/search.json?cites=17804181748849386284&engine=google_scholar&hl=en', 'cites_id': '17804181748849386284'}, 'table': [{'year': 2021, 'citations': 1}, {'year': 2022, 'citations': 8}, {'year': 2023, 'citations': 4}, {'year': 2024, 'citations': 4}]}, 'scholar_articles': [{'title': 'Making the last iterate of sgd information theoretically optimal', 'link': 'https://scholar.google.com/scholar?oi=bibs&cluster=17804181748849386284&btnI=1&hl=en', 'authors': 'P Jain, DM Nagaraj, P Netrapalli - SIAM Journal on Optimization, 2021', 'cited_by': {'total': 17, 'link': 'https://scholar.google.com/scholar?oi=bibs&hl=en&cites=17804181748849386284&as_sdt=5', 'serpapi_link': 'https://serpapi.com/search.json?cites=17804181748849386284&engine=google_scholar&hl=en', 'cites_id': '17804181748849386284'}, 'related_pages_link': {'link': 'https://scholar.google.com/scholar?oi=bibs&hl=en&q=related:LCOpUPgoFfcJ:scholar.google.com/'}, 'versions': {'total': 2, 'link': 'https://scholar.google.com/scholar?oi=bibs&hl=en&cluster=17804181748849386284', 'serpapi_link': 'https://serpapi.com/search.json?cluster=17804181748849386284&engine=google_scholar&hl=en', 'cluster_id': '17804181748849386284'}}]}\n",
      "resource not found\n",
      "{'title': 'Making the last iterate of sgd information theoretically optimal', 'link': 'https://epubs.siam.org/doi/abs/10.1137/19M128908X', 'authors': 'Prateek Jain, Dheeraj M Nagaraj, Praneeth Netrapalli', 'publication_date': '2021', 'journal': 'SIAM Journal on Optimization', 'volume': '31', 'issue': '2', 'pages': '1108-1130', 'publisher': 'Society for Industrial and Applied Mathematics', 'description': 'Stochastic gradient descent (SGD) is one of the most widely used algorithms for large-scale optimization problems. While classical theoretical analysis of SGD for convex problems studies (suffix) averages of iterates and obtains information theoretically optimal bounds on suboptimality, the last point of SGD is, by far, the most preferred choice in practice. The best known results for the last point of SGD [O. Shamir and T. Zhang, Proceedings of the 30th International Conference on Machine Learning, 2013, pp. 71--79] however, are suboptimal compared to information theoretic lower bounds by a factor, where is the number of iterations. Harvey, Liaw, Plan, and Randhawa [Conference on Learning Theory, PMLR, 2019, pp. 1579--1613] shows that in fact, this additional factor is tight for standard step size sequences of and for non-strongly convex and strongly convex settings, respectively. Similarly …', 'total_citations': {'cited_by': {'total': 17, 'link': 'https://scholar.google.com/scholar?oi=bibs&hl=en&cites=17804181748849386284&as_sdt=5', 'serpapi_link': 'https://serpapi.com/search.json?cites=17804181748849386284&engine=google_scholar&hl=en', 'cites_id': '17804181748849386284'}, 'table': [{'year': 2021, 'citations': 1}, {'year': 2022, 'citations': 8}, {'year': 2023, 'citations': 4}, {'year': 2024, 'citations': 4}]}, 'scholar_articles': [{'title': 'Making the last iterate of sgd information theoretically optimal', 'link': 'https://scholar.google.com/scholar?oi=bibs&cluster=17804181748849386284&btnI=1&hl=en', 'authors': 'P Jain, DM Nagaraj, P Netrapalli - SIAM Journal on Optimization, 2021', 'cited_by': {'total': 17, 'link': 'https://scholar.google.com/scholar?oi=bibs&hl=en&cites=17804181748849386284&as_sdt=5', 'serpapi_link': 'https://serpapi.com/search.json?cites=17804181748849386284&engine=google_scholar&hl=en', 'cites_id': '17804181748849386284'}, 'related_pages_link': {'link': 'https://scholar.google.com/scholar?oi=bibs&hl=en&q=related:LCOpUPgoFfcJ:scholar.google.com/'}, 'versions': {'total': 2, 'link': 'https://scholar.google.com/scholar?oi=bibs&hl=en&cluster=17804181748849386284', 'serpapi_link': 'https://serpapi.com/search.json?cluster=17804181748849386284&engine=google_scholar&hl=en', 'cluster_id': '17804181748849386284'}}]}\n",
      "{'title': 'Near-optimal Offline and Streaming Algorithms for Learning Non-Linear Dynamical Systems', 'link': 'https://research.google/pubs/near-optimal-offline-and-streaming-algorithms-for-learning-non-linear-dynamical-systems/', 'authors': 'Dheeraj Nagaraj, Praneeth Netrapalli, Prateek Jain, Suhas Kowshik', 'publication_date': '2021', 'description': 'We study the problem of learning vector valued non-linear dynamical systems from a single trajectory of {\\\\em dependent or correlated} points. We assume a known link function that satisfy a certain {\\\\em expansivity property}. While the problem is well-studied in the linear case with strong learning guarantees even for non-mixing systems, the results in non-linear case hold only for mixing systems and even then the error rates are significantly sub-optimal. In this work, we bridge this gap in a variety of settings: a) we provide first optimal offline algorithm that can learn non-linear dynamical systems without mixing assumption, b) in the much harder one-pass, streaming setting we study a SGD with Reverse Experience Replay ($\\\\sgdber $) method, and demonstrate that for mixing systems, it achieves nearly optimal performance even for heavy-tailed noise, c) we justify the expansivity assumption by showing that when …', 'scholar_articles': [{'title': 'Near-optimal Offline and Streaming Algorithms for Learning Non-Linear Dynamical Systems', 'link': 'https://scholar.google.com/scholar?oi=bibs&cluster=17477769008792990557&btnI=1&hl=en', 'authors': 'D Nagaraj, P Netrapalli, P Jain, S Kowshik - 2021', 'related_pages_link': {'link': 'https://scholar.google.com/scholar?oi=bibs&hl=en&q=related:XfNv80-CjfIJ:scholar.google.com/'}}]}\n",
      "resource not found\n",
      "{'title': 'Near-optimal Offline and Streaming Algorithms for Learning Non-Linear Dynamical Systems', 'link': 'https://research.google/pubs/near-optimal-offline-and-streaming-algorithms-for-learning-non-linear-dynamical-systems/', 'authors': 'Dheeraj Nagaraj, Praneeth Netrapalli, Prateek Jain, Suhas Kowshik', 'publication_date': '2021', 'description': 'We study the problem of learning vector valued non-linear dynamical systems from a single trajectory of {\\\\em dependent or correlated} points. We assume a known link function that satisfy a certain {\\\\em expansivity property}. While the problem is well-studied in the linear case with strong learning guarantees even for non-mixing systems, the results in non-linear case hold only for mixing systems and even then the error rates are significantly sub-optimal. In this work, we bridge this gap in a variety of settings: a) we provide first optimal offline algorithm that can learn non-linear dynamical systems without mixing assumption, b) in the much harder one-pass, streaming setting we study a SGD with Reverse Experience Replay ($\\\\sgdber $) method, and demonstrate that for mixing systems, it achieves nearly optimal performance even for heavy-tailed noise, c) we justify the expansivity assumption by showing that when …', 'scholar_articles': [{'title': 'Near-optimal Offline and Streaming Algorithms for Learning Non-Linear Dynamical Systems', 'link': 'https://scholar.google.com/scholar?oi=bibs&cluster=17477769008792990557&btnI=1&hl=en', 'authors': 'D Nagaraj, P Netrapalli, P Jain, S Kowshik - 2021', 'related_pages_link': {'link': 'https://scholar.google.com/scholar?oi=bibs&hl=en&q=related:XfNv80-CjfIJ:scholar.google.com/'}}]}\n",
      "{'title': 'Streaming Linear System Identification with Reverse Experience Replay', 'link': 'https://research.google/pubs/streaming-linear-system-identification-with-reverse-experience-replay/', 'authors': 'Dheeraj Nagaraj, Praneeth Netrapalli, Prateek Jain, Suhas Kowshik', 'publication_date': '2021', 'description': 'We consider the problem of estimating a stochastic linear time-invariant (LTI) dynamical system from a single trajectory via streaming algorithms. The problem is equivalent to estimating the parameters of vector auto-regressive ($\\\\var $) models encountered in time series analysis (\\\\cite {hamilton2020time}). A recent sequence of papers\\\\citep {faradonbeh2018finite, simchowitz2018learning, sarkar2019near} show that ordinary least squares (OLS) regression can be used to provide optimal finite time estimator for the problem. However, such techniques apply for {\\\\em offline} setting where the optimal solution of OLS is available {\\\\em apriori}. But, in many problems of interest as encountered in reinforcement learning (RL), it is important to estimate the parameters on the go using gradient oracle. This task is challenging since standard methods like SGD might not perform well when using stochastic gradients from …', 'scholar_articles': [{'title': 'Streaming Linear System Identification with Reverse Experience Replay', 'link': 'https://scholar.google.com/scholar?oi=bibs&cluster=17183527213412979535&btnI=1&hl=en', 'authors': 'D Nagaraj, P Netrapalli, P Jain, S Kowshik - 2021', 'related_pages_link': {'link': 'https://scholar.google.com/scholar?oi=bibs&hl=en&q=related:TxO9V_UmeO4J:scholar.google.com/'}}]}\n",
      "resource not found\n",
      "{'title': 'Streaming Linear System Identification with Reverse Experience Replay', 'link': 'https://research.google/pubs/streaming-linear-system-identification-with-reverse-experience-replay/', 'authors': 'Dheeraj Nagaraj, Praneeth Netrapalli, Prateek Jain, Suhas Kowshik', 'publication_date': '2021', 'description': 'We consider the problem of estimating a stochastic linear time-invariant (LTI) dynamical system from a single trajectory via streaming algorithms. The problem is equivalent to estimating the parameters of vector auto-regressive ($\\\\var $) models encountered in time series analysis (\\\\cite {hamilton2020time}). A recent sequence of papers\\\\citep {faradonbeh2018finite, simchowitz2018learning, sarkar2019near} show that ordinary least squares (OLS) regression can be used to provide optimal finite time estimator for the problem. However, such techniques apply for {\\\\em offline} setting where the optimal solution of OLS is available {\\\\em apriori}. But, in many problems of interest as encountered in reinforcement learning (RL), it is important to estimate the parameters on the go using gradient oracle. This task is challenging since standard methods like SGD might not perform well when using stochastic gradients from …', 'scholar_articles': [{'title': 'Streaming Linear System Identification with Reverse Experience Replay', 'link': 'https://scholar.google.com/scholar?oi=bibs&cluster=17183527213412979535&btnI=1&hl=en', 'authors': 'D Nagaraj, P Netrapalli, P Jain, S Kowshik - 2021', 'related_pages_link': {'link': 'https://scholar.google.com/scholar?oi=bibs&hl=en&q=related:TxO9V_UmeO4J:scholar.google.com/'}}]}\n",
      "{'title': 'Differentially Private Model Personalization', 'link': 'https://scholar.google.com/scholar?cluster=15796128257138184621&hl=en&oi=scholarr', 'authors': 'Abhradeep Guha Thakurta, Adam Smith, Keith Rush, Prateek Jain, Shuang Song', 'publication_date': '2021', 'description': \"We study personalization of supervised learning with user-level differential privacy. Consider a setting with many users, each of whom has a training data set drawn from their own distribution . Assuming some shared structure among the problems , can users collectively learn the shared structure---and solve their tasks better than they could individually---while preserving the privacy of their data? We formulate this question using joint,\\\\textit {user-level} differential privacy---that is, we control what is leaked about each user's entire data set.\", 'scholar_articles': [{'title': 'Differentially Private Model Personalization', 'link': 'https://scholar.google.com/scholar?oi=bibs&cluster=15796128257138184621&btnI=1&hl=en', 'authors': 'AG Thakurta, A Smith, K Rush, P Jain, S Song - 2021', 'related_pages_link': {'link': 'https://scholar.google.com/scholar?oi=bibs&hl=en&q=related:rVE7WfUeN9sJ:scholar.google.com/'}}]}\n",
      "resource not found\n",
      "{'title': 'Differentially Private Model Personalization', 'link': 'https://scholar.google.com/scholar?cluster=15796128257138184621&hl=en&oi=scholarr', 'authors': 'Abhradeep Guha Thakurta, Adam Smith, Keith Rush, Prateek Jain, Shuang Song', 'publication_date': '2021', 'description': \"We study personalization of supervised learning with user-level differential privacy. Consider a setting with many users, each of whom has a training data set drawn from their own distribution . Assuming some shared structure among the problems , can users collectively learn the shared structure---and solve their tasks better than they could individually---while preserving the privacy of their data? We formulate this question using joint,\\\\textit {user-level} differential privacy---that is, we control what is leaked about each user's entire data set.\", 'scholar_articles': [{'title': 'Differentially Private Model Personalization', 'link': 'https://scholar.google.com/scholar?oi=bibs&cluster=15796128257138184621&btnI=1&hl=en', 'authors': 'AG Thakurta, A Smith, K Rush, P Jain, S Song - 2021', 'related_pages_link': {'link': 'https://scholar.google.com/scholar?oi=bibs&hl=en&q=related:rVE7WfUeN9sJ:scholar.google.com/'}}]}\n",
      "{'title': 'Sample Efficient Linear Meta-Learning', 'link': 'https://research.google/pubs/sample-efficient-linear-meta-learning/', 'authors': 'Kiran Koshy Thekumparampil, Praneeth Netrapalli, Prateek Jain, Sewoong Oh', 'publication_date': '2021', 'description': 'Meta-learning algorithms synthesizes and leverages the knowledge from a given set of tasks to rapidly learn new tasks using very little data. While methods like ANIL\\\\cite {raghu2019rapid} have been demonstrated to be effective in practical meta-learning problems, their statistical and computational properties are ill-understood. Recent theoretical studies of meta-learning problem in a simple linear/non-linear regression setting still do not explain practical success of the meta-learning approach. For example, existing results either guarantee highly suboptimal estimation errors\\\\cite {tripuraneni2020provable} or require relatively large number of samples per task\\\\cite {}-- samples where is the data dimensionality--which runs counter to practical settings. Additionally, the prescribed algorithms are inefficient and typically are not used in practice.% to achieve these sample complexity are high inefficient. Similar to the …', 'scholar_articles': [{'title': 'Sample Efficient Linear Meta-Learning', 'link': 'https://scholar.google.com/scholar?oi=bibs&cluster=14253344746026045544&btnI=1&hl=en', 'authors': 'KK Thekumparampil, P Netrapalli, P Jain, S Oh - 2021', 'related_pages_link': {'link': 'https://scholar.google.com/scholar?oi=bibs&hl=en&q=related:aBAQcIcNzsUJ:scholar.google.com/'}}]}\n",
      "resource not found\n",
      "{'title': 'Sample Efficient Linear Meta-Learning', 'link': 'https://research.google/pubs/sample-efficient-linear-meta-learning/', 'authors': 'Kiran Koshy Thekumparampil, Praneeth Netrapalli, Prateek Jain, Sewoong Oh', 'publication_date': '2021', 'description': 'Meta-learning algorithms synthesizes and leverages the knowledge from a given set of tasks to rapidly learn new tasks using very little data. While methods like ANIL\\\\cite {raghu2019rapid} have been demonstrated to be effective in practical meta-learning problems, their statistical and computational properties are ill-understood. Recent theoretical studies of meta-learning problem in a simple linear/non-linear regression setting still do not explain practical success of the meta-learning approach. For example, existing results either guarantee highly suboptimal estimation errors\\\\cite {tripuraneni2020provable} or require relatively large number of samples per task\\\\cite {}-- samples where is the data dimensionality--which runs counter to practical settings. Additionally, the prescribed algorithms are inefficient and typically are not used in practice.% to achieve these sample complexity are high inefficient. Similar to the …', 'scholar_articles': [{'title': 'Sample Efficient Linear Meta-Learning', 'link': 'https://scholar.google.com/scholar?oi=bibs&cluster=14253344746026045544&btnI=1&hl=en', 'authors': 'KK Thekumparampil, P Netrapalli, P Jain, S Oh - 2021', 'related_pages_link': {'link': 'https://scholar.google.com/scholar?oi=bibs&hl=en&q=related:aBAQcIcNzsUJ:scholar.google.com/'}}]}\n",
      "{'title': 'Private Alternating Least Squares:(Nearly) OptimalPrivacy/Utility Trade-off for Matrix Completion', 'link': 'https://scholar.google.com/scholar?cluster=1282070183723790876&hl=en&oi=scholarr', 'authors': 'Abhradeep Guha Thakurta, Li Zhang, Prateek Jain, Shuang Song, Steffen Rendle, Walid Krichene', 'publication_date': '2021', 'description': 'We study the problem of differentially private (DP) matrix completion under user-level privacy. We design an -joint differentially private variant of the popular Alternating-Least-Squares (ALS) method that achieves: i)(nearly) optimal sample complexity for matrix completion (in terms of number of items, users), and ii) best known privacy/utility trade-off both theoretically, as well as on benchmark data sets.', 'scholar_articles': [{'title': 'Private Alternating Least Squares:(Nearly) OptimalPrivacy/Utility Trade-off for Matrix Completion', 'link': 'https://scholar.google.com/scholar?oi=bibs&cluster=1282070183723790876&btnI=1&hl=en', 'authors': 'AG Thakurta, L Zhang, P Jain, S Song, S Rendle… - 2021', 'related_pages_link': {'link': 'https://scholar.google.com/scholar?oi=bibs&hl=en&q=related:HPL5FQrUyhEJ:scholar.google.com/'}}]}\n",
      "resource not found\n",
      "{'title': 'Private Alternating Least Squares:(Nearly) OptimalPrivacy/Utility Trade-off for Matrix Completion', 'link': 'https://scholar.google.com/scholar?cluster=1282070183723790876&hl=en&oi=scholarr', 'authors': 'Abhradeep Guha Thakurta, Li Zhang, Prateek Jain, Shuang Song, Steffen Rendle, Walid Krichene', 'publication_date': '2021', 'description': 'We study the problem of differentially private (DP) matrix completion under user-level privacy. We design an -joint differentially private variant of the popular Alternating-Least-Squares (ALS) method that achieves: i)(nearly) optimal sample complexity for matrix completion (in terms of number of items, users), and ii) best known privacy/utility trade-off both theoretically, as well as on benchmark data sets.', 'scholar_articles': [{'title': 'Private Alternating Least Squares:(Nearly) OptimalPrivacy/Utility Trade-off for Matrix Completion', 'link': 'https://scholar.google.com/scholar?oi=bibs&cluster=1282070183723790876&btnI=1&hl=en', 'authors': 'AG Thakurta, L Zhang, P Jain, S Song, S Rendle… - 2021', 'related_pages_link': {'link': 'https://scholar.google.com/scholar?oi=bibs&hl=en&q=related:HPL5FQrUyhEJ:scholar.google.com/'}}]}\n",
      "{'title': 'Projection Efficient Subgradient Method and Optimal Nonsmooth Frank-Wolfe Method', 'link': 'https://ui.adsabs.harvard.edu/abs/2020arXiv201001848K/abstract', 'authors': 'Kiran Koshy Thekumparampil, Prateek Jain, Praneeth Netrapalli, Sewoong Oh', 'publication_date': '2020/10', 'journal': 'arXiv e-prints', 'pages': 'arXiv: 2010.01848', 'description': 'We consider the classical setting of optimizing a nonsmooth Lipschitz continuous convex function over a convex constraint set, when having access to a (stochastic) first-order oracle (FO) for the function and a projection oracle (PO) for the constraint set. It is well known that to achieve -suboptimality in high-dimensions, FO calls are necessary. This is achieved by the projected subgradient method (PGD). However, PGD also entails PO calls, which may be computationally costlier than FO calls (eg nuclear norm constraints). Improving this PO calls complexity of PGD is largely unexplored, despite the fundamental nature of this problem and extensive literature. We present first such improvement. This only requires a mild assumption that the objective function, when extended to a slightly larger neighborhood of the constraint set, still remains Lipschitz and accessible via FO. In particular, we introduce MOPES method, which …', 'scholar_articles': [{'title': 'Projection Efficient Subgradient Method and Optimal Nonsmooth Frank-Wolfe Method', 'link': 'https://scholar.google.com/scholar?oi=bibs&cluster=1670282268539912821&btnI=1&hl=en', 'authors': 'K Koshy Thekumparampil, P Jain, P Netrapalli, S Oh - arXiv e-prints, 2020', 'related_pages_link': {'link': 'https://scholar.google.com/scholar?oi=bibs&hl=en&q=related:dYL0qt8ILhcJ:scholar.google.com/'}}]}\n",
      "resource not found\n",
      "{'title': 'Projection Efficient Subgradient Method and Optimal Nonsmooth Frank-Wolfe Method', 'link': 'https://ui.adsabs.harvard.edu/abs/2020arXiv201001848K/abstract', 'authors': 'Kiran Koshy Thekumparampil, Prateek Jain, Praneeth Netrapalli, Sewoong Oh', 'publication_date': '2020/10', 'journal': 'arXiv e-prints', 'pages': 'arXiv: 2010.01848', 'description': 'We consider the classical setting of optimizing a nonsmooth Lipschitz continuous convex function over a convex constraint set, when having access to a (stochastic) first-order oracle (FO) for the function and a projection oracle (PO) for the constraint set. It is well known that to achieve -suboptimality in high-dimensions, FO calls are necessary. This is achieved by the projected subgradient method (PGD). However, PGD also entails PO calls, which may be computationally costlier than FO calls (eg nuclear norm constraints). Improving this PO calls complexity of PGD is largely unexplored, despite the fundamental nature of this problem and extensive literature. We present first such improvement. This only requires a mild assumption that the objective function, when extended to a slightly larger neighborhood of the constraint set, still remains Lipschitz and accessible via FO. In particular, we introduce MOPES method, which …', 'scholar_articles': [{'title': 'Projection Efficient Subgradient Method and Optimal Nonsmooth Frank-Wolfe Method', 'link': 'https://scholar.google.com/scholar?oi=bibs&cluster=1670282268539912821&btnI=1&hl=en', 'authors': 'K Koshy Thekumparampil, P Jain, P Netrapalli, S Oh - arXiv e-prints, 2020', 'related_pages_link': {'link': 'https://scholar.google.com/scholar?oi=bibs&hl=en&q=related:dYL0qt8ILhcJ:scholar.google.com/'}}]}\n",
      "i am here\n",
      "{'title': 'Rich-item recommendations for rich-users via gcnn: Exploiting dynamic and static side information', 'link': 'https://scholar.google.com/scholar?cluster=6706550175366707566&hl=en&oi=scholarr', 'authors': 'Amarjit Budhiraja, Gaurush Hiranandani, Navya Yarrabelly, Ayush Choure, Oluwasanmi Koyejo, Prateek Jain', 'publication_date': '2020', 'journal': 'arXiv preprint arXiv:2001.10495', 'total_citations': {'cited_by': {'total': 4, 'link': 'https://scholar.google.com/scholar?oi=bibs&hl=en&cites=6706550175366707566&as_sdt=5', 'serpapi_link': 'https://serpapi.com/search.json?cites=6706550175366707566&engine=google_scholar&hl=en', 'cites_id': '6706550175366707566'}, 'table': [{'year': 2020, 'citations': 1}, {'year': 2021, 'citations': 1}, {'year': 2022, 'citations': 1}, {'year': 2023, 'citations': 1}]}, 'scholar_articles': [{'title': 'Rich-item recommendations for rich-users via gcnn: Exploiting dynamic and static side information', 'link': 'https://scholar.google.com/scholar?oi=bibs&cluster=6706550175366707566&btnI=1&hl=en', 'authors': 'A Budhiraja, G Hiranandani, N Yarrabelly, A Choure… - arXiv preprint arXiv:2001.10495, 2020', 'cited_by': {'total': 4, 'link': 'https://scholar.google.com/scholar?oi=bibs&hl=en&cites=6706550175366707566&as_sdt=5', 'serpapi_link': 'https://serpapi.com/search.json?cites=6706550175366707566&engine=google_scholar&hl=en', 'cites_id': '6706550175366707566'}, 'related_pages_link': {'link': 'https://scholar.google.com/scholar?oi=bibs&hl=en&q=related:bvkQB9pzEl0J:scholar.google.com/'}}]}\n",
      "resource not found\n",
      "{'title': 'Rich-item recommendations for rich-users via gcnn: Exploiting dynamic and static side information', 'link': 'https://scholar.google.com/scholar?cluster=6706550175366707566&hl=en&oi=scholarr', 'authors': 'Amarjit Budhiraja, Gaurush Hiranandani, Navya Yarrabelly, Ayush Choure, Oluwasanmi Koyejo, Prateek Jain', 'publication_date': '2020', 'journal': 'arXiv preprint arXiv:2001.10495', 'total_citations': {'cited_by': {'total': 4, 'link': 'https://scholar.google.com/scholar?oi=bibs&hl=en&cites=6706550175366707566&as_sdt=5', 'serpapi_link': 'https://serpapi.com/search.json?cites=6706550175366707566&engine=google_scholar&hl=en', 'cites_id': '6706550175366707566'}, 'table': [{'year': 2020, 'citations': 1}, {'year': 2021, 'citations': 1}, {'year': 2022, 'citations': 1}, {'year': 2023, 'citations': 1}]}, 'scholar_articles': [{'title': 'Rich-item recommendations for rich-users via gcnn: Exploiting dynamic and static side information', 'link': 'https://scholar.google.com/scholar?oi=bibs&cluster=6706550175366707566&btnI=1&hl=en', 'authors': 'A Budhiraja, G Hiranandani, N Yarrabelly, A Choure… - arXiv preprint arXiv:2001.10495, 2020', 'cited_by': {'total': 4, 'link': 'https://scholar.google.com/scholar?oi=bibs&hl=en&cites=6706550175366707566&as_sdt=5', 'serpapi_link': 'https://serpapi.com/search.json?cites=6706550175366707566&engine=google_scholar&hl=en', 'cites_id': '6706550175366707566'}, 'related_pages_link': {'link': 'https://scholar.google.com/scholar?oi=bibs&hl=en&q=related:bvkQB9pzEl0J:scholar.google.com/'}}]}\n",
      "i am here\n",
      "i am here\n",
      "{'title': 'Efficient Algorithms for Smooth Minimax Optimization', 'link': 'https://ui.adsabs.harvard.edu/abs/2019arXiv190701543K/abstract', 'authors': 'Kiran Koshy Thekumparampil, Prateek Jain, Praneeth Netrapalli, Sewoong Oh', 'publication_date': '2019/7', 'journal': 'arXiv e-prints', 'pages': 'arXiv: 1907.01543', 'description': \"This paper studies first order methods for solving smooth minimax optimization problems where is smooth and is concave for each . In terms of , we consider two settings--strongly convex and nonconvex--and improve upon the best known rates in both. For strongly-convex $ g (\\\\cdot, y),\\\\\\\\forall y $, we propose a new algorithm combining Mirror-Prox and Nesterov's AGD, and show that it can find global optimum in iterations, improving over current state-of-the-art rate of . We use this result along with an inexact proximal point method to provide rate for finding stationary points in the nonconvex setting where can be nonconvex. This improves over current best-known rate of . Finally, we instantiate our result for finite nonconvex minimax problems, ie, , with nonconvex , to obtain convergence rate of total gradient …\", 'scholar_articles': [{'title': 'Efficient Algorithms for Smooth Minimax Optimization', 'link': 'https://scholar.google.com/scholar?oi=bibs&cluster=2176004634233111712&btnI=1&hl=en', 'authors': 'K Koshy Thekumparampil, P Jain, P Netrapalli, S Oh - arXiv e-prints, 2019', 'related_pages_link': {'link': 'https://scholar.google.com/scholar?oi=bibs&hl=en&q=related:oIQRla-4Mh4J:scholar.google.com/'}}]}\n",
      "resource not found\n",
      "{'title': 'Efficient Algorithms for Smooth Minimax Optimization', 'link': 'https://ui.adsabs.harvard.edu/abs/2019arXiv190701543K/abstract', 'authors': 'Kiran Koshy Thekumparampil, Prateek Jain, Praneeth Netrapalli, Sewoong Oh', 'publication_date': '2019/7', 'journal': 'arXiv e-prints', 'pages': 'arXiv: 1907.01543', 'description': \"This paper studies first order methods for solving smooth minimax optimization problems where is smooth and is concave for each . In terms of , we consider two settings--strongly convex and nonconvex--and improve upon the best known rates in both. For strongly-convex $ g (\\\\cdot, y),\\\\\\\\forall y $, we propose a new algorithm combining Mirror-Prox and Nesterov's AGD, and show that it can find global optimum in iterations, improving over current state-of-the-art rate of . We use this result along with an inexact proximal point method to provide rate for finding stationary points in the nonconvex setting where can be nonconvex. This improves over current best-known rate of . Finally, we instantiate our result for finite nonconvex minimax problems, ie, , with nonconvex , to obtain convergence rate of total gradient …\", 'scholar_articles': [{'title': 'Efficient Algorithms for Smooth Minimax Optimization', 'link': 'https://scholar.google.com/scholar?oi=bibs&cluster=2176004634233111712&btnI=1&hl=en', 'authors': 'K Koshy Thekumparampil, P Jain, P Netrapalli, S Oh - arXiv e-prints, 2019', 'related_pages_link': {'link': 'https://scholar.google.com/scholar?oi=bibs&hl=en&q=related:oIQRla-4Mh4J:scholar.google.com/'}}]}\n"
     ]
    }
   ],
   "source": [
    "#Parse Pub Results to get information required to generate bib files\n",
    "i=0\n",
    "all_pubs=[]\n",
    "for result in results: \n",
    "    try:\n",
    "        pub_entry={}\n",
    "        pub_entry['title']=result['citation']['title']\n",
    "        pub_entry['patent']='False'\n",
    "        try:\n",
    "            pub_entry['authors']=result['citation']['authors']\n",
    "        except KeyError:\n",
    "            pub_entry['authors']=result['citation']['inventors']\n",
    "            pub_entry['patent']='True'\n",
    "            print('i am here')\n",
    "        pub_entry['date']=result['citation']['publication_date']\n",
    "        all_authors=pub_entry['authors'].split(',')\n",
    "        first_letters = [author.split(' ')[-1][0] for author in all_authors[1:]]\n",
    "        pub_entry['bibkey']=all_authors[0].split(' ')[-1]+''.join(first_letters)+pub_entry['date'].split('/')[0][-2:]\n",
    "        pub_entry['url']='all_papers/'+pub_entry['bibkey']+'.pdf'\n",
    "        try:\n",
    "            pub_entry['conference']=result['citation']['conference']\n",
    "        except KeyError:\n",
    "            pub_entry['conference']=\"\"\n",
    "            pub_entry['journal']=''\n",
    "\n",
    "        try:\n",
    "            pub_entry['journal']=result['citation']['journal']\n",
    "        except KeyError:\n",
    "            j=10\n",
    "\n",
    "\n",
    "\n",
    "        try:\n",
    "            pub_entry['pages']=result['citation']['pages']\n",
    "        except KeyError:\n",
    "            pub_entry['pages']=\"\"\n",
    "\n",
    "        try:\n",
    "            pub_entry['description']=result['citation']['description']\n",
    "        except KeyError:\n",
    "            pub_entry['description']=\"\"\n",
    "\n",
    "        pub_entry['fileformat']=\"\"\n",
    "        pub_entry['pdflink']=\"\"\n",
    "        try: \n",
    "            resources=result['citation']['resources'][0]\n",
    "            pub_entry['fileformat']=resources['file_format']\n",
    "            pub_entry['pdflink']=resources['link']\n",
    "        except KeyError:\n",
    "            print(result['citation'])\n",
    "            print('resource not found')\n",
    "            pub_entry['pdflink']=result['link']\n",
    "\n",
    "        all_pubs.append(pub_entry)\n",
    "    except KeyError:\n",
    "        print(result['citation'])\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate Bib Entries\n",
    "import os\n",
    "import calendar\n",
    "new_pubs_conf=''\n",
    "new_pubs_journal=''\n",
    "new_pubs_patent=''\n",
    "i=0\n",
    "for pub in all_pubs:\n",
    "    new_pub=''\n",
    "\n",
    "    if pub['conference']=='':\n",
    "        if pub['journal']=='Advances in Neural Information Processing Systems':\n",
    "            pub['conference']='Advances in Neural Information Processing Systems (NeurIPS)'\n",
    "            pub['journal']=''\n",
    "\n",
    "    if pub['conference']=='':\n",
    "        new_pub=new_pub+'@article{'+pub['bibkey']+',\\n'\n",
    "    else:\n",
    "        new_pub=new_pub+'@inproceedings{'+pub['bibkey']+',\\n'\n",
    "    new_pub=new_pub+'title = {'+pub['title']+'},\\n'\n",
    "    new_pub=new_pub+'author = {'+pub['authors']+'},\\n'\n",
    "    new_pub=new_pub+'abstract = {'+pub['description']+'},\\n'\n",
    "    if pub['conference']=='':\n",
    "        new_pub=new_pub+'journal = {'+pub['journal']+'},\\n'\n",
    "    else:\n",
    "        new_pub=new_pub+'booktitle = {'+pub['conference']+'},\\n'\n",
    "\n",
    "    new_pub=new_pub+'year = {'+pub['date'].split('/')[0]+'},\\n'\n",
    "    new_pub=new_pub+'pages = {'+pub['pages']+'},\\n'\n",
    "    try:\n",
    "        new_pub=new_pub+'month = '+calendar.month_name[int(pub['date'].split('/')[1])][0:3].lower()+',\\n'\n",
    "    except IndexError:\n",
    "        new_pub=new_pub+'month = '+'dec'+',\\n'\n",
    "    if pub['patent']=='True':\n",
    "        new_pub=new_pub+'url = {'+pub['pdflink']+'}\\n}'\n",
    "    else:\n",
    "        new_pub=new_pub+'url = {'+pub['url']+'}\\n}'\n",
    "\n",
    "    if pub['conference']=='':\n",
    "        if pub['patent']=='True':\n",
    "            new_pubs_patent=new_pubs_patent+'\\n'+new_pub\n",
    "        else:\n",
    "            new_pubs_journal=new_pubs_journal+'\\n'+new_pub\n",
    "    else:\n",
    "        new_pubs_conf=new_pubs_conf+'\\n'+new_pub\n",
    "    i=i+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Append the new bib results to the existing files\n",
    "f=open('pubs_colab_conf.bib','r+')\n",
    "content=f.read()\n",
    "f.seek(0, 0) #get to the first position\n",
    "f.write(new_pubs_conf)\n",
    "#f.write(new_pubs_conf+\"\\n\"+content)\n",
    "f.close()\n",
    "\n",
    "f=open('pubs_colab_journal.bib','r+')\n",
    "content=f.read()\n",
    "f.seek(0, 0) #get to the first position\n",
    "f.write(new_pubs_journal)\n",
    "#f.write(new_pubs_journal+\"\\n\"+content)\n",
    "f.close()\n",
    "\n",
    "f=open('pubs_colab_patent.bib','r+')\n",
    "content=f.read()\n",
    "f.seek(0, 0) #get to the first position\n",
    "f.write(new_pubs_patent)\n",
    "#f.write(new_pubs_patent+\"\\n\"+content)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new files\n",
    "f=open('pubs_colab_conf.bib','w+')\n",
    "f.write('')\n",
    "f.close()\n",
    "f=open('pubs_colab_journal.bib','w+')\n",
    "f.write('')\n",
    "f.close()\n",
    "f=open('pubs_colab_patent.bib','w+')\n",
    "f.write('')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "@article{MenonBHJSCYMLDD24,\n",
      "title = {Multiple-entity-based recommendation system},\n",
      "author = {Lekshmi Menon, Amar Budhiraja, Gaurush Hiranandani, Prateek Jain, Darshatkumar Anandji Shah, Ayush Choure, Navya Yarrabelly, Anurag Mishra, Mohammad Luqman, Shivangi Dhakad, Juhi Dua},\n",
      "abstract = {Systems and methods for entity recommendation can make use of rich data by allowing the items to be recommended and the recipients of the recommendation (eg, users) to be modeled as “complex entities” composed of one or more static sub-entities and/or a dynamic component, and by utilizing information about multiple relationships between the sub-entities as reflected in bipartite graphs. Generating recommendations from such information may involve creating vector representations of the sub-entities based on the bipartite graphs (eg, using graph-based convolutional networks), and combining these vector representations into representations of the items and users (or other recipients) to be fed into a classifier model.},\n",
      "journal = {},\n",
      "year = {2024},\n",
      "pages = {},\n",
      "month = jan,\n",
      "url = {https://patentimages.storage.googleapis.com/0d/3d/fe/40b87c2d92f29e/US20210049442A1.pdf}\n",
      "}\n",
      "@article{GulwaniJPPP21,\n",
      "title = {Syntactic profiling of alphanumeric strings},\n",
      "author = {Sumit Gulwani, Prateek Jain, Daniel Adam Perelman, Saswat Padhi, Oleksandr Polozov},\n",
      "abstract = {A computing device includes a storage machine holding instructions executable by a logic machine to generate multi-string clusters, each containing alphanumeric strings of a dataset. Further multi-string clusters are generated via iterative performance of a combination operation in which a hierarchically-superior cluster is generated from a set of multi-string clusters. The combination operation includes, for candidate pairs of multi-string clusters, generating syn tactic profiles describing an alphanumeric string from each multi-string cluster of the candidate pair. For each of the candidate pairs, a cost factor is determined for at least one of its syntactic profiles. Based on the cost factors determined for the syntactic profiles, one of the candidate pairs is selected. The multi-string clusters from the selected candi date pair are combined to generate the hierarchically-supe rior cluster including all of the alphanumeric strings …},\n",
      "journal = {},\n",
      "year = {2021},\n",
      "pages = {},\n",
      "month = dec,\n",
      "url = {https://patentimages.storage.googleapis.com/df/e4/fe/26362bc393c571/US11210327.pdf}\n",
      "}\n",
      "@article{MenonBHJSCYMLDD21,\n",
      "title = {Message recommendation system},\n",
      "author = {Lekshmi Menon, Amar Budhiraja, Gaurush Hiranandani, Prateek Jain, Darshatkumar Anandji Shah, Ayush Choure, Navya Yarrabelly, Anurag Mishra, Mohammad Luqman, Shivangi Dhakad, Juhi Dua},\n",
      "abstract = {Systems and methods for entity recommendation can make use of rich data by allowing the items to be recommended and the recipients of the recommendation (eg, users) to be modeled as “complex entities” composed of one or more static sub-entities and/or a dynamic component, and by utilizing information about multiple relationships between the sub-entities as reflected in bipartite graphs. Generating recommendations from such information may involve creating vector representations of the sub-entities based on the bipartite graphs (eg, using graph-based convolutional networks), and combining these vector representations into representations of the items and users (or other recipients) to be fed into a classifier model.},\n",
      "journal = {},\n",
      "year = {2021},\n",
      "pages = {},\n",
      "month = nov,\n",
      "url = {https://patentimages.storage.googleapis.com/d1/34/42/d5bd9896ec9e37/US11184301.pdf}\n",
      "}\n",
      "@article{PolozovGJVM21,\n",
      "title = {Neural-guided deductive search for program synthesis},\n",
      "author = {Oleksandr Polozov, Sumit Gulwani, Prateek Jain, Ashwin Kalyan Vijayakumar, Abhishek Mohta},\n",
      "abstract = {Systems, methods, and computer-executable instructions for guiding program synthesis includes receiving a specification that includes an input and output example. Programs are synthesized that meet the specification. During synthesizing each of the programs includes branching decisions. Each branching decision includes a plurality of paths. Synthesizing the programs comprises includes selecting a first score model, for a first branching decision. Each of the programs is scored using the first score model. The paths of the first branching decision are pared based on the score. One the paths is selected. A synthesized program that meets the specification is returned. The synthesized program includes the one of the paths.},\n",
      "journal = {},\n",
      "year = {2021},\n",
      "pages = {},\n",
      "month = sep,\n",
      "url = {https://patentimages.storage.googleapis.com/ed/ad/bd/e0a2d73a3a7721/US11132180.pdf}\n",
      "}\n",
      "@article{JainVLG20,\n",
      "title = {Homomorphic factorization encryption},\n",
      "author = {Prateek Jain, Ramarathnam Venkatesan, Jonathan Lee, Kartik Gupta},\n",
      "abstract = {Systems, methods, and computer-executable instructions for secure data analysis using encrypted data. An encryption key and a decryption key are created. The security of encryption using the encryption key and the decryption key are based upon factoring. A computation key is created based upon the encryption key. Data is encrypted using the encryption key. The encrypted data and the computation key are provided to a remote system. The remote system is requested to perform data analysis on the encrypted data. An encrypted result of the data analysis is received from the remote system. The encrypted result of the data analysis is decrypted with the decryption key.},\n",
      "journal = {},\n",
      "year = {2020},\n",
      "pages = {},\n",
      "month = feb,\n",
      "url = {https://patentimages.storage.googleapis.com/5e/d4/45/d5a42bb576a1aa/US10554390.pdf}\n",
      "}\n",
      "@article{JainVLG19,\n",
      "title = {Homomorphic data analysis},\n",
      "author = {Prateek Jain, Ramarathnam Venkatesan, Jonathan Lee, Kartik Gupta},\n",
      "abstract = {Systems, methods, and computer-executable instructions for homomorphic data analysis. Encrypted data is received, from a remote system, that has been encrypted with an encryption key. A number of iterations to iterate over the encrypted data is determined. A model is iterated over by the number of iterations to create an intermediate model. Each iteration updates the model, and the model and the intermediate model encrypted with the encryption key. The intermediate model is provided to the remote system. An updated model based upon the intermediate model is received from the remote system. The updated model is iterated over until a predetermined precision is reached to create a final model. The final model is provided to the remote system. The final model is encrypted with the encryption key.},\n",
      "journal = {},\n",
      "year = {2019},\n",
      "pages = {},\n",
      "month = nov,\n",
      "url = {https://patentimages.storage.googleapis.com/bd/62/45/a33d543be86616/US10491373.pdf}\n",
      "}\n",
      "@article{GulwaniJPPP19,\n",
      "title = {Syntactic profiling of alphanumeric strings},\n",
      "author = {Sumit Gulwani, Prateek Jain, Daniel Adam Perelman, Saswat Padhi, Oleksandr Polozov},\n",
      "abstract = {A computing device includes a storage machine holding instructions executable by a logic machine to generate multi-string clusters, each containing alphanumeric strings of a dataset. Further multi-string clusters are generated via iterative performance of a combination operation in which a hierarchically-superior cluster is generated from a set of multi-string clusters. The combination operation includes, for candidate pairs of multi-string clusters, generating syntactic profiles describing an alphanumeric string from each multi-string cluster of the candidate pair. For each of the candidate pairs, a cost factor is determined for at least one of its syntactic profiles. Based on the cost factors determined for the syntactic profiles, one of the candidate pairs is selected. The multi-string clusters from the selected candidate pair are combined to generate the hierarchically-superior cluster including all of the alphanumeric strings from …},\n",
      "journal = {},\n",
      "year = {2019},\n",
      "pages = {},\n",
      "month = aug,\n",
      "url = {https://patentimages.storage.googleapis.com/b1/74/90/ca18093fcc5a52/US10394874.pdf}\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(new_pubs_patent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'search_metadata': {'id': '664118230ead419ac9d65ba6',\n",
       "  'status': 'Success',\n",
       "  'json_endpoint': 'https://serpapi.com/searches/be9d6ee9b45f13bb/664118230ead419ac9d65ba6.json',\n",
       "  'created_at': '2024-05-12 19:27:31 UTC',\n",
       "  'processed_at': '2024-05-12 19:27:31 UTC',\n",
       "  'google_scholar_author_url': 'https://scholar.google.com/citations?hl=en&view_op=view_citation&citation_for_view=qYhRbJoAAAAJ:wyM6WWKXmoIC',\n",
       "  'raw_html_file': 'https://serpapi.com/searches/be9d6ee9b45f13bb/664118230ead419ac9d65ba6.html',\n",
       "  'total_time_taken': 2.11},\n",
       " 'search_parameters': {'engine': 'google_scholar_author',\n",
       "  'hl': 'en',\n",
       "  'view_op': 'view_citation',\n",
       "  'citation_id': 'qYhRbJoAAAAJ:wyM6WWKXmoIC'},\n",
       " 'citation': {'title': 'Sample-Efficient Personalization: Modeling User Parameters as Low Rank Plus Sparse Components',\n",
       "  'link': 'https://proceedings.mlr.press/v238/pal24a.html',\n",
       "  'resources': [{'title': 'from mlr.press',\n",
       "    'file_format': 'PDF',\n",
       "    'link': 'https://proceedings.mlr.press/v238/pal24a/pal24a.pdf'}],\n",
       "  'authors': 'Soumyabrata Pal, Prateek Varshney, Gagan Madan, Prateek Jain, Abhradeep Thakurta, Gaurav Aggarwal, Pradeep Shenoy, Gaurav Srivastava',\n",
       "  'publication_date': '2024/4/18',\n",
       "  'conference': 'International Conference on Artificial Intelligence and Statistics',\n",
       "  'pages': '1702-1710',\n",
       "  'publisher': 'PMLR',\n",
       "  'description': 'Personalization of machine learning (ML) predictions for individual users/domains/enterprises is critical for practical recommendation systems. Standard personalization approaches involve learning a user/domain specific\\\\emph {embedding} that is fed into a fixed global model which can be limiting. On the other hand, personalizing/fine-tuning model itself for each user/domain—aka meta-learning—has high storage/infrastructure cost. Moreover, rigorous theoretical studies of scalable personalization approaches have been very limited. To address the above issues, we propose a novel meta-learning style approach that models network weights as a sum of low-rank and sparse components. This captures common information from multiple individuals/users together in the low-rank part while sparse part captures user-specific idiosyncrasies. We then study the framework in the linear setting, where the problem reduces to that of estimating the sum of a rank- and a -column sparse matrix using a small number of linear measurements. We propose a computationally efficient alternating minimization method with iterative hard thresholding—AMHT-LRS—to learn the low-rank and sparse part. Theoretically, for the realizable Gaussian data setting, we show that AMHT-LRS solves the problem efficiently with nearly optimal sample complexity. Finally, a significant challenge in personalization is ensuring privacy of each user’s sensitive data. We alleviate this problem by proposing a differentially private variant of our method that also is equipped with strong generalization guarantees.',\n",
       "  'scholar_articles': [{'title': 'Sample-Efficient Personalization: Modeling User Parameters as Low Rank Plus Sparse Components',\n",
       "    'link': 'https://scholar.google.com/scholar?oi=bibs&cluster=12248542259396760880&btnI=1&hl=en',\n",
       "    'authors': 'S Pal, P Varshney, G Madan, P Jain, A Thakurta… - International Conference on Artificial Intelligence and …, 2024',\n",
       "    'related_pages_link': {'link': 'https://scholar.google.com/scholar?oi=bibs&hl=en&q=related:MFW0E0qQ-6kJ:scholar.google.com/'},\n",
       "    'versions': {'total': 2,\n",
       "     'link': 'https://scholar.google.com/scholar?oi=bibs&hl=en&cluster=12248542259396760880',\n",
       "     'serpapi_link': 'https://serpapi.com/search.json?cluster=12248542259396760880&engine=google_scholar&hl=en',\n",
       "     'cluster_id': '12248542259396760880'}}]}}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://dl.acm.org/doi/pdf/10.1145/3511808.3557293\n",
      "https://ieeexplore.ieee.org/iel7/79/9186128/09186141.pdf\n",
      "https://aaai.org/ojs/index.php/AAAI/article/view/4260/4138\n",
      "https://journal.iisc.ac.in/index.php/iisc/article/download/4767/5054\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlretrieve\n",
    "from urllib.error import HTTPError\n",
    "for pub_entry in all_pubs:\n",
    "    url = ( pub_entry['pdflink'] )\n",
    "    try:\n",
    "        urlretrieve(url,pub_entry['url'])\n",
    "    except HTTPError:\n",
    "        print(pub_entry['pdflink'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'apr'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import calendar\n",
    "import string\n",
    "month='4'\n",
    "calendar.month_name[int(month)][0:3].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Optimal nonsmooth Frank-Wolfe method for stochastic regret minimization',\n",
       " 'patent': 'False',\n",
       " 'authors': 'Kiran Koshy Thekumparampil, Prateek Jain, Praneeth Netrapalli, Sewoong Oh',\n",
       " 'date': '2020',\n",
       " 'bibkey': 'ThekumparampilJNO20',\n",
       " 'url': 'allpapers/ThekumparampilJNO20.pdf',\n",
       " 'conference': '',\n",
       " 'journal': '12th OPT Workshop on Optimization for Machine Learning (OPT2020)',\n",
       " 'pages': '',\n",
       " 'description': 'The current best-known algorithm for convex constrained nonsmooth online stochastic regret minimization using a Linear Minimization Oracle (LMO, a la Frank-Wolfe) and a Stochastic First-order Oracle (SFO) achieves a regret of O (K3/4), where K is the number of iterations [26]. We provide two novel single-loop nonsmooth Frank-Wolfe methods, P-MOLES & PD-MOLES, which achieve the nearly-optimal online stochastic (non-adversarial) regret of O (',\n",
       " 'fileformat': 'PDF',\n",
       " 'pdflink': 'http://www.opt-ml.org/papers/2020/paper_101.pdf'}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pubs[65]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
